[{
  "history_id" : "d2bpqvphy2z",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1727468367395,
  "history_end_time" : 1727468367395,
  "history_notes" : null,
  "history_process" : "9cqdos",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "sxitsbxpcol",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1727468367397,
  "history_end_time" : 1727468367397,
  "history_notes" : null,
  "history_process" : "iq85px",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "uzal58emtbn",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1727468367398,
  "history_end_time" : 1727468367398,
  "history_notes" : null,
  "history_process" : "c8851t",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "4s163sg24s0",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1727468367399,
  "history_end_time" : 1727468367399,
  "history_notes" : null,
  "history_process" : "s20a2d",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "9kadxjtwpuc",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1727468367400,
  "history_end_time" : 1727468367400,
  "history_notes" : null,
  "history_process" : "xnk7n1",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "amd4fb52375",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1727468384413,
  "history_notes" : null,
  "history_process" : "x4ch0z",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "hrnw1lmpmeg",
  "history_input" : "import numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nimport joblib\nimport matplotlib.pyplot as plt\n\n# Step 1: Load the dataset\nfile_path = '/Users/vishesh/Desktop/geo_project/data.csv'\ndata = pd.read_csv(file_path, skiprows=3)\n\n# Step 2: Preprocess the data\ndata['Date'] = pd.to_datetime(data['Date'], format='%Y%m')\nvalues = data['Value'].values\n\n# Step 3: Normalize the data for LSTM\nscaler = MinMaxScaler(feature_range=(0, 1))\nscaled_data = scaler.fit_transform(values.reshape(-1, 1))\n\n# Save the scaler for inverse transformation later\njoblib.dump(scaler, 'scaler.pkl')\n\n# Step 4: Create sequences for LSTM\ndef create_sequences(data, sequence_length):\n    sequences = []\n    targets = []\n    for i in range(len(data) - sequence_length):\n        seq = data[i:i + sequence_length]\n        target = data[i + sequence_length]\n        sequences.append(seq)\n        targets.append(target)\n    return np.array(sequences), np.array(targets)\n\nsequence_length = 12  # 12 months\nX, y = create_sequences(scaled_data, sequence_length)\n\n# Step 5: Split into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n\n# Reshape the data for LSTM\nX_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\nX_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n\n# Step 6: Save preprocessed data for later use\nnp.savez('preprocessed_data.npz', X_train=X_train, X_test=X_test, y_train=y_train, y_test=y_test)\n\nprint(\"Data preprocessing completed and saved!\")\n\n\n\nplt.figure(figsize=(10,6))\nplt.plot(data['Date'], data['Value'], label='Temperature Values')\nplt.title('Temperature Values Over Time')\nplt.xlabel('Date')\nplt.ylabel('Temperature (Degrees Fahrenheit)')\nplt.grid(True)\nplt.legend()\nplt.show()\n",
  "history_output" : "Data preprocessing completed and saved!\n2024-09-27 16:19:30.019 python[35633:5742492] +[IMKClient subclass]: chose IMKClient_Legacy\n",
  "history_begin_time" : 1727468368271,
  "history_end_time" : 1727468371897,
  "history_notes" : null,
  "history_process" : "keghqi",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "lffm47ueniw",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1727468367407,
  "history_end_time" : 1727468367407,
  "history_notes" : null,
  "history_process" : "fhhx69",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "uzj6d5ar5np",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1727468367408,
  "history_end_time" : 1727468367408,
  "history_notes" : null,
  "history_process" : "rfwos9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "s0eg04dpv2v",
  "history_input" : "import numpy as np\nimport optuna\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import Dense, Dropout, Conv1D, MaxPooling1D, Flatten, LSTM, Input, MultiHeadAttention, LayerNormalization, Add\nfrom tensorflow.keras.optimizers import Adam\nfrom pytorch_tabnet.tab_model import TabNetRegressor\n\n# Load preprocessed data\ndata_path = '/Users/vishesh/gw-workspace/L6gDfkNJofh8/preprocessed_data.npz'\ndata = np.load(data_path)\n\n# List all variable names in the .npz file to identify contents\nprint(\"Variables in the .npz file:\", data.files)\n\n# Load data based on contents of the .npz file\nX_train = data['X_train']\ny_train = data['y_train']\n\n# Check if 'X_val' and 'y_val' exist; if not, create validation split\nif 'X_val' in data.files and 'y_val' in data.files:\n    X_val = data['X_val']\n    y_val = data['y_val']\nelse:\n    # Split the data into training and validation sets\n    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n\n# Initialize a dictionary to store model performance\nmodel_performance = {}\n\n# Define the function to create various models\ndef create_model(trial):\n    input_shape = X_train.shape[1]\n    model_type = trial.suggest_categorical('model_type', ['dense', 'cnn', 'lstm', 'transformer', 'tabnet'])\n\n    if model_type == 'dense':\n        model = Sequential()\n        model.add(Input(shape=(input_shape,)))  # Use Input layer to specify input shape\n        num_layers = trial.suggest_int('num_layers', 1, 5)\n        for i in range(num_layers):\n            num_units = trial.suggest_int(f'num_units_l{i}', 16, 128, log=True)\n            model.add(Dense(num_units, activation='relu'))\n            dropout_rate = trial.suggest_float(f'dropout_rate_l{i}', 0.1, 0.5)\n            model.add(Dropout(dropout_rate))\n        model.add(Dense(1, activation='linear'))\n    \n    elif model_type == 'cnn':\n        model = Sequential()\n        model.add(Input(shape=(input_shape, 1)))  # Use Input layer for CNN input shape\n        model.add(Conv1D(filters=trial.suggest_int('filters', 16, 64, log=True),\n                         kernel_size=trial.suggest_int('kernel_size', 3, 5),\n                         activation='relu'))\n        model.add(MaxPooling1D(pool_size=2))\n        model.add(Flatten())\n        model.add(Dense(1, activation='linear'))\n    \n    elif model_type == 'lstm':\n        model = Sequential()\n        model.add(Input(shape=(input_shape, 1)))  # Use Input layer for LSTM input shape\n        model.add(LSTM(units=trial.suggest_int('lstm_units', 16, 64, log=True)))\n        model.add(Dense(1, activation='linear'))\n    \n    elif model_type == 'transformer':\n        inputs = Input(shape=(input_shape, 1))\n        attention = MultiHeadAttention(num_heads=trial.suggest_int('num_heads', 2, 8), key_dim=trial.suggest_int('key_dim', 16, 64))(inputs, inputs)\n        attention = Add()([inputs, attention])\n        attention = LayerNormalization(epsilon=1e-6)(attention)\n        outputs = Flatten()(attention)\n        outputs = Dense(1, activation='linear')(outputs)\n        model = Model(inputs=inputs, outputs=outputs)\n    \n    elif model_type == 'tabnet':\n        tabnet_model = TabNetRegressor(\n            n_d=trial.suggest_int('n_d', 8, 64),\n            n_a=trial.suggest_int('n_a', 8, 64),\n            n_steps=trial.suggest_int('n_steps', 3, 10),\n            gamma=trial.suggest_float('gamma', 1.0, 2.0),\n            lambda_sparse=trial.suggest_float('lambda_sparse', 1e-6, 1e-3)\n        )\n        tabnet_model.fit(\n            X_train, y_train,\n            eval_set=[(X_val, y_val)],\n            eval_metric=['mae'],\n            max_epochs=100,\n            patience=10,\n            batch_size=256,\n            virtual_batch_size=128\n        )\n        return tabnet_model\n\n    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-2, log=True)\n    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mean_squared_error', metrics=['mean_absolute_error'])\n\n    return model\n\n# Define the objective function for Optuna\ndef objective(trial):\n    model = create_model(trial)\n\n    if isinstance(model, TabNetRegressor):\n        model.fit(\n            X_train, y_train,\n            eval_set=[(X_val, y_val)],\n            eval_metric=['mae'],\n            max_epochs=100,\n            patience=10,\n            batch_size=256,\n            virtual_batch_size=128\n        )\n        y_pred = model.predict(X_val)\n        mae = mean_absolute_error(y_val, y_pred)\n    else:\n        model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10, batch_size=32, verbose=0)\n        _, mae = model.evaluate(X_val, y_val, verbose=0)\n\n    model_type = trial.params['model_type']\n    model_performance[model_type] = mae\n    return mae\n\n# Run the Bayesian optimization\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=10)  # Set n_trials as needed\n\n# Get the best hyperparameters\nbest_params = study.best_params\nprint(f\"Best hyperparameters: {best_params}\")\n\n# Display performance of each model\nprint(\"Model performance (MAE):\")\nfor model_type, mae in model_performance.items():\n    print(f\"{model_type}: {mae}\")\n\n# Plot the model performance\nplt.figure(figsize=(10, 5))\nplt.bar(model_performance.keys(), model_performance.values(), color='skyblue')\nplt.xlabel('Model Type')\nplt.ylabel('Mean Absolute Error')\nplt.title('Model Performance Comparison')\nplt.show()\n",
  "history_output" : "Variables in the .npz file: ['X_train', 'X_test', 'y_train', 'y_test']\n[I 2024-09-27 16:19:37,143] A new study created in memory with name: no-name-67072c35-a556-4562-8504-c9ff67217379\n[I 2024-09-27 16:19:37,747] Trial 0 finished with value: 0.0449388213455677 and parameters: {'model_type': 'dense', 'num_layers': 1, 'num_units_l0': 60, 'dropout_rate_l0': 0.14686138411278696, 'learning_rate': 0.0008496021206082136}. Best is trial 0 with value: 0.0449388213455677.\n[I 2024-09-27 16:19:38,887] Trial 1 finished with value: 0.31271544098854065 and parameters: {'model_type': 'transformer', 'num_heads': 4, 'key_dim': 60, 'learning_rate': 0.00011239198060713093}. Best is trial 0 with value: 0.0449388213455677.\n[I 2024-09-27 16:19:39,411] Trial 2 finished with value: 0.04936045780777931 and parameters: {'model_type': 'cnn', 'filters': 53, 'kernel_size': 4, 'learning_rate': 0.0033878796948567788}. Best is trial 0 with value: 0.0449388213455677.\n[I 2024-09-27 16:19:40,642] Trial 3 finished with value: 0.09967638552188873 and parameters: {'model_type': 'transformer', 'num_heads': 6, 'key_dim': 36, 'learning_rate': 0.0005487583696645022}. Best is trial 0 with value: 0.0449388213455677.\n[I 2024-09-27 16:19:41,458] Trial 4 finished with value: 0.06228038668632507 and parameters: {'model_type': 'dense', 'num_layers': 4, 'num_units_l0': 28, 'dropout_rate_l0': 0.11442556816131698, 'num_units_l1': 19, 'dropout_rate_l1': 0.1837025189683541, 'num_units_l2': 28, 'dropout_rate_l2': 0.3043043827472813, 'num_units_l3': 44, 'dropout_rate_l3': 0.2975610526197204, 'learning_rate': 0.0009736690321857154}. Best is trial 0 with value: 0.0449388213455677.\n[I 2024-09-27 16:19:42,744] Trial 5 finished with value: 0.09982165694236755 and parameters: {'model_type': 'transformer', 'num_heads': 7, 'key_dim': 53, 'learning_rate': 0.0024062138871676917}. Best is trial 0 with value: 0.0449388213455677.\n[I 2024-09-27 16:19:43,759] Trial 6 finished with value: 0.2032669633626938 and parameters: {'model_type': 'lstm', 'lstm_units': 22, 'learning_rate': 3.4913565367779823e-05}. Best is trial 0 with value: 0.0449388213455677.\n/opt/anaconda3/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n  warnings.warn(f\"Device used : {self.device}\")\n[W 2024-09-27 16:19:43,762] Trial 7 failed with parameters: {'model_type': 'tabnet', 'n_d': 39, 'n_a': 17, 'n_steps': 8, 'gamma': 1.6718383771259564, 'lambda_sparse': 0.0005228912641464338} because of the following error: ValueError('Found array with dim 3. None expected <= 2.').\nTraceback (most recent call last):\n  File \"/opt/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n    value_or_values = func(trial)\n                      ^^^^^^^^^^^\n  File \"/Users/vishesh/gw-workspace/s0eg04dpv2v/auto_ml.py\", line 100, in objective\n    model = create_model(trial)\n            ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/vishesh/gw-workspace/s0eg04dpv2v/auto_ml.py\", line 82, in create_model\n    tabnet_model.fit(\n  File \"/opt/anaconda3/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py\", line 217, in fit\n    check_input(X_train)\n  File \"/opt/anaconda3/lib/python3.11/site-packages/pytorch_tabnet/utils.py\", line 507, in check_input\n    check_array(X, accept_sparse=True)\n  File \"/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py\", line 915, in check_array\n    raise ValueError(\nValueError: Found array with dim 3. None expected <= 2.\n[W 2024-09-27 16:19:43,763] Trial 7 failed with value None.\nTraceback (most recent call last):\n  File \"/Users/vishesh/gw-workspace/s0eg04dpv2v/auto_ml.py\", line 124, in <module>\n    study.optimize(objective, n_trials=10)  # Set n_trials as needed\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/optuna/study/study.py\", line 475, in optimize\n    _optimize(\n  File \"/opt/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 63, in _optimize\n    _optimize_sequential(\n  File \"/opt/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 160, in _optimize_sequential\n    frozen_trial = _run_trial(study, func, catch)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 248, in _run_trial\n    raise func_err\n  File \"/opt/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n    value_or_values = func(trial)\n                      ^^^^^^^^^^^\n  File \"/Users/vishesh/gw-workspace/s0eg04dpv2v/auto_ml.py\", line 100, in objective\n    model = create_model(trial)\n            ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/vishesh/gw-workspace/s0eg04dpv2v/auto_ml.py\", line 82, in create_model\n    tabnet_model.fit(\n  File \"/opt/anaconda3/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py\", line 217, in fit\n    check_input(X_train)\n  File \"/opt/anaconda3/lib/python3.11/site-packages/pytorch_tabnet/utils.py\", line 507, in check_input\n    check_array(X, accept_sparse=True)\n  File \"/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py\", line 915, in check_array\n    raise ValueError(\nValueError: Found array with dim 3. None expected <= 2.\n",
  "history_begin_time" : 1727468372881,
  "history_end_time" : 1727468384412,
  "history_notes" : null,
  "history_process" : "vnrvoi",
  "host_id" : "100001",
  "indicator" : "Failed"
}]
