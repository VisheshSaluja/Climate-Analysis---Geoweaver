[{
  "history_id" : "2leumb7iq4e",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1727715414196,
  "history_end_time" : 1727715414196,
  "history_notes" : null,
  "history_process" : "9cqdos",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "9nd2kiga6n4",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1727715414198,
  "history_end_time" : 1727715414198,
  "history_notes" : null,
  "history_process" : "iq85px",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "hmjiol7bnut",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1727715414198,
  "history_end_time" : 1727715414198,
  "history_notes" : null,
  "history_process" : "c8851t",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "7ki7e4srlgw",
  "history_input" : "\n\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM, Dense, Dropout\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.regularizers import l2\nimport joblib\n\n# Step 1: Load the preprocessed data\ndata = np.load('/Users/vishesh/gw-workspace/L6gDfkNJofh8/preprocessed_data.npz')\nX_train, X_test, y_train, y_test = data['X_train'], data['X_test'], data['y_train'], data['y_test']\n\n# Step 2: Build the LSTM model\nmodel = Sequential()\nmodel.add(LSTM(units=50, return_sequences=True, input_shape=(X_train.shape[1], 1), \n               kernel_regularizer=l2(0.001)))  # L2 regularization\nmodel.add(Dropout(0.2))  # Increased dropout rate\nmodel.add(LSTM(units=50, return_sequences=False, kernel_regularizer=l2(0.001)))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(units=1))\n\n# Step 3: Compile the model with reduced learning rate\noptimizer = tf.keras.optimizers.Adam(learning_rate=0.0003)  # Reduced learning rate\nmodel.compile(optimizer=optimizer, loss='mean_squared_error')\n\n# Step 4: Train the model with early stopping\nearly_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n\nhistory = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test), \n                    callbacks=[early_stopping])\n\n# Step 5: Save the model and the training history\nmodel.save('lstm_model.h5')\nnp.save('training_history.npy', history.history)\n\n# Step 6: Make predictions on the test data\npredictions = model.predict(X_test)\n\n# Save predictions for later use\nnp.save('predictions.npy', predictions)\n\nprint(\"Model training completed, predictions made and saved!\")\n\n",
  "history_output" : "/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(**kwargs)\nEpoch 1/50\n\u001B[1m 1/39\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m36s\u001B[0m 973ms/step - loss: 0.2240\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m12/39\u001B[0m \u001B[32m━━━━━━\u001B[0m\u001B[37m━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 0.1983   \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m25/39\u001B[0m \u001B[32m━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.1732\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m36/39\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 0.1588\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m39/39\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 9ms/step - loss: 0.1548 - val_loss: 0.0865\nEpoch 2/50\n\u001B[1m 1/39\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 13ms/step - loss: 0.0832\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m15/39\u001B[0m \u001B[32m━━━━━━━\u001B[0m\u001B[37m━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0824 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m28/39\u001B[0m \u001B[32m━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0813\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m39/39\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0803 - val_loss: 0.0812\nEpoch 3/50\n\u001B[1m 1/39\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 12ms/step - loss: 0.0783\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m14/39\u001B[0m \u001B[32m━━━━━━━\u001B[0m\u001B[37m━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0731 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m28/39\u001B[0m \u001B[32m━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0712\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m39/39\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0700 - val_loss: 0.0713\nEpoch 4/50\n\u001B[1m 1/39\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 11ms/step - loss: 0.0607\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m15/39\u001B[0m \u001B[32m━━━━━━━\u001B[0m\u001B[37m━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0601 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m28/39\u001B[0m \u001B[32m━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0594\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m39/39\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0589 - val_loss: 0.0617\nEpoch 5/50\n\u001B[1m 1/39\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 12ms/step - loss: 0.0494\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m15/39\u001B[0m \u001B[32m━━━━━━━\u001B[0m\u001B[37m━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0532 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m29/39\u001B[0m \u001B[32m━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0523\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m39/39\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0518 - val_loss: 0.0554\nEpoch 6/50\n\u001B[1m 1/39\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 12ms/step - loss: 0.0465\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m15/39\u001B[0m \u001B[32m━━━━━━━\u001B[0m\u001B[37m━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0479 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m26/39\u001B[0m \u001B[32m━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0471\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m27/39\u001B[0m \u001B[32m━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 0.0470\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m31/39\u001B[0m \u001B[32m━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - loss: 0.0468\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m39/39\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 7ms/step - loss: 0.0463 - val_loss: 0.0542\nEpoch 7/50\n\u001B[1m 1/39\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 12ms/step - loss: 0.0433\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m14/39\u001B[0m \u001B[32m━━━━━━━\u001B[0m\u001B[37m━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0416 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m27/39\u001B[0m \u001B[32m━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0409\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m39/39\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0404 - val_loss: 0.0453\nEpoch 8/50\n\u001B[1m 1/39\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 12ms/step - loss: 0.0401\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m15/39\u001B[0m \u001B[32m━━━━━━━\u001B[0m\u001B[37m━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0370 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m29/39\u001B[0m \u001B[32m━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0363\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m39/39\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0359 - val_loss: 0.0420\nEpoch 9/50\n\u001B[1m 1/39\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 12ms/step - loss: 0.0327\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m14/39\u001B[0m \u001B[32m━━━━━━━\u001B[0m\u001B[37m━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0323 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m27/39\u001B[0m \u001B[32m━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0321\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m39/39\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0320 - val_loss: 0.0392\nEpoch 10/50\n\u001B[1m 1/39\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 14ms/step - loss: 0.0314\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m14/39\u001B[0m \u001B[32m━━━━━━━\u001B[0m\u001B[37m━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0296 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m27/39\u001B[0m \u001B[32m━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0291\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m39/39\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 0.0287 - val_loss: 0.0345\nEpoch 11/50\n\u001B[1m 1/39\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 13ms/step - loss: 0.0272\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m14/39\u001B[0m \u001B[32m━━━━━━━\u001B[0m\u001B[37m━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0267 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m27/39\u001B[0m \u001B[32m━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0262\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m39/39\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 0.0259 - val_loss: 0.0322\nEpoch 12/50\n\u001B[1m 1/39\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 12ms/step - loss: 0.0247\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m14/39\u001B[0m \u001B[32m━━━━━━━\u001B[0m\u001B[37m━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0238 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m27/39\u001B[0m \u001B[32m━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0239\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m39/39\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0239 - val_loss: 0.0342\nEpoch 13/50\n\u001B[1m 1/39\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 15ms/step - loss: 0.0206\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m14/39\u001B[0m \u001B[32m━━━━━━━\u001B[0m\u001B[37m━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0217 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m28/39\u001B[0m \u001B[32m━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0220\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m39/39\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0220 - val_loss: 0.0340\nEpoch 14/50\n\u001B[1m 1/39\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 12ms/step - loss: 0.0211\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m14/39\u001B[0m \u001B[32m━━━━━━━\u001B[0m\u001B[37m━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0210 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m28/39\u001B[0m \u001B[32m━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0209\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m39/39\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0208 - val_loss: 0.0287\nEpoch 15/50\n\u001B[1m 1/39\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 13ms/step - loss: 0.0208\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m15/39\u001B[0m \u001B[32m━━━━━━━\u001B[0m\u001B[37m━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0207 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m29/39\u001B[0m \u001B[32m━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0201\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m39/39\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0199 - val_loss: 0.0247\nEpoch 16/50\n\u001B[1m 1/39\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 15ms/step - loss: 0.0181\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m14/39\u001B[0m \u001B[32m━━━━━━━\u001B[0m\u001B[37m━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0183 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m28/39\u001B[0m \u001B[32m━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0181\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m39/39\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0180 - val_loss: 0.0269\nEpoch 17/50\n\u001B[1m 1/39\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 13ms/step - loss: 0.0179\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m14/39\u001B[0m \u001B[32m━━━━━━━\u001B[0m\u001B[37m━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0174 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m24/39\u001B[0m \u001B[32m━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 0.0173\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m37/39\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0172\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m39/39\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 0.0172 - val_loss: 0.0239\nEpoch 18/50\n\u001B[1m 1/39\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 12ms/step - loss: 0.0168\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m14/39\u001B[0m \u001B[32m━━━━━━━\u001B[0m\u001B[37m━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0161 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m27/39\u001B[0m \u001B[32m━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0160\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m39/39\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0159 - val_loss: 0.0272\nEpoch 19/50\n\u001B[1m 1/39\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 12ms/step - loss: 0.0176\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m14/39\u001B[0m \u001B[32m━━━━━━━\u001B[0m\u001B[37m━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0156 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m23/39\u001B[0m \u001B[32m━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 0.0155\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m37/39\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0155\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m39/39\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 0.0155 - val_loss: 0.0176\nEpoch 20/50\n\u001B[1m 1/39\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 12ms/step - loss: 0.0134\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m15/39\u001B[0m \u001B[32m━━━━━━━\u001B[0m\u001B[37m━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0136 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m29/39\u001B[0m \u001B[32m━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0139\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m39/39\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0140 - val_loss: 0.0179\nEpoch 21/50\n\u001B[1m 1/39\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m1s\u001B[0m 27ms/step - loss: 0.0143\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m13/39\u001B[0m \u001B[32m━━━━━━\u001B[0m\u001B[37m━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0142 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m27/39\u001B[0m \u001B[32m━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0141\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m39/39\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0140 - val_loss: 0.0224\nEpoch 22/50\n\u001B[1m 1/39\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 12ms/step - loss: 0.0130\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m14/39\u001B[0m \u001B[32m━━━━━━━\u001B[0m\u001B[37m━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0135 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m24/39\u001B[0m \u001B[32m━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 0.0136\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m33/39\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 0.0136\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m39/39\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 6ms/step - loss: 0.0136 - val_loss: 0.0185\nWARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n\u001B[1m 1/10\u001B[0m \u001B[32m━━\u001B[0m\u001B[37m━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 101ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m10/10\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 20ms/step \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m10/10\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 20ms/step\nModel training completed, predictions made and saved!\n",
  "history_begin_time" : 1727715421010,
  "history_end_time" : 1727715430722,
  "history_notes" : null,
  "history_process" : "s20a2d",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "aifa3cqaqyo",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1727715414202,
  "history_end_time" : 1727715414202,
  "history_notes" : null,
  "history_process" : "xnk7n1",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "yyk2as9w104",
  "history_input" : "# import pandas as pd\n# import matplotlib.pyplot as plt\n# import seaborn as sns\n\n# def visualize(file_name, predictions_file, anomalies_file):\n#     data = pd.read_csv(file_name)\n#     predictions = pd.read_csv(predictions_file)\n#     anomalies = pd.read_csv(anomalies_file)\n    \n#     plt.figure(figsize=(12, 8))\n    \n#     # Plotting temperature trends with predictions\n#     plt.subplot(2, 1, 1)\n#     plt.plot(data['Year'], data['Temp_Value'], label='Observed Temperature')\n#     plt.plot(predictions['Year'], predictions['Temp_Prediction'], label='Predicted Temperature', linestyle='--')\n#     plt.title('Temperature Trends and Predictions')\n#     plt.xlabel('Year')\n#     plt.ylabel('Temperature (°C)')\n#     plt.legend()\n    \n#     # Plotting precipitation trends with predictions\n#     plt.subplot(2, 1, 2)\n#     plt.plot(data['Year'], data['Precipitation'], label='Observed Precipitation')\n#     plt.plot(predictions['Year'], predictions['Precip_Prediction'], label='Predicted Precipitation', linestyle='--')\n#     plt.title('Precipitation Trends and Predictions')\n#     plt.xlabel('Year')\n#     plt.ylabel('Precipitation (mm)')\n#     plt.legend()\n    \n#     # Highlighting anomalies\n#     sns.scatterplot(data=anomalies, x='Year', y='Temp_Value', hue='Cluster', palette='deep', legend='full', s=100)\n    \n#     plt.tight_layout()\n#     plt.savefig('climate_trends_and_anomalies.png')\n#     plt.show()\n\n# if __name__ == \"__main__\":\n#     visualize('/Users/vishesh/Desktop/geo_project/analyzed_climate_data.csv', '/Users/vishesh/Desktop/geo_project/predicted_climate_data.csv', '/Users/vishesh/Desktop/geo_project/anomaly_detected_data.csv')\n\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport joblib\n\n# Step 1: Load predictions, actual values, and scaler\npredictions = np.load('/Users/vishesh/gw-workspace/9I3eI9y6wb9B/predictions.npy')\ndata = np.load('/Users/vishesh/gw-workspace/L6gDfkNJofh8/preprocessed_data.npz')\ny_test = data['y_test']\n\n# Load the scaler for inverse transformation\nscaler = joblib.load('/Users/vishesh/gw-workspace/f7bNu3SQ6blN/scaler.pkl')\n\n# Step 2: Inverse scale the predictions and real values\npredicted_values = scaler.inverse_transform(predictions)\nreal_values = scaler.inverse_transform(y_test.reshape(-1, 1))\n\n# Step 3: Visualize the loss over epochs and print loss values\nhistory = np.load('/Users/vishesh/gw-workspace/9I3eI9y6wb9B/training_history.npy', allow_pickle=True).item()\n\n# Print the loss values after each epoch\ntrain_loss = history['loss']\nval_loss = history['val_loss']\n\nprint(\"Epoch-wise Losses:\")\nfor i in range(len(train_loss)):\n    print(f\"Epoch {i+1}: Train Loss = {train_loss[i]}, Validation Loss = {val_loss[i]}\")\n\n# Plot the loss curves\nplt.plot(train_loss, label='Train Loss')\nplt.plot(val_loss, label='Validation Loss')\nplt.title('Model Loss over Epochs')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()\n\n# Step 4: Visualize the predicted vs actual values\nplt.plot(real_values, color='blue', label='Real Temperature Values')\nplt.plot(predicted_values, color='red', label='Predicted Temperature Values')\nplt.title('Real vs Predicted Temperature Values')\nplt.xlabel('Time')\nplt.ylabel('Temperature')\nplt.legend()\nplt.show()\n",
  "history_output" : "Epoch-wise Losses:\nEpoch 1: Train Loss = 0.03072919137775898, Validation Loss = 0.01742088980972767\nEpoch 2: Train Loss = 0.011098137125372887, Validation Loss = 0.02268320694565773\nEpoch 3: Train Loss = 0.010209755040705204, Validation Loss = 0.01720193214714527\nEpoch 4: Train Loss = 0.00854005292057991, Validation Loss = 0.012641695328056812\nEpoch 5: Train Loss = 0.007384682539850473, Validation Loss = 0.017973309382796288\nEpoch 6: Train Loss = 0.006740374490618706, Validation Loss = 0.009798855520784855\nEpoch 7: Train Loss = 0.005943419877439737, Validation Loss = 0.010053860954940319\nEpoch 8: Train Loss = 0.00539526529610157, Validation Loss = 0.01057480089366436\nEpoch 9: Train Loss = 0.005453225690871477, Validation Loss = 0.00832605641335249\nEpoch 10: Train Loss = 0.005200219806283712, Validation Loss = 0.006214628461748362\nEpoch 11: Train Loss = 0.005218852777034044, Validation Loss = 0.006327028851956129\nEpoch 12: Train Loss = 0.005223800893872976, Validation Loss = 0.011008824221789837\nEpoch 13: Train Loss = 0.0046732318587601185, Validation Loss = 0.0043561640195548534\nEpoch 14: Train Loss = 0.004665676970034838, Validation Loss = 0.005817031487822533\nEpoch 15: Train Loss = 0.004796288441866636, Validation Loss = 0.009089156053960323\nEpoch 16: Train Loss = 0.004646167159080505, Validation Loss = 0.013137206435203552\nEpoch 17: Train Loss = 0.005103453528136015, Validation Loss = 0.003600385505706072\nEpoch 18: Train Loss = 0.0041281539015471935, Validation Loss = 0.005638223607093096\nEpoch 19: Train Loss = 0.004267961252480745, Validation Loss = 0.006245030555874109\nEpoch 20: Train Loss = 0.004086713772267103, Validation Loss = 0.0034914754796773195\n2024-09-30 12:59:16.932 python[90235:7713960] +[IMKClient subclass]: chose IMKClient_Legacy\n2024-09-30 12:59:16.932 python[90235:7713960] +[IMKInputSession subclass]: chose IMKInputSession_Legacy\n",
  "history_begin_time" : 1727715555465,
  "history_end_time" : 1727715572015,
  "history_notes" : null,
  "history_process" : "x4ch0z",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "ejel6jtewc9",
  "history_input" : "import numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nimport joblib\nimport matplotlib.pyplot as plt\n\n# Step 1: Load the dataset\nfile_path = '/Users/vishesh/Desktop/geo_project/data.csv'\ndata = pd.read_csv(file_path, skiprows=3)\n\n# Step 2: Preprocess the data\ndata['Date'] = pd.to_datetime(data['Date'], format='%Y%m')\nvalues = data['Value'].values\n\n# Step 3: Normalize the data for LSTM\nscaler = MinMaxScaler(feature_range=(0, 1))\nscaled_data = scaler.fit_transform(values.reshape(-1, 1))\n\n# Save the scaler for inverse transformation later\njoblib.dump(scaler, 'scaler.pkl')\n\n# Step 4: Create sequences for LSTM\ndef create_sequences(data, sequence_length):\n    sequences = []\n    targets = []\n    for i in range(len(data) - sequence_length):\n        seq = data[i:i + sequence_length]\n        target = data[i + sequence_length]\n        sequences.append(seq)\n        targets.append(target)\n    return np.array(sequences), np.array(targets)\n\nsequence_length = 12  # 12 months\nX, y = create_sequences(scaled_data, sequence_length)\n\n# Step 5: Split into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n\n# Reshape the data for LSTM\nX_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\nX_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n\n# Step 6: Save preprocessed data for later use\nnp.savez('preprocessed_data.npz', X_train=X_train, X_test=X_test, y_train=y_train, y_test=y_test)\n\nprint(\"Data preprocessing completed and saved!\")\n\n\n\nplt.figure(figsize=(10,6))\nplt.plot(data['Date'], data['Value'], label='Temperature Values')\nplt.title('Temperature Values Over Time')\nplt.xlabel('Date')\nplt.ylabel('Temperature (Degrees Fahrenheit)')\nplt.grid(True)\nplt.legend()\nplt.show()\n",
  "history_output" : "Data preprocessing completed and saved!\n2024-09-30 12:56:57.023 python[90094:7706517] +[IMKClient subclass]: chose IMKClient_Legacy\n2024-09-30 12:56:57.023 python[90094:7706517] +[IMKInputSession subclass]: chose IMKInputSession_Legacy\n",
  "history_begin_time" : 1727715414898,
  "history_end_time" : 1727715420491,
  "history_notes" : null,
  "history_process" : "keghqi",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "41zqhkuscyn",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1727715414204,
  "history_end_time" : 1727715414204,
  "history_notes" : null,
  "history_process" : "fhhx69",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "no7uv08n8dg",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1727715414205,
  "history_end_time" : 1727715414205,
  "history_notes" : null,
  "history_process" : "rfwos9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "bpv7day5xpj",
  "history_input" : "import numpy as np\nimport optuna\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import Dense, Dropout, Conv1D, MaxPooling1D, Flatten, LSTM, Input, MultiHeadAttention, LayerNormalization, Add\nfrom tensorflow.keras.optimizers import Adam\nfrom pytorch_tabnet.tab_model import TabNetRegressor\n\n# Load preprocessed data\ndata_path = '/Users/vishesh/gw-workspace/L6gDfkNJofh8/preprocessed_data.npz'\ndata = np.load(data_path)\n\n# List all variable names in the .npz file to identify contents\nprint(\"Variables in the .npz file:\", data.files)\n\n# Load data based on contents of the .npz file\nX_train = data['X_train']\ny_train = data['y_train']\n\n# Check if 'X_val' and 'y_val' exist; if not, create validation split\nif 'X_val' in data.files and 'y_val' in data.files:\n    X_val = data['X_val']\n    y_val = data['y_val']\nelse:\n    # Split the data into training and validation sets\n    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n\n# Initialize a dictionary to store model performance\nmodel_performance = {}\n\n# Define the function to create various models\ndef create_model(trial):\n    input_shape = X_train.shape[1]\n    model_type = trial.suggest_categorical('model_type', ['dense', 'cnn', 'lstm', 'transformer'])\n    # model_type = trial.suggest_categorical('model_type', ['dense', 'cnn', 'lstm', 'transformer', 'tabnet'])\n\n    if model_type == 'dense':\n        model = Sequential()\n        model.add(Input(shape=(input_shape,)))  # Use Input layer to specify input shape\n        num_layers = trial.suggest_int('num_layers', 1, 5)\n        for i in range(num_layers):\n            num_units = trial.suggest_int(f'num_units_l{i}', 16, 128, log=True)\n            model.add(Dense(num_units, activation='relu'))\n            dropout_rate = trial.suggest_float(f'dropout_rate_l{i}', 0.1, 0.5)\n            model.add(Dropout(dropout_rate))\n        model.add(Dense(1, activation='linear'))\n    \n    elif model_type == 'cnn':\n        model = Sequential()\n        model.add(Input(shape=(input_shape, 1)))  # Use Input layer for CNN input shape\n        model.add(Conv1D(filters=trial.suggest_int('filters', 16, 64, log=True),\n                         kernel_size=trial.suggest_int('kernel_size', 3, 5),\n                         activation='relu'))\n        model.add(MaxPooling1D(pool_size=2))\n        model.add(Flatten())\n        model.add(Dense(1, activation='linear'))\n    \n    elif model_type == 'lstm':\n        model = Sequential()\n        model.add(Input(shape=(input_shape, 1)))  # Use Input layer for LSTM input shape\n        model.add(LSTM(units=trial.suggest_int('lstm_units', 16, 64, log=True)))\n        model.add(Dense(1, activation='linear'))\n    \n    elif model_type == 'transformer':\n        inputs = Input(shape=(input_shape, 1))\n        attention = MultiHeadAttention(num_heads=trial.suggest_int('num_heads', 2, 8), key_dim=trial.suggest_int('key_dim', 16, 64))(inputs, inputs)\n        attention = Add()([inputs, attention])\n        attention = LayerNormalization(epsilon=1e-6)(attention)\n        outputs = Flatten()(attention)\n        outputs = Dense(1, activation='linear')(outputs)\n        model = Model(inputs=inputs, outputs=outputs)\n    \n    # elif model_type == 'tabnet':\n    #     tabnet_model = TabNetRegressor(\n    #         n_d=trial.suggest_int('n_d', 8, 64),\n    #         n_a=trial.suggest_int('n_a', 8, 64),\n    #         n_steps=trial.suggest_int('n_steps', 3, 10),\n    #         gamma=trial.suggest_float('gamma', 1.0, 2.0),\n    #         lambda_sparse=trial.suggest_float('lambda_sparse', 1e-6, 1e-3)\n    #     )\n    #     tabnet_model.fit(\n    #         X_train, y_train,\n    #         eval_set=[(X_val, y_val)],\n    #         eval_metric=['mae'],\n    #         max_epochs=100,\n    #         patience=10,\n    #         batch_size=256,\n    #         virtual_batch_size=128\n    #     )\n    #     return tabnet_model\n\n    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-2, log=True)\n    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mean_squared_error', metrics=['mean_absolute_error'])\n\n    return model\n\n# Define the objective function for Optuna\ndef objective(trial):\n    model = create_model(trial)\n\n    if isinstance(model, TabNetRegressor):\n        model.fit(\n            X_train, y_train,\n            eval_set=[(X_val, y_val)],\n            eval_metric=['mae'],\n            max_epochs=100,\n            patience=10,\n            batch_size=256,\n            virtual_batch_size=128\n        )\n        y_pred = model.predict(X_val)\n        mae = mean_absolute_error(y_val, y_pred)\n    else:\n        model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10, batch_size=32, verbose=0)\n        _, mae = model.evaluate(X_val, y_val, verbose=0)\n\n    model_type = trial.params['model_type']\n    model_performance[model_type] = mae\n    return mae\n\n# Run the Bayesian optimization\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=10)  # Set n_trials as needed\n\n# Get the best hyperparameters\nbest_params = study.best_params\nprint(f\"Best hyperparameters: {best_params}\")\n\n# Display performance of each model\nprint(\"Model performance (MAE):\")\nfor model_type, mae in model_performance.items():\n    print(f\"{model_type}: {mae}\")\n\n# Plot the model performance\nplt.figure(figsize=(10, 5))\nplt.bar(model_performance.keys(), model_performance.values(), color='skyblue')\nplt.xlabel('Model Type')\nplt.ylabel('Mean Absolute Error')\nplt.title('Model Performance Comparison')\nplt.show()\n",
  "history_output" : "Variables in the .npz file: ['X_train', 'X_test', 'y_train', 'y_test']\n[I 2024-09-30 12:57:05,920] A new study created in memory with name: no-name-d4c58874-f353-414e-8580-d7343db233fd\n[I 2024-09-30 12:57:06,601] Trial 0 finished with value: 0.04098828509449959 and parameters: {'model_type': 'dense', 'num_layers': 1, 'num_units_l0': 63, 'dropout_rate_l0': 0.390621291530455, 'learning_rate': 0.0009756397004626707}. Best is trial 0 with value: 0.04098828509449959.\n[I 2024-09-30 12:57:08,258] Trial 1 finished with value: 0.28850334882736206 and parameters: {'model_type': 'transformer', 'num_heads': 6, 'key_dim': 57, 'learning_rate': 0.00014783019039912043}. Best is trial 0 with value: 0.04098828509449959.\n[I 2024-09-30 12:57:09,689] Trial 2 finished with value: 0.20327264070510864 and parameters: {'model_type': 'lstm', 'lstm_units': 16, 'learning_rate': 4.626365553265496e-05}. Best is trial 0 with value: 0.04098828509449959.\n[I 2024-09-30 12:57:11,146] Trial 3 finished with value: 0.07043729722499847 and parameters: {'model_type': 'lstm', 'lstm_units': 47, 'learning_rate': 0.00015839789135540041}. Best is trial 0 with value: 0.04098828509449959.\n[I 2024-09-30 12:57:11,744] Trial 4 finished with value: 0.12144634127616882 and parameters: {'model_type': 'cnn', 'filters': 38, 'kernel_size': 4, 'learning_rate': 9.017941643598241e-05}. Best is trial 0 with value: 0.04098828509449959.\n[I 2024-09-30 12:57:13,016] Trial 5 finished with value: 0.10025979578495026 and parameters: {'model_type': 'transformer', 'num_heads': 7, 'key_dim': 19, 'learning_rate': 0.005916363083889092}. Best is trial 0 with value: 0.04098828509449959.\n[I 2024-09-30 12:57:14,776] Trial 6 finished with value: 0.37735515832901 and parameters: {'model_type': 'transformer', 'num_heads': 7, 'key_dim': 52, 'learning_rate': 4.1776521395589e-05}. Best is trial 0 with value: 0.04098828509449959.\n[I 2024-09-30 12:57:16,131] Trial 7 finished with value: 0.39068496227264404 and parameters: {'model_type': 'transformer', 'num_heads': 2, 'key_dim': 28, 'learning_rate': 1.7338537229650213e-05}. Best is trial 0 with value: 0.04098828509449959.\n[I 2024-09-30 12:57:17,303] Trial 8 finished with value: 0.08064393699169159 and parameters: {'model_type': 'dense', 'num_layers': 3, 'num_units_l0': 25, 'dropout_rate_l0': 0.38744389599066853, 'num_units_l1': 58, 'dropout_rate_l1': 0.10823179362735535, 'num_units_l2': 33, 'dropout_rate_l2': 0.48741943043260083, 'learning_rate': 0.0034257880312312675}. Best is trial 0 with value: 0.04098828509449959.\n[I 2024-09-30 12:57:19,632] Trial 9 finished with value: 0.09967779368162155 and parameters: {'model_type': 'transformer', 'num_heads': 7, 'key_dim': 35, 'learning_rate': 0.002616485279481814}. Best is trial 0 with value: 0.04098828509449959.\nBest hyperparameters: {'model_type': 'dense', 'num_layers': 1, 'num_units_l0': 63, 'dropout_rate_l0': 0.390621291530455, 'learning_rate': 0.0009756397004626707}\nModel performance (MAE):\ndense: 0.08064393699169159\ntransformer: 0.09967779368162155\nlstm: 0.07043729722499847\ncnn: 0.12144634127616882\n2024-09-30 12:57:20.209 python[90113:7706932] +[IMKClient subclass]: chose IMKClient_Legacy\n2024-09-30 12:57:20.209 python[90113:7706932] +[IMKInputSession subclass]: chose IMKInputSession_Legacy\n",
  "history_begin_time" : 1727715421878,
  "history_end_time" : 1727715554194,
  "history_notes" : null,
  "history_process" : "vnrvoi",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "u8xm4dbfvp3",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1727715414206,
  "history_end_time" : 1727715414206,
  "history_notes" : null,
  "history_process" : "0lsb3i",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "sv65v83nt5u",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1727715414207,
  "history_end_time" : 1727715414207,
  "history_notes" : null,
  "history_process" : "yqiqdt",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "9eb6l92nevu",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1727715414207,
  "history_end_time" : 1727715414207,
  "history_notes" : null,
  "history_process" : "y30gbw",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "q384w540fc0",
  "history_input" : "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\nimport joblib\n\n# Load predictions, actual values, and scaler\npredictions = np.load('/Users/vishesh/gw-workspace/9I3eI9y6wb9B/predictions.npy')\ndata = np.load('/Users/vishesh/gw-workspace/L6gDfkNJofh8/preprocessed_data.npz')\ny_test = data['y_test']\n\n# Load the scaler for inverse transformation\nscaler = joblib.load('/Users/vishesh/gw-workspace/f7bNu3SQ6blN/scaler.pkl')\n\n# Step 1: Inverse scale the predictions and real values\npredicted_values = scaler.inverse_transform(predictions)\nreal_values = scaler.inverse_transform(y_test.reshape(-1, 1))\n\n# Step 2: Evaluation Metrics\nmae = mean_absolute_error(real_values, predicted_values)\nmse = mean_squared_error(real_values, predicted_values)\nrmse = np.sqrt(mse)\nr2 = r2_score(real_values, predicted_values)\n\nprint(f\"Mean Absolute Error (MAE): {mae}\")\nprint(f\"Mean Squared Error (MSE): {mse}\")\nprint(f\"Root Mean Squared Error (RMSE): {rmse}\")\nprint(f\"R-squared (R²): {r2}\")\n\n# Step 3: Error Distribution Plot\nerrors = real_values - predicted_values\nplt.figure(figsize=(10, 6))\nplt.hist(errors, bins=50, color='skyblue', edgecolor='black')\nplt.title('Error Distribution')\nplt.xlabel('Error')\nplt.ylabel('Frequency')\nplt.grid(True)\nplt.show()\n\n# Step 4: Seasonal Error Analysis (if Date information is available)\ndate_series = pd.to_datetime(data['Date'], format='%Y%m')\nseasonal_errors = pd.DataFrame({'Date': date_series, 'Error': errors.flatten()})\nseasonal_errors['Month'] = seasonal_errors['Date'].dt.month\n\n# Calculate average error for each month\nmonthly_error = seasonal_errors.groupby('Month')['Error'].mean()\nplt.figure(figsize=(10, 6))\nmonthly_error.plot(kind='bar', color='coral', edgecolor='black')\nplt.title('Average Error by Month')\nplt.xlabel('Month')\nplt.ylabel('Average Error')\nplt.grid(True)\nplt.show()\n\n# Step 5: Backtesting\n# Here, a rolling forecast is performed by retraining the model iteratively. This is a placeholder implementation.\n# Replace this with your specific model training code.\nbacktest_steps = 12  # Example: 12 months\nbacktest_real = []\nbacktest_pred = []\n\nfor i in range(backtest_steps, len(real_values)):\n    # Perform training using historical data up to the current point\n    current_train_data = real_values[:i]\n    # (Insert model training code here)\n    # Predict the next step\n    current_prediction = predicted_values[i]  # Replace with model's predicted output\n    backtest_real.append(real_values[i])\n    backtest_pred.append(current_prediction)\n\n# Visualizing backtest results\nplt.figure(figsize=(10, 6))\nplt.plot(backtest_real, color='blue', label='Real Values')\nplt.plot(backtest_pred, color='red', label='Backtest Predictions')\nplt.title('Backtesting Real vs Predicted Values')\nplt.xlabel('Time')\nplt.ylabel('Temperature')\nplt.legend()\nplt.show()\n",
  "history_output" : "Mean Absolute Error (MAE): 0.26507065919969897\nMean Squared Error (MSE): 0.11343795708910351\nRoot Mean Squared Error (RMSE): 0.33680551819871285\nR-squared (R²): 0.8375067339665936\n2024-09-30 12:59:17.435 python[90245:7714037] +[IMKClient subclass]: chose IMKClient_Legacy\n2024-09-30 12:59:17.435 python[90245:7714037] +[IMKInputSession subclass]: chose IMKInputSession_Legacy\nTraceback (most recent call last):\n  File \"/Users/vishesh/gw-workspace/q384w540fc0/model_eval.py\", line 41, in <module>\n    date_series = pd.to_datetime(data['Date'], format='%Y%m')\n                                 ~~~~^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/numpy/lib/npyio.py\", line 263, in __getitem__\n    raise KeyError(f\"{key} is not a file in the archive\")\nKeyError: 'Date is not a file in the archive'\n",
  "history_begin_time" : 1727715555667,
  "history_end_time" : 1727715568209,
  "history_notes" : null,
  "history_process" : "7ev3w3",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "pw702ssssia",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1727715414209,
  "history_end_time" : 1727715414209,
  "history_notes" : null,
  "history_process" : "tz54ub",
  "host_id" : "100001",
  "indicator" : "Skipped"
}]
