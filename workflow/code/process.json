[{
  "id" : "9cqdos",
  "name" : "data_acquisition",
  "description" : "python",
  "code" : "No code saved",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "iq85px",
  "name" : "data_preprocessing",
  "description" : null,
  "code" : "# import pandas as pd\n# from sklearn.preprocessing import StandardScaler\n\n# def preprocess_data(temp_file, precip_file):\n#     # Load datasets\n#     temp_data = pd.read_csv(temp_file, skiprows=4)\n#     precip_data = pd.read_csv(precip_file, skiprows=4)\n    \n#     # Rename columns for easier access\n#     temp_data.columns = ['Year', 'Temp_Value', 'Temp_Anomaly', 'Temp_Uncertainty']\n#     precip_data.columns = ['Year', 'Precipitation', 'Uncertainty']\n    \n#     # Merge the datasets on Year\n#     merged_data = pd.merge(temp_data, precip_data, on='Year')\n    \n#     # Fill missing values\n#     merged_data.fillna(method='ffill', inplace=True)\n    \n#     # Normalize the features\n#     scaler = StandardScaler()\n#     scaled_data = pd.DataFrame(scaler.fit_transform(merged_data[['Temp_Value', 'Precipitation']]), \n#                                columns=['Temp_Value', 'Precipitation'])\n#     scaled_data['Year'] = merged_data['Year']\n    \n#     # Save the preprocessed data\n#     scaled_data.to_csv('preprocessed_climate_data.csv', index=False)\n#     print(\"Preprocessed data saved as preprocessed_climate_data.csv\")\n\n# if __name__ == \"__main__\":\n#     preprocess_data('/Users/vishesh/Desktop/geo_project/data.csv', 'precipitation_data.csv')\n\n\n\n\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\n\ndef preprocess_data(temp_file, precip_file):\n    # Load datasets\n    temp_data = pd.read_csv(temp_file, skiprows=4)\n    precip_data = pd.read_csv(precip_file, skiprows=4)\n    \n    # Automatically rename columns based on the dataset (assumes first column is Year, and others are feature values)\n    temp_data.columns = ['Year'] + list(temp_data.columns[1:])\n    precip_data.columns = ['Year'] + list(precip_data.columns[1:])\n    \n    # Merge the datasets on 'Year' column\n    merged_data = pd.merge(temp_data, precip_data, on='Year')\n    \n    # Fill missing values (forward fill method)\n    merged_data.ffill(inplace=True)\n    \n    # Select all columns except 'Year' for normalization\n    features = merged_data.drop(columns=['Year'])\n    \n    # Normalize all feature columns\n    scaler = StandardScaler()\n    scaled_features = pd.DataFrame(scaler.fit_transform(features), columns=features.columns)\n    \n    # Add 'Year' column back to the scaled dataset\n    scaled_data = pd.concat([merged_data[['Year']], scaled_features], axis=1)\n    \n    # Save the preprocessed data\n    scaled_data.to_csv('preprocessed_climate_data.csv', index=False)\n    print(\"Preprocessed data saved as preprocessed_climate_data.csv\")\n\nif __name__ == \"__main__\":\n    preprocess_data('/Users/vishesh/Desktop/geo_project/data.csv', '/Users/vishesh/Desktop/geo_project/data-2.csv')\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "c8851t",
  "name" : "trend_analysis",
  "description" : null,
  "code" : "No code saved",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "s20a2d",
  "name" : "ml_model",
  "description" : null,
  "code" : "\n\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM, Dense, Dropout\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.regularizers import l2\nimport joblib\n\n# Step 1: Load the preprocessed data\ndata = np.load('/Users/vishesh/gw-workspace/L6gDfkNJofh8/preprocessed_data.npz')\nX_train, X_test, y_train, y_test = data['X_train'], data['X_test'], data['y_train'], data['y_test']\n\n# Step 2: Build the LSTM model\nmodel = Sequential()\nmodel.add(LSTM(units=50, return_sequences=True, input_shape=(X_train.shape[1], 1), \n               kernel_regularizer=l2(0.001)))  # L2 regularization\nmodel.add(Dropout(0.2))  # Increased dropout rate\nmodel.add(LSTM(units=50, return_sequences=False, kernel_regularizer=l2(0.001)))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(units=1))\n\n# Step 3: Compile the model with reduced learning rate\noptimizer = tf.keras.optimizers.Adam(learning_rate=0.0003)  # Reduced learning rate\nmodel.compile(optimizer=optimizer, loss='mean_squared_error')\n\n# Step 4: Train the model with early stopping\nearly_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n\nhistory = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test), \n                    callbacks=[early_stopping])\n\n# Step 5: Save the model and the training history\nmodel.save('lstm_model.h5')\nnp.save('training_history.npy', history.history)\n\n# Step 6: Make predictions on the test data\npredictions = model.predict(X_test)\n\n# Save predictions for later use\nnp.save('predictions.npy', predictions)\n\nprint(\"Model training completed, predictions made and saved!\")\n\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "xnk7n1",
  "name" : "anomaly_detection",
  "description" : null,
  "code" : "No code saved",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "x4ch0z",
  "name" : "visualization",
  "description" : null,
  "code" : "# import pandas as pd\n# import matplotlib.pyplot as plt\n# import seaborn as sns\n\n# def visualize(file_name, predictions_file, anomalies_file):\n#     data = pd.read_csv(file_name)\n#     predictions = pd.read_csv(predictions_file)\n#     anomalies = pd.read_csv(anomalies_file)\n    \n#     plt.figure(figsize=(12, 8))\n    \n#     # Plotting temperature trends with predictions\n#     plt.subplot(2, 1, 1)\n#     plt.plot(data['Year'], data['Temp_Value'], label='Observed Temperature')\n#     plt.plot(predictions['Year'], predictions['Temp_Prediction'], label='Predicted Temperature', linestyle='--')\n#     plt.title('Temperature Trends and Predictions')\n#     plt.xlabel('Year')\n#     plt.ylabel('Temperature (Â°C)')\n#     plt.legend()\n    \n#     # Plotting precipitation trends with predictions\n#     plt.subplot(2, 1, 2)\n#     plt.plot(data['Year'], data['Precipitation'], label='Observed Precipitation')\n#     plt.plot(predictions['Year'], predictions['Precip_Prediction'], label='Predicted Precipitation', linestyle='--')\n#     plt.title('Precipitation Trends and Predictions')\n#     plt.xlabel('Year')\n#     plt.ylabel('Precipitation (mm)')\n#     plt.legend()\n    \n#     # Highlighting anomalies\n#     sns.scatterplot(data=anomalies, x='Year', y='Temp_Value', hue='Cluster', palette='deep', legend='full', s=100)\n    \n#     plt.tight_layout()\n#     plt.savefig('climate_trends_and_anomalies.png')\n#     plt.show()\n\n# if __name__ == \"__main__\":\n#     visualize('/Users/vishesh/Desktop/geo_project/analyzed_climate_data.csv', '/Users/vishesh/Desktop/geo_project/predicted_climate_data.csv', '/Users/vishesh/Desktop/geo_project/anomaly_detected_data.csv')\n\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport joblib\n\n# Step 1: Load predictions, actual values, and scaler\npredictions = np.load('/Users/vishesh/gw-workspace/9I3eI9y6wb9B/predictions.npy')\ndata = np.load('/Users/vishesh/gw-workspace/L6gDfkNJofh8/preprocessed_data.npz')\ny_test = data['y_test']\n\n# Load the scaler for inverse transformation\nscaler = joblib.load('/Users/vishesh/gw-workspace/f7bNu3SQ6blN/scaler.pkl')\n\n# Step 2: Inverse scale the predictions and real values\npredicted_values = scaler.inverse_transform(predictions)\nreal_values = scaler.inverse_transform(y_test.reshape(-1, 1))\n\n# Step 3: Visualize the loss over epochs and print loss values\nhistory = np.load('/Users/vishesh/gw-workspace/9I3eI9y6wb9B/training_history.npy', allow_pickle=True).item()\n\n# Print the loss values after each epoch\ntrain_loss = history['loss']\nval_loss = history['val_loss']\n\nprint(\"Epoch-wise Losses:\")\nfor i in range(len(train_loss)):\n    print(f\"Epoch {i+1}: Train Loss = {train_loss[i]}, Validation Loss = {val_loss[i]}\")\n\n# Plot the loss curves\nplt.plot(train_loss, label='Train Loss')\nplt.plot(val_loss, label='Validation Loss')\nplt.title('Model Loss over Epochs')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()\n\n# Step 4: Visualize the predicted vs actual values\nplt.plot(real_values, color='blue', label='Real Temperature Values')\nplt.plot(predicted_values, color='red', label='Predicted Temperature Values')\nplt.title('Real vs Predicted Temperature Values')\nplt.xlabel('Time')\nplt.ylabel('Temperature')\nplt.legend()\nplt.show()\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "keghqi",
  "name" : "load_temp",
  "description" : null,
  "code" : "import numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nimport joblib\nimport matplotlib.pyplot as plt\n\n# Step 1: Load the dataset\nfile_path = '/Users/vishesh/Desktop/geo_project/data.csv'\ndata = pd.read_csv(file_path, skiprows=3)\n\n# Step 2: Preprocess the data\ndata['Date'] = pd.to_datetime(data['Date'], format='%Y%m')\nvalues = data['Value'].values\n\n# Step 3: Normalize the data for LSTM\nscaler = MinMaxScaler(feature_range=(0, 1))\nscaled_data = scaler.fit_transform(values.reshape(-1, 1))\n\n# Save the scaler for inverse transformation later\njoblib.dump(scaler, 'scaler.pkl')\n\n# Step 4: Create sequences for LSTM\ndef create_sequences(data, sequence_length):\n    sequences = []\n    targets = []\n    for i in range(len(data) - sequence_length):\n        seq = data[i:i + sequence_length]\n        target = data[i + sequence_length]\n        sequences.append(seq)\n        targets.append(target)\n    return np.array(sequences), np.array(targets)\n\nsequence_length = 12  # 12 months\nX, y = create_sequences(scaled_data, sequence_length)\n\n# Step 5: Split into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n\n# Reshape the data for LSTM\nX_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\nX_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n\n# Step 6: Save preprocessed data for later use\nnp.savez('preprocessed_data.npz', X_train=X_train, X_test=X_test, y_train=y_train, y_test=y_test)\n\nprint(\"Data preprocessing completed and saved!\")\n\n\n\nplt.figure(figsize=(10,6))\nplt.plot(data['Date'], data['Value'], label='Temperature Values')\nplt.title('Temperature Values Over Time')\nplt.xlabel('Date')\nplt.ylabel('Temperature (Degrees Fahrenheit)')\nplt.grid(True)\nplt.legend()\nplt.show()\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "fhhx69",
  "name" : "low_precipitation",
  "description" : null,
  "code" : "No code saved",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "rfwos9",
  "name" : "evaluate_model",
  "description" : null,
  "code" : "import numpy as np\nimport tensorflow as tf\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nimport joblib\nimport matplotlib.pyplot as plt\n\n# Step 1: Load the saved model\nmodel = tf.keras.models.load_model('lstm_model.h5')\n\n# Step 2: Load the preprocessed data (X_test and y_test)\ndata = np.load('preprocessed_data.npz')\nX_test, y_test = data['X_test'], data['y_test']\n\n# Step 3: Load the scaler for inverse transformation\nscaler = joblib.load('scaler.pkl')\n\n# Step 4: Make predictions on the test data\npredictions = model.predict(X_test)\n\n# Step 5: Inverse transform the predictions and the actual values (y_test)\npredicted_values = scaler.inverse_transform(predictions)\nreal_values = scaler.inverse_transform(y_test.reshape(-1, 1))\n\n# Step 6: Calculate evaluation metrics\nmse = mean_squared_error(real_values, predicted_values)\nmae = mean_absolute_error(real_values, predicted_values)\n\nprint(f\"Mean Squared Error (MSE): {mse}\")\nprint(f\"Mean Absolute Error (MAE): {mae}\")\n\n# Step 7: Plot the predicted vs actual values\nplt.figure(figsize=(10,6))\nplt.plot(real_values, color='blue', label='Actual Values')\nplt.plot(predicted_values, color='red', label='Predicted Values')\nplt.title('Actual vs Predicted Values')\nplt.xlabel('Time')\nplt.ylabel('Values')\nplt.legend()\nplt.grid(True)\nplt.show()\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
},{
  "id" : "vnrvoi",
  "name" : "auto_ml",
  "description" : null,
  "code" : "import numpy as np\nimport optuna\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import Dense, Dropout, Conv1D, MaxPooling1D, Flatten, LSTM, Input, MultiHeadAttention, LayerNormalization, Add\nfrom tensorflow.keras.optimizers import Adam\nfrom pytorch_tabnet.tab_model import TabNetRegressor\n\n# Load preprocessed data\ndata_path = '/Users/vishesh/gw-workspace/L6gDfkNJofh8/preprocessed_data.npz'\ndata = np.load(data_path)\n\n# List all variable names in the .npz file to identify contents\nprint(\"Variables in the .npz file:\", data.files)\n\n# Load data based on contents of the .npz file\nX_train = data['X_train']\ny_train = data['y_train']\n\n# Check if 'X_val' and 'y_val' exist; if not, create validation split\nif 'X_val' in data.files and 'y_val' in data.files:\n    X_val = data['X_val']\n    y_val = data['y_val']\nelse:\n    # Split the data into training and validation sets\n    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n\n# Initialize a dictionary to store model performance\nmodel_performance = {}\n\n# Define the function to create various models\ndef create_model(trial):\n    input_shape = X_train.shape[1]\n    model_type = trial.suggest_categorical('model_type', ['dense', 'cnn', 'lstm', 'transformer'])\n    # model_type = trial.suggest_categorical('model_type', ['dense', 'cnn', 'lstm', 'transformer', 'tabnet'])\n\n    if model_type == 'dense':\n        model = Sequential()\n        model.add(Input(shape=(input_shape,)))  # Use Input layer to specify input shape\n        num_layers = trial.suggest_int('num_layers', 1, 5)\n        for i in range(num_layers):\n            num_units = trial.suggest_int(f'num_units_l{i}', 16, 128, log=True)\n            model.add(Dense(num_units, activation='relu'))\n            dropout_rate = trial.suggest_float(f'dropout_rate_l{i}', 0.1, 0.5)\n            model.add(Dropout(dropout_rate))\n        model.add(Dense(1, activation='linear'))\n    \n    elif model_type == 'cnn':\n        model = Sequential()\n        model.add(Input(shape=(input_shape, 1)))  # Use Input layer for CNN input shape\n        model.add(Conv1D(filters=trial.suggest_int('filters', 16, 64, log=True),\n                         kernel_size=trial.suggest_int('kernel_size', 3, 5),\n                         activation='relu'))\n        model.add(MaxPooling1D(pool_size=2))\n        model.add(Flatten())\n        model.add(Dense(1, activation='linear'))\n    \n    elif model_type == 'lstm':\n        model = Sequential()\n        model.add(Input(shape=(input_shape, 1)))  # Use Input layer for LSTM input shape\n        model.add(LSTM(units=trial.suggest_int('lstm_units', 16, 64, log=True)))\n        model.add(Dense(1, activation='linear'))\n    \n    elif model_type == 'transformer':\n        inputs = Input(shape=(input_shape, 1))\n        attention = MultiHeadAttention(num_heads=trial.suggest_int('num_heads', 2, 8), key_dim=trial.suggest_int('key_dim', 16, 64))(inputs, inputs)\n        attention = Add()([inputs, attention])\n        attention = LayerNormalization(epsilon=1e-6)(attention)\n        outputs = Flatten()(attention)\n        outputs = Dense(1, activation='linear')(outputs)\n        model = Model(inputs=inputs, outputs=outputs)\n    \n    # elif model_type == 'tabnet':\n    #     tabnet_model = TabNetRegressor(\n    #         n_d=trial.suggest_int('n_d', 8, 64),\n    #         n_a=trial.suggest_int('n_a', 8, 64),\n    #         n_steps=trial.suggest_int('n_steps', 3, 10),\n    #         gamma=trial.suggest_float('gamma', 1.0, 2.0),\n    #         lambda_sparse=trial.suggest_float('lambda_sparse', 1e-6, 1e-3)\n    #     )\n    #     tabnet_model.fit(\n    #         X_train, y_train,\n    #         eval_set=[(X_val, y_val)],\n    #         eval_metric=['mae'],\n    #         max_epochs=100,\n    #         patience=10,\n    #         batch_size=256,\n    #         virtual_batch_size=128\n    #     )\n    #     return tabnet_model\n\n    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-2, log=True)\n    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mean_squared_error', metrics=['mean_absolute_error'])\n\n    return model\n\n# Define the objective function for Optuna\ndef objective(trial):\n    model = create_model(trial)\n\n    if isinstance(model, TabNetRegressor):\n        model.fit(\n            X_train, y_train,\n            eval_set=[(X_val, y_val)],\n            eval_metric=['mae'],\n            max_epochs=100,\n            patience=10,\n            batch_size=256,\n            virtual_batch_size=128\n        )\n        y_pred = model.predict(X_val)\n        mae = mean_absolute_error(y_val, y_pred)\n    else:\n        model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10, batch_size=32, verbose=0)\n        _, mae = model.evaluate(X_val, y_val, verbose=0)\n\n    model_type = trial.params['model_type']\n    model_performance[model_type] = mae\n    return mae\n\n# Run the Bayesian optimization\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=10)  # Set n_trials as needed\n\n# Get the best hyperparameters\nbest_params = study.best_params\nprint(f\"Best hyperparameters: {best_params}\")\n\n# Display performance of each model\nprint(\"Model performance (MAE):\")\nfor model_type, mae in model_performance.items():\n    print(f\"{model_type}: {mae}\")\n\n# Plot the model performance\nplt.figure(figsize=(10, 5))\nplt.bar(model_performance.keys(), model_performance.values(), color='skyblue')\nplt.xlabel('Model Type')\nplt.ylabel('Mean Absolute Error')\nplt.title('Model Performance Comparison')\nplt.show()\n",
  "lang" : "python",
  "owner" : "111111",
  "confidential" : "FALSE"
}]
