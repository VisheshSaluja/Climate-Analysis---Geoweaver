[{
  "history_id" : "bmxg7mg9uj6",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1727468233376,
  "history_end_time" : 1727468233376,
  "history_notes" : null,
  "history_process" : "9cqdos",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "ktrhblt09f0",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1727468233379,
  "history_end_time" : 1727468233379,
  "history_notes" : null,
  "history_process" : "iq85px",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "yctrhi1969a",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1727468233379,
  "history_end_time" : 1727468233379,
  "history_notes" : null,
  "history_process" : "c8851t",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "gy76le57n1c",
  "history_input" : "# import pandas as pd\n# from sklearn.model_selection import train_test_split\n# from sklearn.linear_model import LinearRegression\n# from sklearn.preprocessing import PolynomialFeatures\n# from sklearn.metrics import mean_squared_error\n\n# def train_predict(file_name, degree=3):\n#     data = pd.read_csv(file_name)\n    \n#     # Use Year as input, Temp_Value and Precipitation as targets\n#     X = data[['Year']]\n#     y_temp = data['Temp_Value']\n#     y_precip = data['Precipitation']\n    \n#     # Save original X_test for use in the final output\n#     X_train_orig, X_test_orig, y_temp_train, y_temp_test, y_precip_train, y_precip_test = train_test_split(\n#         X, y_temp, y_precip, test_size=0.2, random_state=42)\n    \n#     # Polynomial Features Transformation\n#     poly = PolynomialFeatures(degree=degree)\n#     X_poly = poly.fit_transform(X)\n    \n#     # Perform the same train-test split on the transformed data\n#     X_train, X_test, y_temp_train, y_temp_test, y_precip_train, y_precip_test = train_test_split(\n#         X_poly, y_temp, y_precip, test_size=0.2, random_state=42)\n    \n#     # Train Polynomial Regression models for temperature and precipitation\n#     model_temp = LinearRegression().fit(X_train, y_temp_train)\n#     model_precip = LinearRegression().fit(X_train, y_precip_train)\n    \n#     # Make predictions\n#     temp_predictions = model_temp.predict(X_test)\n#     precip_predictions = model_precip.predict(X_test)\n    \n#     # Evaluate the models\n#     temp_mse = mean_squared_error(y_temp_test, temp_predictions)\n#     precip_mse = mean_squared_error(y_precip_test, precip_predictions)\n#     print(f\"Temperature Model MSE: {temp_mse}\")\n#     print(f\"Precipitation Model MSE: {precip_mse}\")\n    \n#     # Save predictions using the original Year from X_test_orig\n#     predictions = pd.DataFrame({'Year': X_test_orig['Year'], 'Temp_Prediction': temp_predictions, \n#                                 'Precip_Prediction': precip_predictions})\n#     predictions.to_csv('/Users/vishesh/Desktop/geo_project/predicted_climate_data.csv', index=False)\n#     print(\"Predictions saved as predicted_climate_data.csv\")\n\n# if __name__ == \"__main__\":\n#     train_predict('/Users/vishesh/Desktop/geo_project/analyzed_climate_data.csv', degree=3)\n##############################\n\n\n# import pandas as pd\n# import numpy as np\n# from sklearn.model_selection import train_test_split\n# from sklearn.preprocessing import MinMaxScaler\n# from tensorflow.keras.models import Sequential\n# from tensorflow.keras.layers import Dense, LSTM, Dropout\n# from tensorflow.keras.optimizers import Adam\n# from tensorflow.keras.callbacks import EarlyStopping\n# from sklearn.metrics import mean_squared_error\n\n# def prepare_lstm_data(X, y, time_steps=30):\n#     Xs, ys = [], []\n#     for i in range(len(X) - time_steps):\n#         Xs.append(X[i:(i + time_steps)])\n#         ys.append(y[i + time_steps])\n#     return np.array(Xs), np.array(ys)\n\n# def train_predict(file_name, time_steps=30, learning_rate=0.0001):\n#     data = pd.read_csv(file_name)\n    \n#     # Use Year as input, Temp_Value and Precipitation as targets\n#     X = data[['Year']]\n#     y_temp = data['Temp_Value'].values\n#     y_precip = data['Precipitation'].values\n    \n#     # Normalize the data using MinMaxScaler\n#     scaler = MinMaxScaler()\n#     X_scaled = scaler.fit_transform(X)\n    \n#     # Prepare data for LSTM (look back by time_steps)\n#     X_temp, y_temp = prepare_lstm_data(X_scaled, y_temp, time_steps)\n#     X_precip, y_precip = prepare_lstm_data(X_scaled, y_precip, time_steps)\n    \n#     # Split data into training and test sets\n#     X_temp_train, X_temp_test, y_temp_train, y_temp_test = train_test_split(\n#         X_temp, y_temp, test_size=0.2, random_state=42)\n    \n#     X_precip_train, X_precip_test, y_precip_train, y_precip_test = train_test_split(\n#         X_precip, y_precip, test_size=0.2, random_state=42)\n    \n#     # Define the LSTM model with more layers, dropout, and lower learning rate\n#     def build_lstm_model():\n#         model = Sequential()\n#         model.add(LSTM(100, return_sequences=True, input_shape=(time_steps, 1)))\n#         model.add(LSTM(100, return_sequences=False))\n#         model.add(Dropout(0.2))  # Adding dropout to prevent overfitting\n#         model.add(Dense(50))\n#         model.add(Dense(1))\n        \n#         # Compile model with a lower learning rate\n#         optimizer = Adam(learning_rate=learning_rate)\n#         model.compile(optimizer=optimizer, loss='mean_squared_error')\n#         return model\n    \n#     # Reshape the data for LSTM (LSTM expects 3D input: [samples, time_steps, features])\n#     X_temp_train = np.reshape(X_temp_train, (X_temp_train.shape[0], X_temp_train.shape[1], 1))\n#     X_temp_test = np.reshape(X_temp_test, (X_temp_test.shape[0], X_temp_test.shape[1], 1))\n    \n#     X_precip_train = np.reshape(X_precip_train, (X_precip_train.shape[0], X_precip_train.shape[1], 1))\n#     X_precip_test = np.reshape(X_precip_test, (X_precip_test.shape[0], X_precip_test.shape[1], 1))\n    \n#     # Early stopping to prevent overfitting\n#     early_stop = EarlyStopping(monitor='loss', patience=5, verbose=1)\n    \n#     # Train LSTM model for temperature prediction\n#     model_temp = build_lstm_model()\n#     model_temp.fit(X_temp_train, y_temp_train, batch_size=64, epochs=100, callbacks=[early_stop], verbose=1)\n    \n#     # Train LSTM model for precipitation prediction\n#     model_precip = build_lstm_model()\n#     model_precip.fit(X_precip_train, y_precip_train, batch_size=64, epochs=100, callbacks=[early_stop], verbose=1)\n    \n#     # Make predictions\n#     temp_predictions = model_temp.predict(X_temp_test)\n#     precip_predictions = model_precip.predict(X_precip_test)\n    \n#     # Evaluate the models\n#     temp_mse = mean_squared_error(y_temp_test, temp_predictions)\n#     precip_mse = mean_squared_error(y_precip_test, precip_predictions)\n#     print(f\"Temperature Model MSE: {temp_mse}\")\n#     print(f\"Precipitation Model MSE: {precip_mse}\")\n    \n#     # Save predictions using the original Year from X_temp_test\n#     predictions = pd.DataFrame({\n#         'Year': scaler.inverse_transform(X_temp_test.reshape(X_temp_test.shape[0], X_temp_test.shape[1]))[:, -1],\n#         'Temp_Prediction': temp_predictions.flatten(),\n#         'Precip_Prediction': precip_predictions.flatten()\n#     })\n    \n#     predictions.to_csv('/Users/vishesh/Desktop/geo_project/predicted_climate_data.csv', index=False)\n#     print(\"Predictions saved as predicted_climate_data.csv\")\n\n# if __name__ == \"__main__\":\n#     train_predict('/Users/vishesh/Desktop/geo_project/analyzed_climate_data.csv', time_steps=30, learning_rate=0.0001)\n\n\n\n########################\n\n# import numpy as np\n# import tensorflow as tf\n# from tensorflow.keras.models import Sequential\n# from tensorflow.keras.layers import LSTM, Dense, Dropout\n# import joblib\n\n# # Step 1: Load the preprocessed data\n# data = np.load('/Users/vishesh/gw-workspace/L6gDfkNJofh8/preprocessed_data.npz')\n# X_train, X_test, y_train, y_test = data['X_train'], data['X_test'], data['y_train'], data['y_test']\n\n# # Step 2: Build the LSTM model\n# model = Sequential()\n# model.add(LSTM(units=50, return_sequences=True, input_shape=(X_train.shape[1], 1)))\n# model.add(Dropout(0.2))\n# model.add(LSTM(units=50, return_sequences=False))\n# model.add(Dropout(0.2))\n# model.add(Dense(units=1))\n\n# # Step 3: Compile the model\n# model.compile(optimizer='adam', loss='mean_squared_error')\n\n# # Step 4: Train the model\n# history = model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_test, y_test))\n\n# # Step 5: Save the model and the training history\n# model.save('lstm_model.h5')\n# np.save('training_history.npy', history.history)\n\n# # Step 6: Make predictions on the test data\n# predictions = model.predict(X_test)\n\n# # Save predictions for later use\n# np.save('predictions.npy', predictions)\n\n# print(\"Model training completed, predictions made and saved!\")\n\n\n\n\n\n#######################################\n\n\n\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM, Dense, Dropout\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.regularizers import l2\nimport joblib\n\n# Step 1: Load the preprocessed data\ndata = np.load('/Users/vishesh/gw-workspace/L6gDfkNJofh8/preprocessed_data.npz')\nX_train, X_test, y_train, y_test = data['X_train'], data['X_test'], data['y_train'], data['y_test']\n\n# Step 2: Build the LSTM model\nmodel = Sequential()\nmodel.add(LSTM(units=50, return_sequences=True, input_shape=(X_train.shape[1], 1), \n               kernel_regularizer=l2(0.001)))  # L2 regularization\nmodel.add(Dropout(0.2))  # Increased dropout rate\nmodel.add(LSTM(units=50, return_sequences=False, kernel_regularizer=l2(0.001)))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(units=1))\n\n# Step 3: Compile the model with reduced learning rate\noptimizer = tf.keras.optimizers.Adam(learning_rate=0.0003)  # Reduced learning rate\nmodel.compile(optimizer=optimizer, loss='mean_squared_error')\n\n# Step 4: Train the model with early stopping\nearly_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n\nhistory = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test), \n                    callbacks=[early_stopping])\n\n# Step 5: Save the model and the training history\nmodel.save('lstm_model.h5')\nnp.save('training_history.npy', history.history)\n\n# Step 6: Make predictions on the test data\npredictions = model.predict(X_test)\n\n# Save predictions for later use\nnp.save('predictions.npy', predictions)\n\nprint(\"Model training completed, predictions made and saved!\")\n\n",
  "history_output" : "/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(**kwargs)\nEpoch 1/50\n\u001B[1m 1/39\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m28s\u001B[0m 756ms/step - loss: 0.2803\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m12/39\u001B[0m \u001B[32m━━━━━━\u001B[0m\u001B[37m━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 0.2486   \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m25/39\u001B[0m \u001B[32m━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.2245\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m39/39\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 8ms/step - loss: 0.2014 - val_loss: 0.0898\nEpoch 2/50\n\u001B[1m 1/39\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 10ms/step - loss: 0.0858\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m15/39\u001B[0m \u001B[32m━━━━━━━\u001B[0m\u001B[37m━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0853 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m29/39\u001B[0m \u001B[32m━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0836\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m39/39\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0825 - val_loss: 0.0869\nEpoch 3/50\n\u001B[1m 1/39\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 10ms/step - loss: 0.0766\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m15/39\u001B[0m \u001B[32m━━━━━━━\u001B[0m\u001B[37m━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0715 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m30/39\u001B[0m \u001B[32m━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0704\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m39/39\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0698 - val_loss: 0.0805\nEpoch 4/50\n\u001B[1m 1/39\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 0.0664\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m16/39\u001B[0m \u001B[32m━━━━━━━━\u001B[0m\u001B[37m━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0603\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m31/39\u001B[0m \u001B[32m━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0602\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m39/39\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0599 - val_loss: 0.0718\nEpoch 5/50\n\u001B[1m 1/39\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 0.0540\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m16/39\u001B[0m \u001B[32m━━━━━━━━\u001B[0m\u001B[37m━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0535\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m31/39\u001B[0m \u001B[32m━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0528\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m39/39\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0526 - val_loss: 0.0626\nEpoch 6/50\n\u001B[1m 1/39\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 0.0459\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m16/39\u001B[0m \u001B[32m━━━━━━━━\u001B[0m\u001B[37m━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0466\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m31/39\u001B[0m \u001B[32m━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0461\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m39/39\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0458 - val_loss: 0.0560\nEpoch 7/50\n\u001B[1m 1/39\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 10ms/step - loss: 0.0381\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m15/39\u001B[0m \u001B[32m━━━━━━━\u001B[0m\u001B[37m━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0406 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m30/39\u001B[0m \u001B[32m━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0407\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m39/39\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0405 - val_loss: 0.0496\nEpoch 8/50\n\u001B[1m 1/39\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 10ms/step - loss: 0.0326\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m16/39\u001B[0m \u001B[32m━━━━━━━━\u001B[0m\u001B[37m━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0369 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m31/39\u001B[0m \u001B[32m━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0365\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m39/39\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0362 - val_loss: 0.0412\nEpoch 9/50\n\u001B[1m 1/39\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 0.0328\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m16/39\u001B[0m \u001B[32m━━━━━━━━\u001B[0m\u001B[37m━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0330\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m31/39\u001B[0m \u001B[32m━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0328\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m39/39\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0326 - val_loss: 0.0417\nEpoch 10/50\n\u001B[1m 1/39\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 0.0305\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m15/39\u001B[0m \u001B[32m━━━━━━━\u001B[0m\u001B[37m━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0301\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m23/39\u001B[0m \u001B[32m━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 0.0303\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m38/39\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0301\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m39/39\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 0.0301 - val_loss: 0.0367\nEpoch 11/50\n\u001B[1m 1/39\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 10ms/step - loss: 0.0275\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m15/39\u001B[0m \u001B[32m━━━━━━━\u001B[0m\u001B[37m━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0276 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m29/39\u001B[0m \u001B[32m━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0275\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m39/39\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0273 - val_loss: 0.0425\nEpoch 12/50\n\u001B[1m 1/39\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 10ms/step - loss: 0.0248\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m16/39\u001B[0m \u001B[32m━━━━━━━━\u001B[0m\u001B[37m━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0257 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m31/39\u001B[0m \u001B[32m━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0254\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m39/39\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0252 - val_loss: 0.0334\nEpoch 13/50\n\u001B[1m 1/39\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 12ms/step - loss: 0.0255\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m11/39\u001B[0m \u001B[32m━━━━━\u001B[0m\u001B[37m━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 0.0245 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m24/39\u001B[0m \u001B[32m━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 0.0242\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m38/39\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0239\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m39/39\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 0.0238 - val_loss: 0.0320\nEpoch 14/50\n\u001B[1m 1/39\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 10ms/step - loss: 0.0216\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m15/39\u001B[0m \u001B[32m━━━━━━━\u001B[0m\u001B[37m━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0227 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m29/39\u001B[0m \u001B[32m━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0224\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m39/39\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0220 - val_loss: 0.0285\nEpoch 15/50\n\u001B[1m 1/39\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 10ms/step - loss: 0.0175\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m16/39\u001B[0m \u001B[32m━━━━━━━━\u001B[0m\u001B[37m━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0188 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m32/39\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0191\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m39/39\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0193 - val_loss: 0.0242\nEpoch 16/50\n\u001B[1m 1/39\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 10ms/step - loss: 0.0186\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m17/39\u001B[0m \u001B[32m━━━━━━━━\u001B[0m\u001B[37m━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0198 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m32/39\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0197\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m39/39\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0196 - val_loss: 0.0312\nEpoch 17/50\n\u001B[1m 1/39\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 0.0153\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m13/39\u001B[0m \u001B[32m━━━━━━\u001B[0m\u001B[37m━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0165\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m28/39\u001B[0m \u001B[32m━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0170\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m39/39\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0172 - val_loss: 0.0243\nEpoch 18/50\n\u001B[1m 1/39\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 0.0157\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m16/39\u001B[0m \u001B[32m━━━━━━━━\u001B[0m\u001B[37m━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0181\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m32/39\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0179\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m39/39\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0178 - val_loss: 0.0224\nEpoch 19/50\n\u001B[1m 1/39\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 0.0184\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m16/39\u001B[0m \u001B[32m━━━━━━━━\u001B[0m\u001B[37m━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0176\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m32/39\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0169\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m39/39\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0167 - val_loss: 0.0237\nEpoch 20/50\n\u001B[1m 1/39\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 0.0194\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m17/39\u001B[0m \u001B[32m━━━━━━━━\u001B[0m\u001B[37m━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0157\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m33/39\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0155\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m39/39\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0154 - val_loss: 0.0419\nEpoch 21/50\n\u001B[1m 1/39\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 0.0149\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m17/39\u001B[0m \u001B[32m━━━━━━━━\u001B[0m\u001B[37m━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0150\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m33/39\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0146\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m39/39\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0146 - val_loss: 0.0221\nEpoch 22/50\n\u001B[1m 1/39\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 0.0128\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m16/39\u001B[0m \u001B[32m━━━━━━━━\u001B[0m\u001B[37m━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0129\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m32/39\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0133\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m39/39\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0133 - val_loss: 0.0283\nEpoch 23/50\n\u001B[1m 1/39\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 0.0156\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m17/39\u001B[0m \u001B[32m━━━━━━━━\u001B[0m\u001B[37m━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0146\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m33/39\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0140\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m39/39\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0138 - val_loss: 0.0342\nEpoch 24/50\n\u001B[1m 1/39\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 0.0127\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m16/39\u001B[0m \u001B[32m━━━━━━━━\u001B[0m\u001B[37m━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0130\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m32/39\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0127\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m39/39\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0126 - val_loss: 0.0305\nWARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n\u001B[1m 1/10\u001B[0m \u001B[32m━━\u001B[0m\u001B[37m━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 72ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m10/10\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 15ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m10/10\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 15ms/step\nModel training completed, predictions made and saved!\n",
  "history_begin_time" : 1727468239861,
  "history_end_time" : 1727468247463,
  "history_notes" : null,
  "history_process" : "s20a2d",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "0f6ijofvoej",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1727468233383,
  "history_end_time" : 1727468233383,
  "history_notes" : null,
  "history_process" : "xnk7n1",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "x2q0yts3uxz",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1727468247465,
  "history_notes" : null,
  "history_process" : "x4ch0z",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "r2pozd51lri",
  "history_input" : "import numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nimport joblib\nimport matplotlib.pyplot as plt\n\n# Step 1: Load the dataset\nfile_path = '/Users/vishesh/Desktop/geo_project/data.csv'\ndata = pd.read_csv(file_path, skiprows=3)\n\n# Step 2: Preprocess the data\ndata['Date'] = pd.to_datetime(data['Date'], format='%Y%m')\nvalues = data['Value'].values\n\n# Step 3: Normalize the data for LSTM\nscaler = MinMaxScaler(feature_range=(0, 1))\nscaled_data = scaler.fit_transform(values.reshape(-1, 1))\n\n# Save the scaler for inverse transformation later\njoblib.dump(scaler, 'scaler.pkl')\n\n# Step 4: Create sequences for LSTM\ndef create_sequences(data, sequence_length):\n    sequences = []\n    targets = []\n    for i in range(len(data) - sequence_length):\n        seq = data[i:i + sequence_length]\n        target = data[i + sequence_length]\n        sequences.append(seq)\n        targets.append(target)\n    return np.array(sequences), np.array(targets)\n\nsequence_length = 12  # 12 months\nX, y = create_sequences(scaled_data, sequence_length)\n\n# Step 5: Split into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n\n# Reshape the data for LSTM\nX_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\nX_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n\n# Step 6: Save preprocessed data for later use\nnp.savez('preprocessed_data.npz', X_train=X_train, X_test=X_test, y_train=y_train, y_test=y_test)\n\nprint(\"Data preprocessing completed and saved!\")\n\n\n\nplt.figure(figsize=(10,6))\nplt.plot(data['Date'], data['Value'], label='Temperature Values')\nplt.title('Temperature Values Over Time')\nplt.xlabel('Date')\nplt.ylabel('Temperature (Degrees Fahrenheit)')\nplt.grid(True)\nplt.legend()\nplt.show()\n",
  "history_output" : "Data preprocessing completed and saved!\n2024-09-27 16:17:16.034 python[35490:5731395] +[IMKClient subclass]: chose IMKClient_Legacy\n2024-09-27 16:17:16.034 python[35490:5731395] +[IMKInputSession subclass]: chose IMKInputSession_Legacy\n",
  "history_begin_time" : 1727468233958,
  "history_end_time" : 1727468239211,
  "history_notes" : null,
  "history_process" : "keghqi",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "qzqjbltkh39",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1727468233386,
  "history_end_time" : 1727468233386,
  "history_notes" : null,
  "history_process" : "fhhx69",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "5ossiyw04zz",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1727468233387,
  "history_end_time" : 1727468233387,
  "history_notes" : null,
  "history_process" : "rfwos9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "2vlyjf0rio2",
  "history_input" : "import numpy as np\nimport optuna\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import Dense, Dropout, Conv1D, MaxPooling1D, Flatten, LSTM, Input, MultiHeadAttention, LayerNormalization, Add\nfrom tensorflow.keras.optimizers import Adam\nfrom pytorch_tabnet.tab_model import TabNetRegressor\n\n# Load preprocessed data\ndata_path = '/Users/vishesh/gw-workspace/L6gDfkNJofh8/preprocessed_data.npz'\ndata = np.load(data_path)\n\n# List all variable names in the .npz file to identify contents\nprint(\"Variables in the .npz file:\", data.files)\n\n# Load data based on contents of the .npz file\nX_train = data['X_train']\ny_train = data['y_train']\n\n# Check if 'X_val' and 'y_val' exist; if not, create validation split\nif 'X_val' in data.files and 'y_val' in data.files:\n    X_val = data['X_val']\n    y_val = data['y_val']\nelse:\n    # Split the data into training and validation sets\n    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n\n# Initialize a dictionary to store model performance\nmodel_performance = {}\n\n# Define the function to create various models\ndef create_model(trial):\n    input_shape = X_train.shape[1]\n    model_type = trial.suggest_categorical('model_type', ['dense', 'cnn', 'lstm', 'transformer', 'tabnet'])\n\n    if model_type == 'dense':\n        model = Sequential()\n        model.add(Input(shape=(input_shape,)))  # Use Input layer to specify input shape\n        num_layers = trial.suggest_int('num_layers', 1, 5)\n        for i in range(num_layers):\n            num_units = trial.suggest_int(f'num_units_l{i}', 16, 128, log=True)\n            model.add(Dense(num_units, activation='relu'))\n            dropout_rate = trial.suggest_float(f'dropout_rate_l{i}', 0.1, 0.5)\n            model.add(Dropout(dropout_rate))\n        model.add(Dense(1, activation='linear'))\n    \n    elif model_type == 'cnn':\n        model = Sequential()\n        model.add(Input(shape=(input_shape, 1)))  # Use Input layer for CNN input shape\n        model.add(Conv1D(filters=trial.suggest_int('filters', 16, 64, log=True),\n                         kernel_size=trial.suggest_int('kernel_size', 3, 5),\n                         activation='relu'))\n        model.add(MaxPooling1D(pool_size=2))\n        model.add(Flatten())\n        model.add(Dense(1, activation='linear'))\n    \n    elif model_type == 'lstm':\n        model = Sequential()\n        model.add(Input(shape=(input_shape, 1)))  # Use Input layer for LSTM input shape\n        model.add(LSTM(units=trial.suggest_int('lstm_units', 16, 64, log=True)))\n        model.add(Dense(1, activation='linear'))\n    \n    elif model_type == 'transformer':\n        inputs = Input(shape=(input_shape, 1))\n        attention = MultiHeadAttention(num_heads=trial.suggest_int('num_heads', 2, 8), key_dim=trial.suggest_int('key_dim', 16, 64))(inputs, inputs)\n        attention = Add()([inputs, attention])\n        attention = LayerNormalization(epsilon=1e-6)(attention)\n        outputs = Flatten()(attention)\n        outputs = Dense(1, activation='linear')(outputs)\n        model = Model(inputs=inputs, outputs=outputs)\n    \n    elif model_type == 'tabnet':\n        tabnet_model = TabNetRegressor(\n            n_d=trial.suggest_int('n_d', 8, 64),\n            n_a=trial.suggest_int('n_a', 8, 64),\n            n_steps=trial.suggest_int('n_steps', 3, 10),\n            gamma=trial.suggest_float('gamma', 1.0, 2.0),\n            lambda_sparse=trial.suggest_float('lambda_sparse', 1e-6, 1e-3)\n        )\n        tabnet_model.fit(\n            X_train, y_train,\n            eval_set=[(X_val, y_val)],\n            eval_metric=['mae'],\n            max_epochs=100,\n            patience=10,\n            batch_size=256,\n            virtual_batch_size=128\n        )\n        return tabnet_model\n\n    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-2, log=True)\n    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mean_squared_error', metrics=['mean_absolute_error'])\n\n    return model\n\n# Define the objective function for Optuna\ndef objective(trial):\n    model = create_model(trial)\n\n    if isinstance(model, TabNetRegressor):\n        model.fit(\n            X_train, y_train,\n            eval_set=[(X_val, y_val)],\n            eval_metric=['mae'],\n            max_epochs=100,\n            patience=10,\n            batch_size=256,\n            virtual_batch_size=128\n        )\n        y_pred = model.predict(X_val)\n        mae = mean_absolute_error(y_val, y_pred)\n    else:\n        model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10, batch_size=32, verbose=0)\n        _, mae = model.evaluate(X_val, y_val, verbose=0)\n\n    model_type = trial.params['model_type']\n    model_performance[model_type] = mae\n    return mae\n\n# Run the Bayesian optimization\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=10)  # Set n_trials as needed\n\n# Get the best hyperparameters\nbest_params = study.best_params\nprint(f\"Best hyperparameters: {best_params}\")\n\n# Display performance of each model\nprint(\"Model performance (MAE):\")\nfor model_type, mae in model_performance.items():\n    print(f\"{model_type}: {mae}\")\n\n# Plot the model performance\nplt.figure(figsize=(10, 5))\nplt.bar(model_performance.keys(), model_performance.values(), color='skyblue')\nplt.xlabel('Model Type')\nplt.ylabel('Mean Absolute Error')\nplt.title('Model Performance Comparison')\nplt.show()\n",
  "history_output" : "Variables in the .npz file: ['X_train', 'X_test', 'y_train', 'y_test']\n[I 2024-09-27 16:17:19,547] A new study created in memory with name: no-name-a78612d9-4821-48e1-9658-bbc07ff15bdd\n[I 2024-09-27 16:17:21,432] Trial 0 finished with value: 0.09966925531625748 and parameters: {'model_type': 'transformer', 'num_heads': 7, 'key_dim': 63, 'learning_rate': 0.0010772565853657305}. Best is trial 0 with value: 0.09966925531625748.\n[I 2024-09-27 16:17:22,271] Trial 1 finished with value: 0.10364071279764175 and parameters: {'model_type': 'dense', 'num_layers': 4, 'num_units_l0': 45, 'dropout_rate_l0': 0.29706023895014866, 'num_units_l1': 19, 'dropout_rate_l1': 0.17102353149779398, 'num_units_l2': 29, 'dropout_rate_l2': 0.11857037775772361, 'num_units_l3': 100, 'dropout_rate_l3': 0.40609896068131157, 'learning_rate': 0.0002848917992839276}. Best is trial 0 with value: 0.09966925531625748.\n[I 2024-09-27 16:17:23,592] Trial 2 finished with value: 0.12180174142122269 and parameters: {'model_type': 'transformer', 'num_heads': 5, 'key_dim': 34, 'learning_rate': 0.0003502869857149193}. Best is trial 0 with value: 0.09966925531625748.\n[I 2024-09-27 16:17:24,630] Trial 3 finished with value: 0.06288791447877884 and parameters: {'model_type': 'lstm', 'lstm_units': 23, 'learning_rate': 0.0008689753971346424}. Best is trial 3 with value: 0.06288791447877884.\n/opt/anaconda3/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n  warnings.warn(f\"Device used : {self.device}\")\n[W 2024-09-27 16:17:24,633] Trial 4 failed with parameters: {'model_type': 'tabnet', 'n_d': 29, 'n_a': 34, 'n_steps': 5, 'gamma': 1.2488469032940572, 'lambda_sparse': 0.00035443179712279485} because of the following error: ValueError('Found array with dim 3. None expected <= 2.').\nTraceback (most recent call last):\n  File \"/opt/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n    value_or_values = func(trial)\n                      ^^^^^^^^^^^\n  File \"/Users/vishesh/gw-workspace/2vlyjf0rio2/auto_ml.py\", line 100, in objective\n    model = create_model(trial)\n            ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/vishesh/gw-workspace/2vlyjf0rio2/auto_ml.py\", line 82, in create_model\n    tabnet_model.fit(\n  File \"/opt/anaconda3/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py\", line 217, in fit\n    check_input(X_train)\n  File \"/opt/anaconda3/lib/python3.11/site-packages/pytorch_tabnet/utils.py\", line 507, in check_input\n    check_array(X, accept_sparse=True)\n  File \"/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py\", line 915, in check_array\n    raise ValueError(\nValueError: Found array with dim 3. None expected <= 2.\n[W 2024-09-27 16:17:24,635] Trial 4 failed with value None.\nTraceback (most recent call last):\n  File \"/Users/vishesh/gw-workspace/2vlyjf0rio2/auto_ml.py\", line 124, in <module>\n    study.optimize(objective, n_trials=10)  # Set n_trials as needed\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/optuna/study/study.py\", line 475, in optimize\n    _optimize(\n  File \"/opt/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 63, in _optimize\n    _optimize_sequential(\n  File \"/opt/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 160, in _optimize_sequential\n    frozen_trial = _run_trial(study, func, catch)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 248, in _run_trial\n    raise func_err\n  File \"/opt/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n    value_or_values = func(trial)\n                      ^^^^^^^^^^^\n  File \"/Users/vishesh/gw-workspace/2vlyjf0rio2/auto_ml.py\", line 100, in objective\n    model = create_model(trial)\n            ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/vishesh/gw-workspace/2vlyjf0rio2/auto_ml.py\", line 82, in create_model\n    tabnet_model.fit(\n  File \"/opt/anaconda3/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py\", line 217, in fit\n    check_input(X_train)\n  File \"/opt/anaconda3/lib/python3.11/site-packages/pytorch_tabnet/utils.py\", line 507, in check_input\n    check_array(X, accept_sparse=True)\n  File \"/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py\", line 915, in check_array\n    raise ValueError(\nValueError: Found array with dim 3. None expected <= 2.\n",
  "history_begin_time" : 1727468234600,
  "history_end_time" : 1727468245338,
  "history_notes" : null,
  "history_process" : "vnrvoi",
  "host_id" : "100001",
  "indicator" : "Failed"
}]
