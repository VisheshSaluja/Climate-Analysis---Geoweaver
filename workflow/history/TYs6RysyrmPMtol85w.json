[{
  "history_id" : "6l3pcib77f5",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1727468284957,
  "history_end_time" : 1727468284957,
  "history_notes" : null,
  "history_process" : "9cqdos",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "dnstzcq4uyn",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1727468284959,
  "history_end_time" : 1727468284959,
  "history_notes" : null,
  "history_process" : "iq85px",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "jz8nvxwisu5",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1727468284960,
  "history_end_time" : 1727468284960,
  "history_notes" : null,
  "history_process" : "c8851t",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "vuau7yg9x82",
  "history_input" : "# import pandas as pd\n# from sklearn.model_selection import train_test_split\n# from sklearn.linear_model import LinearRegression\n# from sklearn.preprocessing import PolynomialFeatures\n# from sklearn.metrics import mean_squared_error\n\n# def train_predict(file_name, degree=3):\n#     data = pd.read_csv(file_name)\n    \n#     # Use Year as input, Temp_Value and Precipitation as targets\n#     X = data[['Year']]\n#     y_temp = data['Temp_Value']\n#     y_precip = data['Precipitation']\n    \n#     # Save original X_test for use in the final output\n#     X_train_orig, X_test_orig, y_temp_train, y_temp_test, y_precip_train, y_precip_test = train_test_split(\n#         X, y_temp, y_precip, test_size=0.2, random_state=42)\n    \n#     # Polynomial Features Transformation\n#     poly = PolynomialFeatures(degree=degree)\n#     X_poly = poly.fit_transform(X)\n    \n#     # Perform the same train-test split on the transformed data\n#     X_train, X_test, y_temp_train, y_temp_test, y_precip_train, y_precip_test = train_test_split(\n#         X_poly, y_temp, y_precip, test_size=0.2, random_state=42)\n    \n#     # Train Polynomial Regression models for temperature and precipitation\n#     model_temp = LinearRegression().fit(X_train, y_temp_train)\n#     model_precip = LinearRegression().fit(X_train, y_precip_train)\n    \n#     # Make predictions\n#     temp_predictions = model_temp.predict(X_test)\n#     precip_predictions = model_precip.predict(X_test)\n    \n#     # Evaluate the models\n#     temp_mse = mean_squared_error(y_temp_test, temp_predictions)\n#     precip_mse = mean_squared_error(y_precip_test, precip_predictions)\n#     print(f\"Temperature Model MSE: {temp_mse}\")\n#     print(f\"Precipitation Model MSE: {precip_mse}\")\n    \n#     # Save predictions using the original Year from X_test_orig\n#     predictions = pd.DataFrame({'Year': X_test_orig['Year'], 'Temp_Prediction': temp_predictions, \n#                                 'Precip_Prediction': precip_predictions})\n#     predictions.to_csv('/Users/vishesh/Desktop/geo_project/predicted_climate_data.csv', index=False)\n#     print(\"Predictions saved as predicted_climate_data.csv\")\n\n# if __name__ == \"__main__\":\n#     train_predict('/Users/vishesh/Desktop/geo_project/analyzed_climate_data.csv', degree=3)\n##############################\n\n\n# import pandas as pd\n# import numpy as np\n# from sklearn.model_selection import train_test_split\n# from sklearn.preprocessing import MinMaxScaler\n# from tensorflow.keras.models import Sequential\n# from tensorflow.keras.layers import Dense, LSTM, Dropout\n# from tensorflow.keras.optimizers import Adam\n# from tensorflow.keras.callbacks import EarlyStopping\n# from sklearn.metrics import mean_squared_error\n\n# def prepare_lstm_data(X, y, time_steps=30):\n#     Xs, ys = [], []\n#     for i in range(len(X) - time_steps):\n#         Xs.append(X[i:(i + time_steps)])\n#         ys.append(y[i + time_steps])\n#     return np.array(Xs), np.array(ys)\n\n# def train_predict(file_name, time_steps=30, learning_rate=0.0001):\n#     data = pd.read_csv(file_name)\n    \n#     # Use Year as input, Temp_Value and Precipitation as targets\n#     X = data[['Year']]\n#     y_temp = data['Temp_Value'].values\n#     y_precip = data['Precipitation'].values\n    \n#     # Normalize the data using MinMaxScaler\n#     scaler = MinMaxScaler()\n#     X_scaled = scaler.fit_transform(X)\n    \n#     # Prepare data for LSTM (look back by time_steps)\n#     X_temp, y_temp = prepare_lstm_data(X_scaled, y_temp, time_steps)\n#     X_precip, y_precip = prepare_lstm_data(X_scaled, y_precip, time_steps)\n    \n#     # Split data into training and test sets\n#     X_temp_train, X_temp_test, y_temp_train, y_temp_test = train_test_split(\n#         X_temp, y_temp, test_size=0.2, random_state=42)\n    \n#     X_precip_train, X_precip_test, y_precip_train, y_precip_test = train_test_split(\n#         X_precip, y_precip, test_size=0.2, random_state=42)\n    \n#     # Define the LSTM model with more layers, dropout, and lower learning rate\n#     def build_lstm_model():\n#         model = Sequential()\n#         model.add(LSTM(100, return_sequences=True, input_shape=(time_steps, 1)))\n#         model.add(LSTM(100, return_sequences=False))\n#         model.add(Dropout(0.2))  # Adding dropout to prevent overfitting\n#         model.add(Dense(50))\n#         model.add(Dense(1))\n        \n#         # Compile model with a lower learning rate\n#         optimizer = Adam(learning_rate=learning_rate)\n#         model.compile(optimizer=optimizer, loss='mean_squared_error')\n#         return model\n    \n#     # Reshape the data for LSTM (LSTM expects 3D input: [samples, time_steps, features])\n#     X_temp_train = np.reshape(X_temp_train, (X_temp_train.shape[0], X_temp_train.shape[1], 1))\n#     X_temp_test = np.reshape(X_temp_test, (X_temp_test.shape[0], X_temp_test.shape[1], 1))\n    \n#     X_precip_train = np.reshape(X_precip_train, (X_precip_train.shape[0], X_precip_train.shape[1], 1))\n#     X_precip_test = np.reshape(X_precip_test, (X_precip_test.shape[0], X_precip_test.shape[1], 1))\n    \n#     # Early stopping to prevent overfitting\n#     early_stop = EarlyStopping(monitor='loss', patience=5, verbose=1)\n    \n#     # Train LSTM model for temperature prediction\n#     model_temp = build_lstm_model()\n#     model_temp.fit(X_temp_train, y_temp_train, batch_size=64, epochs=100, callbacks=[early_stop], verbose=1)\n    \n#     # Train LSTM model for precipitation prediction\n#     model_precip = build_lstm_model()\n#     model_precip.fit(X_precip_train, y_precip_train, batch_size=64, epochs=100, callbacks=[early_stop], verbose=1)\n    \n#     # Make predictions\n#     temp_predictions = model_temp.predict(X_temp_test)\n#     precip_predictions = model_precip.predict(X_precip_test)\n    \n#     # Evaluate the models\n#     temp_mse = mean_squared_error(y_temp_test, temp_predictions)\n#     precip_mse = mean_squared_error(y_precip_test, precip_predictions)\n#     print(f\"Temperature Model MSE: {temp_mse}\")\n#     print(f\"Precipitation Model MSE: {precip_mse}\")\n    \n#     # Save predictions using the original Year from X_temp_test\n#     predictions = pd.DataFrame({\n#         'Year': scaler.inverse_transform(X_temp_test.reshape(X_temp_test.shape[0], X_temp_test.shape[1]))[:, -1],\n#         'Temp_Prediction': temp_predictions.flatten(),\n#         'Precip_Prediction': precip_predictions.flatten()\n#     })\n    \n#     predictions.to_csv('/Users/vishesh/Desktop/geo_project/predicted_climate_data.csv', index=False)\n#     print(\"Predictions saved as predicted_climate_data.csv\")\n\n# if __name__ == \"__main__\":\n#     train_predict('/Users/vishesh/Desktop/geo_project/analyzed_climate_data.csv', time_steps=30, learning_rate=0.0001)\n\n\n\n########################\n\n# import numpy as np\n# import tensorflow as tf\n# from tensorflow.keras.models import Sequential\n# from tensorflow.keras.layers import LSTM, Dense, Dropout\n# import joblib\n\n# # Step 1: Load the preprocessed data\n# data = np.load('/Users/vishesh/gw-workspace/L6gDfkNJofh8/preprocessed_data.npz')\n# X_train, X_test, y_train, y_test = data['X_train'], data['X_test'], data['y_train'], data['y_test']\n\n# # Step 2: Build the LSTM model\n# model = Sequential()\n# model.add(LSTM(units=50, return_sequences=True, input_shape=(X_train.shape[1], 1)))\n# model.add(Dropout(0.2))\n# model.add(LSTM(units=50, return_sequences=False))\n# model.add(Dropout(0.2))\n# model.add(Dense(units=1))\n\n# # Step 3: Compile the model\n# model.compile(optimizer='adam', loss='mean_squared_error')\n\n# # Step 4: Train the model\n# history = model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_test, y_test))\n\n# # Step 5: Save the model and the training history\n# model.save('lstm_model.h5')\n# np.save('training_history.npy', history.history)\n\n# # Step 6: Make predictions on the test data\n# predictions = model.predict(X_test)\n\n# # Save predictions for later use\n# np.save('predictions.npy', predictions)\n\n# print(\"Model training completed, predictions made and saved!\")\n\n\n\n\n\n#######################################\n\n\n\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM, Dense, Dropout\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.regularizers import l2\nimport joblib\n\n# Step 1: Load the preprocessed data\ndata = np.load('/Users/vishesh/gw-workspace/L6gDfkNJofh8/preprocessed_data.npz')\nX_train, X_test, y_train, y_test = data['X_train'], data['X_test'], data['y_train'], data['y_test']\n\n# Step 2: Build the LSTM model\nmodel = Sequential()\nmodel.add(LSTM(units=50, return_sequences=True, input_shape=(X_train.shape[1], 1), \n               kernel_regularizer=l2(0.001)))  # L2 regularization\nmodel.add(Dropout(0.2))  # Increased dropout rate\nmodel.add(LSTM(units=50, return_sequences=False, kernel_regularizer=l2(0.001)))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(units=1))\n\n# Step 3: Compile the model with reduced learning rate\noptimizer = tf.keras.optimizers.Adam(learning_rate=0.0003)  # Reduced learning rate\nmodel.compile(optimizer=optimizer, loss='mean_squared_error')\n\n# Step 4: Train the model with early stopping\nearly_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n\nhistory = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test), \n                    callbacks=[early_stopping])\n\n# Step 5: Save the model and the training history\nmodel.save('lstm_model.h5')\nnp.save('training_history.npy', history.history)\n\n# Step 6: Make predictions on the test data\npredictions = model.predict(X_test)\n\n# Save predictions for later use\nnp.save('predictions.npy', predictions)\n\nprint(\"Model training completed, predictions made and saved!\")\n\n",
  "history_output" : "/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(**kwargs)\nEpoch 1/50\n\u001B[1m 1/39\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m32s\u001B[0m 853ms/step - loss: 0.2327\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m14/39\u001B[0m \u001B[32m━━━━━━━\u001B[0m\u001B[37m━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.2203   \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m29/39\u001B[0m \u001B[32m━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.1910\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m39/39\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1s\u001B[0m 7ms/step - loss: 0.1757 - val_loss: 0.0854\nEpoch 2/50\n\u001B[1m 1/39\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 0.0909\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m16/39\u001B[0m \u001B[32m━━━━━━━━\u001B[0m\u001B[37m━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0837\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m32/39\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0816\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m39/39\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0808 - val_loss: 0.0846\nEpoch 3/50\n\u001B[1m 1/39\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 0.0696\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m17/39\u001B[0m \u001B[32m━━━━━━━━\u001B[0m\u001B[37m━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0703\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m31/39\u001B[0m \u001B[32m━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0694\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m39/39\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0690 - val_loss: 0.0783\nEpoch 4/50\n\u001B[1m 1/39\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 0.0634\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m15/39\u001B[0m \u001B[32m━━━━━━━\u001B[0m\u001B[37m━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0614\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m29/39\u001B[0m \u001B[32m━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0606\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m39/39\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0600 - val_loss: 0.0666\nEpoch 5/50\n\u001B[1m 1/39\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 10ms/step - loss: 0.0488\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m15/39\u001B[0m \u001B[32m━━━━━━━\u001B[0m\u001B[37m━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0532 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m29/39\u001B[0m \u001B[32m━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0525\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m39/39\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0519 - val_loss: 0.0582\nEpoch 6/50\n\u001B[1m 1/39\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 11ms/step - loss: 0.0449\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m15/39\u001B[0m \u001B[32m━━━━━━━\u001B[0m\u001B[37m━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0466 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m29/39\u001B[0m \u001B[32m━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0462\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m39/39\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0458 - val_loss: 0.0499\nEpoch 7/50\n\u001B[1m 1/39\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 10ms/step - loss: 0.0443\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m15/39\u001B[0m \u001B[32m━━━━━━━\u001B[0m\u001B[37m━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0414 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m29/39\u001B[0m \u001B[32m━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0410\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m39/39\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0407 - val_loss: 0.0486\nEpoch 8/50\n\u001B[1m 1/39\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 10ms/step - loss: 0.0339\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m16/39\u001B[0m \u001B[32m━━━━━━━━\u001B[0m\u001B[37m━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0365 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m32/39\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0361\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m39/39\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0359 - val_loss: 0.0447\nEpoch 9/50\n\u001B[1m 1/39\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 10ms/step - loss: 0.0324\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m16/39\u001B[0m \u001B[32m━━━━━━━━\u001B[0m\u001B[37m━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0318 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m32/39\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0318\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m39/39\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0317 - val_loss: 0.0397\nEpoch 10/50\n\u001B[1m 1/39\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 0.0323\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m15/39\u001B[0m \u001B[32m━━━━━━━\u001B[0m\u001B[37m━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0298\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m26/39\u001B[0m \u001B[32m━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0294\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m36/39\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 0.0292\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m39/39\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 5ms/step - loss: 0.0291 - val_loss: 0.0416\nEpoch 11/50\n\u001B[1m 1/39\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 10ms/step - loss: 0.0265\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m16/39\u001B[0m \u001B[32m━━━━━━━━\u001B[0m\u001B[37m━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0265 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m31/39\u001B[0m \u001B[32m━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0266\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m39/39\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0265 - val_loss: 0.0328\nEpoch 12/50\n\u001B[1m 1/39\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 0.0255\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m16/39\u001B[0m \u001B[32m━━━━━━━━\u001B[0m\u001B[37m━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0245\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m31/39\u001B[0m \u001B[32m━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0243\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m39/39\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0242 - val_loss: 0.0360\nEpoch 13/50\n\u001B[1m 1/39\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 0.0222\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m16/39\u001B[0m \u001B[32m━━━━━━━━\u001B[0m\u001B[37m━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0224\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m31/39\u001B[0m \u001B[32m━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0222\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m39/39\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0222 - val_loss: 0.0360\nEpoch 14/50\n\u001B[1m 1/39\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 0.0194\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m16/39\u001B[0m \u001B[32m━━━━━━━━\u001B[0m\u001B[37m━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0216\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m27/39\u001B[0m \u001B[32m━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0215\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m39/39\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0213 - val_loss: 0.0303\nEpoch 15/50\n\u001B[1m 1/39\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 0.0199\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m16/39\u001B[0m \u001B[32m━━━━━━━━\u001B[0m\u001B[37m━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0197\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m31/39\u001B[0m \u001B[32m━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0195\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m39/39\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0194 - val_loss: 0.0286\nEpoch 16/50\n\u001B[1m 1/39\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 0.0195\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m15/39\u001B[0m \u001B[32m━━━━━━━\u001B[0m\u001B[37m━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0179\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m29/39\u001B[0m \u001B[32m━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0181\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m39/39\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0181 - val_loss: 0.0259\nEpoch 17/50\n\u001B[1m 1/39\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 0.0213\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m16/39\u001B[0m \u001B[32m━━━━━━━━\u001B[0m\u001B[37m━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0182\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m32/39\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0177\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m39/39\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0176 - val_loss: 0.0280\nEpoch 18/50\n\u001B[1m 1/39\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 10ms/step - loss: 0.0137\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m16/39\u001B[0m \u001B[32m━━━━━━━━\u001B[0m\u001B[37m━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0157 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m31/39\u001B[0m \u001B[32m━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0159\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m39/39\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0159 - val_loss: 0.0256\nEpoch 19/50\n\u001B[1m 1/39\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 0.0138\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m16/39\u001B[0m \u001B[32m━━━━━━━━\u001B[0m\u001B[37m━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0154\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m31/39\u001B[0m \u001B[32m━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0154\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m39/39\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0154 - val_loss: 0.0267\nEpoch 20/50\n\u001B[1m 1/39\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 10ms/step - loss: 0.0148\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m14/39\u001B[0m \u001B[32m━━━━━━━\u001B[0m\u001B[37m━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0153 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m29/39\u001B[0m \u001B[32m━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0152\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m39/39\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0150 - val_loss: 0.0229\nEpoch 21/50\n\u001B[1m 1/39\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 0.0113\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m16/39\u001B[0m \u001B[32m━━━━━━━━\u001B[0m\u001B[37m━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0146\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m32/39\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0147\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m39/39\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0147 - val_loss: 0.0244\nEpoch 22/50\n\u001B[1m 1/39\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 0.0124\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m16/39\u001B[0m \u001B[32m━━━━━━━━\u001B[0m\u001B[37m━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0143\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m30/39\u001B[0m \u001B[32m━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0142\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m39/39\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0142 - val_loss: 0.0199\nEpoch 23/50\n\u001B[1m 1/39\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 10ms/step - loss: 0.0136\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m15/39\u001B[0m \u001B[32m━━━━━━━\u001B[0m\u001B[37m━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0133 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m30/39\u001B[0m \u001B[32m━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0132\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m39/39\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0132 - val_loss: 0.0234\nEpoch 24/50\n\u001B[1m 1/39\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 10ms/step - loss: 0.0128\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m16/39\u001B[0m \u001B[32m━━━━━━━━\u001B[0m\u001B[37m━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0134 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m31/39\u001B[0m \u001B[32m━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0132\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m39/39\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0131 - val_loss: 0.0127\nEpoch 25/50\n\u001B[1m 1/39\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 0.0124\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m16/39\u001B[0m \u001B[32m━━━━━━━━\u001B[0m\u001B[37m━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0129\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m31/39\u001B[0m \u001B[32m━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0127\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m39/39\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0126 - val_loss: 0.0204\nEpoch 26/50\n\u001B[1m 1/39\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 9ms/step - loss: 0.0120\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m16/39\u001B[0m \u001B[32m━━━━━━━━\u001B[0m\u001B[37m━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0124\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m31/39\u001B[0m \u001B[32m━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0123\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m39/39\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0122 - val_loss: 0.0266\nEpoch 27/50\n\u001B[1m 1/39\u001B[0m \u001B[37m━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 10ms/step - loss: 0.0136\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m16/39\u001B[0m \u001B[32m━━━━━━━━\u001B[0m\u001B[37m━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0122 \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m31/39\u001B[0m \u001B[32m━━━━━━━━━━━━━━━\u001B[0m\u001B[37m━━━━━\u001B[0m \u001B[1m0s\u001B[0m 3ms/step - loss: 0.0122\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m39/39\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 4ms/step - loss: 0.0121 - val_loss: 0.0183\nWARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n\u001B[1m 1/10\u001B[0m \u001B[32m━━\u001B[0m\u001B[37m━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[1m0s\u001B[0m 74ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m10/10\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n\u001B[1m10/10\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m0s\u001B[0m 16ms/step\nModel training completed, predictions made and saved!\n",
  "history_begin_time" : 1727468290984,
  "history_end_time" : 1727468299575,
  "history_notes" : null,
  "history_process" : "s20a2d",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "dfneu1r599s",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1727468284964,
  "history_end_time" : 1727468284964,
  "history_notes" : null,
  "history_process" : "xnk7n1",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "4xt00ltpryt",
  "history_input" : "# import pandas as pd\n# import matplotlib.pyplot as plt\n# import seaborn as sns\n\n# def visualize(file_name, predictions_file, anomalies_file):\n#     data = pd.read_csv(file_name)\n#     predictions = pd.read_csv(predictions_file)\n#     anomalies = pd.read_csv(anomalies_file)\n    \n#     plt.figure(figsize=(12, 8))\n    \n#     # Plotting temperature trends with predictions\n#     plt.subplot(2, 1, 1)\n#     plt.plot(data['Year'], data['Temp_Value'], label='Observed Temperature')\n#     plt.plot(predictions['Year'], predictions['Temp_Prediction'], label='Predicted Temperature', linestyle='--')\n#     plt.title('Temperature Trends and Predictions')\n#     plt.xlabel('Year')\n#     plt.ylabel('Temperature (°C)')\n#     plt.legend()\n    \n#     # Plotting precipitation trends with predictions\n#     plt.subplot(2, 1, 2)\n#     plt.plot(data['Year'], data['Precipitation'], label='Observed Precipitation')\n#     plt.plot(predictions['Year'], predictions['Precip_Prediction'], label='Predicted Precipitation', linestyle='--')\n#     plt.title('Precipitation Trends and Predictions')\n#     plt.xlabel('Year')\n#     plt.ylabel('Precipitation (mm)')\n#     plt.legend()\n    \n#     # Highlighting anomalies\n#     sns.scatterplot(data=anomalies, x='Year', y='Temp_Value', hue='Cluster', palette='deep', legend='full', s=100)\n    \n#     plt.tight_layout()\n#     plt.savefig('climate_trends_and_anomalies.png')\n#     plt.show()\n\n# if __name__ == \"__main__\":\n#     visualize('/Users/vishesh/Desktop/geo_project/analyzed_climate_data.csv', '/Users/vishesh/Desktop/geo_project/predicted_climate_data.csv', '/Users/vishesh/Desktop/geo_project/anomaly_detected_data.csv')\n\n\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport joblib\n\n# Step 1: Load predictions, actual values, and scaler\npredictions = np.load('/Users/vishesh/gw-workspace/9I3eI9y6wb9B/predictions.npy')\ndata = np.load('/Users/vishesh/gw-workspace/L6gDfkNJofh8/preprocessed_data.npz')\ny_test = data['y_test']\n\n# Load the scaler for inverse transformation\nscaler = joblib.load('/Users/vishesh/gw-workspace/f7bNu3SQ6blN/scaler.pkl')\n\n# Step 2: Inverse scale the predictions and real values\npredicted_values = scaler.inverse_transform(predictions)\nreal_values = scaler.inverse_transform(y_test.reshape(-1, 1))\n\n# Step 3: Visualize the loss over epochs and print loss values\nhistory = np.load('/Users/vishesh/gw-workspace/9I3eI9y6wb9B/training_history.npy', allow_pickle=True).item()\n\n# Print the loss values after each epoch\ntrain_loss = history['loss']\nval_loss = history['val_loss']\n\nprint(\"Epoch-wise Losses:\")\nfor i in range(len(train_loss)):\n    print(f\"Epoch {i+1}: Train Loss = {train_loss[i]}, Validation Loss = {val_loss[i]}\")\n\n# Plot the loss curves\nplt.plot(train_loss, label='Train Loss')\nplt.plot(val_loss, label='Validation Loss')\nplt.title('Model Loss over Epochs')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()\n\n# Step 4: Visualize the predicted vs actual values\nplt.plot(real_values, color='blue', label='Real Temperature Values')\nplt.plot(predicted_values, color='red', label='Predicted Temperature Values')\nplt.title('Real vs Predicted Temperature Values')\nplt.xlabel('Time')\nplt.ylabel('Temperature')\nplt.legend()\nplt.show()\n",
  "history_output" : "Epoch-wise Losses:\nEpoch 1: Train Loss = 0.03072919137775898, Validation Loss = 0.01742088980972767\nEpoch 2: Train Loss = 0.011098137125372887, Validation Loss = 0.02268320694565773\nEpoch 3: Train Loss = 0.010209755040705204, Validation Loss = 0.01720193214714527\nEpoch 4: Train Loss = 0.00854005292057991, Validation Loss = 0.012641695328056812\nEpoch 5: Train Loss = 0.007384682539850473, Validation Loss = 0.017973309382796288\nEpoch 6: Train Loss = 0.006740374490618706, Validation Loss = 0.009798855520784855\nEpoch 7: Train Loss = 0.005943419877439737, Validation Loss = 0.010053860954940319\nEpoch 8: Train Loss = 0.00539526529610157, Validation Loss = 0.01057480089366436\nEpoch 9: Train Loss = 0.005453225690871477, Validation Loss = 0.00832605641335249\nEpoch 10: Train Loss = 0.005200219806283712, Validation Loss = 0.006214628461748362\nEpoch 11: Train Loss = 0.005218852777034044, Validation Loss = 0.006327028851956129\nEpoch 12: Train Loss = 0.005223800893872976, Validation Loss = 0.011008824221789837\nEpoch 13: Train Loss = 0.0046732318587601185, Validation Loss = 0.0043561640195548534\nEpoch 14: Train Loss = 0.004665676970034838, Validation Loss = 0.005817031487822533\nEpoch 15: Train Loss = 0.004796288441866636, Validation Loss = 0.009089156053960323\nEpoch 16: Train Loss = 0.004646167159080505, Validation Loss = 0.013137206435203552\nEpoch 17: Train Loss = 0.005103453528136015, Validation Loss = 0.003600385505706072\nEpoch 18: Train Loss = 0.0041281539015471935, Validation Loss = 0.005638223607093096\nEpoch 19: Train Loss = 0.004267961252480745, Validation Loss = 0.006245030555874109\nEpoch 20: Train Loss = 0.004086713772267103, Validation Loss = 0.0034914754796773195\n2024-09-27 16:18:34.506 python[35583:5739476] +[IMKClient subclass]: chose IMKClient_Legacy\n2024-09-27 16:18:34.506 python[35583:5739476] +[IMKInputSession subclass]: chose IMKInputSession_Legacy\n",
  "history_begin_time" : 1727468313199,
  "history_end_time" : 1727468320743,
  "history_notes" : null,
  "history_process" : "x4ch0z",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "1movovpdyfk",
  "history_input" : "import numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nimport joblib\nimport matplotlib.pyplot as plt\n\n# Step 1: Load the dataset\nfile_path = '/Users/vishesh/Desktop/geo_project/data.csv'\ndata = pd.read_csv(file_path, skiprows=3)\n\n# Step 2: Preprocess the data\ndata['Date'] = pd.to_datetime(data['Date'], format='%Y%m')\nvalues = data['Value'].values\n\n# Step 3: Normalize the data for LSTM\nscaler = MinMaxScaler(feature_range=(0, 1))\nscaled_data = scaler.fit_transform(values.reshape(-1, 1))\n\n# Save the scaler for inverse transformation later\njoblib.dump(scaler, 'scaler.pkl')\n\n# Step 4: Create sequences for LSTM\ndef create_sequences(data, sequence_length):\n    sequences = []\n    targets = []\n    for i in range(len(data) - sequence_length):\n        seq = data[i:i + sequence_length]\n        target = data[i + sequence_length]\n        sequences.append(seq)\n        targets.append(target)\n    return np.array(sequences), np.array(targets)\n\nsequence_length = 12  # 12 months\nX, y = create_sequences(scaled_data, sequence_length)\n\n# Step 5: Split into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n\n# Reshape the data for LSTM\nX_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\nX_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n\n# Step 6: Save preprocessed data for later use\nnp.savez('preprocessed_data.npz', X_train=X_train, X_test=X_test, y_train=y_train, y_test=y_test)\n\nprint(\"Data preprocessing completed and saved!\")\n\n\n\nplt.figure(figsize=(10,6))\nplt.plot(data['Date'], data['Value'], label='Temperature Values')\nplt.title('Temperature Values Over Time')\nplt.xlabel('Date')\nplt.ylabel('Temperature (Degrees Fahrenheit)')\nplt.grid(True)\nplt.legend()\nplt.show()\n",
  "history_output" : "Data preprocessing completed and saved!\n2024-09-27 16:18:07.696 python[35549:5734674] +[IMKClient subclass]: chose IMKClient_Legacy\n2024-09-27 16:18:07.696 python[35549:5734674] +[IMKInputSession subclass]: chose IMKInputSession_Legacy\n",
  "history_begin_time" : 1727468285877,
  "history_end_time" : 1727468289594,
  "history_notes" : null,
  "history_process" : "keghqi",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "t2vvbjln8si",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1727468284967,
  "history_end_time" : 1727468284967,
  "history_notes" : null,
  "history_process" : "fhhx69",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "6hxboy7adbg",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1727468284968,
  "history_end_time" : 1727468284968,
  "history_notes" : null,
  "history_process" : "rfwos9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "51ecty8pw5z",
  "history_input" : "import numpy as np\nimport optuna\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import Dense, Dropout, Conv1D, MaxPooling1D, Flatten, LSTM, Input, MultiHeadAttention, LayerNormalization, Add\nfrom tensorflow.keras.optimizers import Adam\nfrom pytorch_tabnet.tab_model import TabNetRegressor\n\n# Load preprocessed data\ndata_path = '/Users/vishesh/gw-workspace/L6gDfkNJofh8/preprocessed_data.npz'\ndata = np.load(data_path)\n\n# List all variable names in the .npz file to identify contents\nprint(\"Variables in the .npz file:\", data.files)\n\n# Load data based on contents of the .npz file\nX_train = data['X_train']\ny_train = data['y_train']\n\n# Check if 'X_val' and 'y_val' exist; if not, create validation split\nif 'X_val' in data.files and 'y_val' in data.files:\n    X_val = data['X_val']\n    y_val = data['y_val']\nelse:\n    # Split the data into training and validation sets\n    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n\n# Initialize a dictionary to store model performance\nmodel_performance = {}\n\n# Define the function to create various models\ndef create_model(trial):\n    input_shape = X_train.shape[1]\n    model_type = trial.suggest_categorical('model_type', ['dense', 'cnn', 'lstm', 'transformer', 'tabnet'])\n\n    if model_type == 'dense':\n        model = Sequential()\n        model.add(Input(shape=(input_shape,)))  # Use Input layer to specify input shape\n        num_layers = trial.suggest_int('num_layers', 1, 5)\n        for i in range(num_layers):\n            num_units = trial.suggest_int(f'num_units_l{i}', 16, 128, log=True)\n            model.add(Dense(num_units, activation='relu'))\n            dropout_rate = trial.suggest_float(f'dropout_rate_l{i}', 0.1, 0.5)\n            model.add(Dropout(dropout_rate))\n        model.add(Dense(1, activation='linear'))\n    \n    elif model_type == 'cnn':\n        model = Sequential()\n        model.add(Input(shape=(input_shape, 1)))  # Use Input layer for CNN input shape\n        model.add(Conv1D(filters=trial.suggest_int('filters', 16, 64, log=True),\n                         kernel_size=trial.suggest_int('kernel_size', 3, 5),\n                         activation='relu'))\n        model.add(MaxPooling1D(pool_size=2))\n        model.add(Flatten())\n        model.add(Dense(1, activation='linear'))\n    \n    elif model_type == 'lstm':\n        model = Sequential()\n        model.add(Input(shape=(input_shape, 1)))  # Use Input layer for LSTM input shape\n        model.add(LSTM(units=trial.suggest_int('lstm_units', 16, 64, log=True)))\n        model.add(Dense(1, activation='linear'))\n    \n    elif model_type == 'transformer':\n        inputs = Input(shape=(input_shape, 1))\n        attention = MultiHeadAttention(num_heads=trial.suggest_int('num_heads', 2, 8), key_dim=trial.suggest_int('key_dim', 16, 64))(inputs, inputs)\n        attention = Add()([inputs, attention])\n        attention = LayerNormalization(epsilon=1e-6)(attention)\n        outputs = Flatten()(attention)\n        outputs = Dense(1, activation='linear')(outputs)\n        model = Model(inputs=inputs, outputs=outputs)\n    \n    elif model_type == 'tabnet':\n        tabnet_model = TabNetRegressor(\n            n_d=trial.suggest_int('n_d', 8, 64),\n            n_a=trial.suggest_int('n_a', 8, 64),\n            n_steps=trial.suggest_int('n_steps', 3, 10),\n            gamma=trial.suggest_float('gamma', 1.0, 2.0),\n            lambda_sparse=trial.suggest_float('lambda_sparse', 1e-6, 1e-3)\n        )\n        tabnet_model.fit(\n            X_train, y_train,\n            eval_set=[(X_val, y_val)],\n            eval_metric=['mae'],\n            max_epochs=100,\n            patience=10,\n            batch_size=256,\n            virtual_batch_size=128\n        )\n        return tabnet_model\n\n    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-2, log=True)\n    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mean_squared_error', metrics=['mean_absolute_error'])\n\n    return model\n\n# Define the objective function for Optuna\ndef objective(trial):\n    model = create_model(trial)\n\n    if isinstance(model, TabNetRegressor):\n        model.fit(\n            X_train, y_train,\n            eval_set=[(X_val, y_val)],\n            eval_metric=['mae'],\n            max_epochs=100,\n            patience=10,\n            batch_size=256,\n            virtual_batch_size=128\n        )\n        y_pred = model.predict(X_val)\n        mae = mean_absolute_error(y_val, y_pred)\n    else:\n        model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10, batch_size=32, verbose=0)\n        _, mae = model.evaluate(X_val, y_val, verbose=0)\n\n    model_type = trial.params['model_type']\n    model_performance[model_type] = mae\n    return mae\n\n# Run the Bayesian optimization\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=10)  # Set n_trials as needed\n\n# Get the best hyperparameters\nbest_params = study.best_params\nprint(f\"Best hyperparameters: {best_params}\")\n\n# Display performance of each model\nprint(\"Model performance (MAE):\")\nfor model_type, mae in model_performance.items():\n    print(f\"{model_type}: {mae}\")\n\n# Plot the model performance\nplt.figure(figsize=(10, 5))\nplt.bar(model_performance.keys(), model_performance.values(), color='skyblue')\nplt.xlabel('Model Type')\nplt.ylabel('Mean Absolute Error')\nplt.title('Model Performance Comparison')\nplt.show()\n",
  "history_output" : "Variables in the .npz file: ['X_train', 'X_test', 'y_train', 'y_test']\n[I 2024-09-27 16:18:14,466] A new study created in memory with name: no-name-13b36e9b-8623-4adf-8dc8-52ceb04ee47f\n[I 2024-09-27 16:18:15,816] Trial 0 finished with value: 0.10084524005651474 and parameters: {'model_type': 'transformer', 'num_heads': 7, 'key_dim': 48, 'learning_rate': 0.002681520726582834}. Best is trial 0 with value: 0.10084524005651474.\n[I 2024-09-27 16:18:16,889] Trial 1 finished with value: 0.0784810483455658 and parameters: {'model_type': 'lstm', 'lstm_units': 20, 'learning_rate': 0.00015395164089192117}. Best is trial 1 with value: 0.0784810483455658.\n[I 2024-09-27 16:18:17,722] Trial 2 finished with value: 0.07196629047393799 and parameters: {'model_type': 'dense', 'num_layers': 3, 'num_units_l0': 22, 'dropout_rate_l0': 0.21543404066736993, 'num_units_l1': 53, 'dropout_rate_l1': 0.22056908562647057, 'num_units_l2': 72, 'dropout_rate_l2': 0.18181044195288562, 'learning_rate': 0.0035107093823946137}. Best is trial 2 with value: 0.07196629047393799.\n[I 2024-09-27 16:18:18,349] Trial 3 finished with value: 0.07505429536104202 and parameters: {'model_type': 'dense', 'num_layers': 2, 'num_units_l0': 44, 'dropout_rate_l0': 0.36251897420997586, 'num_units_l1': 21, 'dropout_rate_l1': 0.2513171977153634, 'learning_rate': 0.0004349775174661655}. Best is trial 2 with value: 0.07196629047393799.\n[I 2024-09-27 16:18:19,505] Trial 4 finished with value: 0.06955752521753311 and parameters: {'model_type': 'lstm', 'lstm_units': 48, 'learning_rate': 7.959066799447995e-05}. Best is trial 4 with value: 0.06955752521753311.\n[I 2024-09-27 16:18:20,511] Trial 5 finished with value: 0.1039140522480011 and parameters: {'model_type': 'lstm', 'lstm_units': 23, 'learning_rate': 5.6604814620760397e-05}. Best is trial 4 with value: 0.06955752521753311.\n[I 2024-09-27 16:18:20,996] Trial 6 finished with value: 0.03620130196213722 and parameters: {'model_type': 'dense', 'num_layers': 1, 'num_units_l0': 57, 'dropout_rate_l0': 0.2295830906235939, 'learning_rate': 0.0031816699772112818}. Best is trial 6 with value: 0.03620130196213722.\n[I 2024-09-27 16:18:21,944] Trial 7 finished with value: 0.19392570853233337 and parameters: {'model_type': 'dense', 'num_layers': 5, 'num_units_l0': 32, 'dropout_rate_l0': 0.4971360510228091, 'num_units_l1': 58, 'dropout_rate_l1': 0.48577309292626025, 'num_units_l2': 98, 'dropout_rate_l2': 0.2874729899173593, 'num_units_l3': 54, 'dropout_rate_l3': 0.30302410976802446, 'num_units_l4': 19, 'dropout_rate_l4': 0.14405669902548954, 'learning_rate': 8.072443561640008e-05}. Best is trial 6 with value: 0.03620130196213722.\n[I 2024-09-27 16:18:23,117] Trial 8 finished with value: 0.04415113478899002 and parameters: {'model_type': 'lstm', 'lstm_units': 63, 'learning_rate': 0.0009475788023192316}. Best is trial 6 with value: 0.03620130196213722.\n[I 2024-09-27 16:18:24,390] Trial 9 finished with value: 0.24924220144748688 and parameters: {'model_type': 'lstm', 'lstm_units': 60, 'learning_rate': 1.4865207146498436e-05}. Best is trial 6 with value: 0.03620130196213722.\nBest hyperparameters: {'model_type': 'dense', 'num_layers': 1, 'num_units_l0': 57, 'dropout_rate_l0': 0.2295830906235939, 'learning_rate': 0.0031816699772112818}\nModel performance (MAE):\ntransformer: 0.10084524005651474\nlstm: 0.24924220144748688\ndense: 0.19392570853233337\n2024-09-27 16:18:24.947 python[35562:5734945] +[IMKClient subclass]: chose IMKClient_Legacy\n2024-09-27 16:18:24.948 python[35562:5734945] +[IMKInputSession subclass]: chose IMKInputSession_Legacy\n",
  "history_begin_time" : 1727468290112,
  "history_end_time" : 1727468311727,
  "history_notes" : null,
  "history_process" : "vnrvoi",
  "host_id" : "100001",
  "indicator" : "Done"
}]
