[{
  "history_id" : "a3a2dhg3i90",
  "history_input" : "import numpy as np\nimport optuna\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import Dense, Dropout, Conv1D, MaxPooling1D, Flatten, LSTM, Input, MultiHeadAttention, LayerNormalization, Add\nfrom tensorflow.keras.optimizers import Adam\nfrom pytorch_tabnet.tab_model import TabNetRegressor\n\n# Load preprocessed data\ndata_path = '/Users/vishesh/gw-workspace/L6gDfkNJofh8/preprocessed_data.npz'\ndata = np.load(data_path)\n\n# List all variable names in the .npz file to identify contents\nprint(\"Variables in the .npz file:\", data.files)\n\n# Load data based on contents of the .npz file\nX_train = data['X_train']\ny_train = data['y_train']\n\n# Check if 'X_val' and 'y_val' exist; if not, create validation split\nif 'X_val' in data.files and 'y_val' in data.files:\n    X_val = data['X_val']\n    y_val = data['y_val']\nelse:\n    # Split the data into training and validation sets\n    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n\n# Initialize a dictionary to store model performance\nmodel_performance = {}\n\n# Define the function to create various models\ndef create_model(trial):\n    input_shape = X_train.shape[1]\n    model_type = trial.suggest_categorical('model_type', ['dense', 'cnn', 'lstm', 'transformer'])\n    # model_type = trial.suggest_categorical('model_type', ['dense', 'cnn', 'lstm', 'transformer', 'tabnet'])\n\n    if model_type == 'dense':\n        model = Sequential()\n        model.add(Input(shape=(input_shape,)))  # Use Input layer to specify input shape\n        num_layers = trial.suggest_int('num_layers', 1, 5)\n        for i in range(num_layers):\n            num_units = trial.suggest_int(f'num_units_l{i}', 16, 128, log=True)\n            model.add(Dense(num_units, activation='relu'))\n            dropout_rate = trial.suggest_float(f'dropout_rate_l{i}', 0.1, 0.5)\n            model.add(Dropout(dropout_rate))\n        model.add(Dense(1, activation='linear'))\n    \n    elif model_type == 'cnn':\n        model = Sequential()\n        model.add(Input(shape=(input_shape, 1)))  # Use Input layer for CNN input shape\n        model.add(Conv1D(filters=trial.suggest_int('filters', 16, 64, log=True),\n                         kernel_size=trial.suggest_int('kernel_size', 3, 5),\n                         activation='relu'))\n        model.add(MaxPooling1D(pool_size=2))\n        model.add(Flatten())\n        model.add(Dense(1, activation='linear'))\n    \n    elif model_type == 'lstm':\n        model = Sequential()\n        model.add(Input(shape=(input_shape, 1)))  # Use Input layer for LSTM input shape\n        model.add(LSTM(units=trial.suggest_int('lstm_units', 16, 64, log=True)))\n        model.add(Dense(1, activation='linear'))\n    \n    elif model_type == 'transformer':\n        inputs = Input(shape=(input_shape, 1))\n        attention = MultiHeadAttention(num_heads=trial.suggest_int('num_heads', 2, 8), key_dim=trial.suggest_int('key_dim', 16, 64))(inputs, inputs)\n        attention = Add()([inputs, attention])\n        attention = LayerNormalization(epsilon=1e-6)(attention)\n        outputs = Flatten()(attention)\n        outputs = Dense(1, activation='linear')(outputs)\n        model = Model(inputs=inputs, outputs=outputs)\n    \n    # elif model_type == 'tabnet':\n    #     tabnet_model = TabNetRegressor(\n    #         n_d=trial.suggest_int('n_d', 8, 64),\n    #         n_a=trial.suggest_int('n_a', 8, 64),\n    #         n_steps=trial.suggest_int('n_steps', 3, 10),\n    #         gamma=trial.suggest_float('gamma', 1.0, 2.0),\n    #         lambda_sparse=trial.suggest_float('lambda_sparse', 1e-6, 1e-3)\n    #     )\n    #     tabnet_model.fit(\n    #         X_train, y_train,\n    #         eval_set=[(X_val, y_val)],\n    #         eval_metric=['mae'],\n    #         max_epochs=100,\n    #         patience=10,\n    #         batch_size=256,\n    #         virtual_batch_size=128\n    #     )\n    #     return tabnet_model\n\n    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-2, log=True)\n    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mean_squared_error', metrics=['mean_absolute_error'])\n\n    return model\n\n# Define the objective function for Optuna\ndef objective(trial):\n    model = create_model(trial)\n\n    if isinstance(model, TabNetRegressor):\n        model.fit(\n            X_train, y_train,\n            eval_set=[(X_val, y_val)],\n            eval_metric=['mae'],\n            max_epochs=100,\n            patience=10,\n            batch_size=256,\n            virtual_batch_size=128\n        )\n        y_pred = model.predict(X_val)\n        mae = mean_absolute_error(y_val, y_pred)\n    else:\n        model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10, batch_size=32, verbose=0)\n        _, mae = model.evaluate(X_val, y_val, verbose=0)\n\n    model_type = trial.params['model_type']\n    model_performance[model_type] = mae\n    return mae\n\n# Run the Bayesian optimization\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=10)  # Set n_trials as needed\n\n# Get the best hyperparameters\nbest_params = study.best_params\nprint(f\"Best hyperparameters: {best_params}\")\n\n# Display performance of each model\nprint(\"Model performance (MAE):\")\nfor model_type, mae in model_performance.items():\n    print(f\"{model_type}: {mae}\")\n\n# Plot the model performance\nplt.figure(figsize=(10, 5))\nplt.bar(model_performance.keys(), model_performance.values(), color='skyblue')\nplt.xlabel('Model Type')\nplt.ylabel('Mean Absolute Error')\nplt.title('Model Performance Comparison')\nplt.show()\n",
  "history_output" : "Variables in the .npz file: ['X_train', 'X_test', 'y_train', 'y_test']\n[I 2024-09-27 16:54:30,336] A new study created in memory with name: no-name-26e82e06-8e46-4c33-b914-6683e6d34622\n[I 2024-09-27 16:54:31,974] Trial 0 finished with value: 0.17771868407726288 and parameters: {'model_type': 'transformer', 'num_heads': 7, 'key_dim': 54, 'learning_rate': 0.0002188816560027344}. Best is trial 0 with value: 0.17771868407726288.\n[I 2024-09-27 16:54:32,844] Trial 1 finished with value: 0.08036426454782486 and parameters: {'model_type': 'cnn', 'filters': 32, 'kernel_size': 3, 'learning_rate': 0.0002368393248777139}. Best is trial 1 with value: 0.08036426454782486.\n[I 2024-09-27 16:54:33,483] Trial 2 finished with value: 0.08247923105955124 and parameters: {'model_type': 'cnn', 'filters': 16, 'kernel_size': 3, 'learning_rate': 3.376330268624217e-05}. Best is trial 1 with value: 0.08036426454782486.\n[I 2024-09-27 16:54:34,737] Trial 3 finished with value: 0.09817364066839218 and parameters: {'model_type': 'dense', 'num_layers': 4, 'num_units_l0': 92, 'dropout_rate_l0': 0.4220722540103784, 'num_units_l1': 44, 'dropout_rate_l1': 0.2647193226493904, 'num_units_l2': 78, 'dropout_rate_l2': 0.20675138917828784, 'num_units_l3': 121, 'dropout_rate_l3': 0.22474119470833426, 'learning_rate': 0.002196296034639536}. Best is trial 1 with value: 0.08036426454782486.\n[I 2024-09-27 16:54:36,166] Trial 4 finished with value: 0.09968335926532745 and parameters: {'model_type': 'transformer', 'num_heads': 5, 'key_dim': 62, 'learning_rate': 0.0020001780865905295}. Best is trial 1 with value: 0.08036426454782486.\n[I 2024-09-27 16:54:36,923] Trial 5 finished with value: 0.07639056444168091 and parameters: {'model_type': 'dense', 'num_layers': 2, 'num_units_l0': 48, 'dropout_rate_l0': 0.4373346807117161, 'num_units_l1': 99, 'dropout_rate_l1': 0.39410871480129767, 'learning_rate': 0.001793572837162847}. Best is trial 5 with value: 0.07639056444168091.\n[I 2024-09-27 16:54:38,199] Trial 6 finished with value: 0.04157111421227455 and parameters: {'model_type': 'lstm', 'lstm_units': 24, 'learning_rate': 0.00304252745143496}. Best is trial 6 with value: 0.04157111421227455.\n[I 2024-09-27 16:54:39,482] Trial 7 finished with value: 0.03139379248023033 and parameters: {'model_type': 'lstm', 'lstm_units': 41, 'learning_rate': 0.00786860258348012}. Best is trial 7 with value: 0.03139379248023033.\n[I 2024-09-27 16:54:40,660] Trial 8 finished with value: 0.31630149483680725 and parameters: {'model_type': 'lstm', 'lstm_units': 26, 'learning_rate': 1.6735474609546008e-05}. Best is trial 7 with value: 0.03139379248023033.\n[I 2024-09-27 16:54:42,126] Trial 9 finished with value: 0.03876425698399544 and parameters: {'model_type': 'lstm', 'lstm_units': 33, 'learning_rate': 0.0036112229943515545}. Best is trial 7 with value: 0.03139379248023033.\nBest hyperparameters: {'model_type': 'lstm', 'lstm_units': 41, 'learning_rate': 0.00786860258348012}\nModel performance (MAE):\ntransformer: 0.09968335926532745\ncnn: 0.08247923105955124\ndense: 0.07639056444168091\nlstm: 0.03876425698399544\n2024-09-27 16:54:42.691 python[37674:5815198] +[IMKClient subclass]: chose IMKClient_Legacy\n2024-09-27 16:54:42.691 python[37674:5815198] +[IMKInputSession subclass]: chose IMKInputSession_Legacy\n",
  "history_begin_time" : 1727470464985,
  "history_end_time" : 1727470491196,
  "history_notes" : null,
  "history_process" : "vnrvoi",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "hwffnjzrxJv5",
  "history_input" : "import numpy as np\nimport optuna\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import Dense, Dropout, Conv1D, MaxPooling1D, Flatten, LSTM, Input, MultiHeadAttention, LayerNormalization, Add\nfrom tensorflow.keras.optimizers import Adam\nfrom pytorch_tabnet.tab_model import TabNetRegressor\n\n# Load preprocessed data\ndata_path = '/Users/vishesh/gw-workspace/L6gDfkNJofh8/preprocessed_data.npz'\ndata = np.load(data_path)\n\n# List all variable names in the .npz file to identify contents\nprint(\"Variables in the .npz file:\", data.files)\n\n# Load data based on contents of the .npz file\nX_train = data['X_train']\ny_train = data['y_train']\n\n# Check if 'X_val' and 'y_val' exist; if not, create validation split\nif 'X_val' in data.files and 'y_val' in data.files:\n    X_val = data['X_val']\n    y_val = data['y_val']\nelse:\n    # Split the data into training and validation sets\n    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n\n# Initialize a dictionary to store model performance\nmodel_performance = {}\n\n# Define the function to create various models\ndef create_model(trial):\n    input_shape = X_train.shape[1]\n    model_type = trial.suggest_categorical('model_type', ['dense', 'cnn', 'lstm', 'transformer'])\n    # model_type = trial.suggest_categorical('model_type', ['dense', 'cnn', 'lstm', 'transformer', 'tabnet'])\n\n    if model_type == 'dense':\n        model = Sequential()\n        model.add(Input(shape=(input_shape,)))  # Use Input layer to specify input shape\n        num_layers = trial.suggest_int('num_layers', 1, 5)\n        for i in range(num_layers):\n            num_units = trial.suggest_int(f'num_units_l{i}', 16, 128, log=True)\n            model.add(Dense(num_units, activation='relu'))\n            dropout_rate = trial.suggest_float(f'dropout_rate_l{i}', 0.1, 0.5)\n            model.add(Dropout(dropout_rate))\n        model.add(Dense(1, activation='linear'))\n    \n    elif model_type == 'cnn':\n        model = Sequential()\n        model.add(Input(shape=(input_shape, 1)))  # Use Input layer for CNN input shape\n        model.add(Conv1D(filters=trial.suggest_int('filters', 16, 64, log=True),\n                         kernel_size=trial.suggest_int('kernel_size', 3, 5),\n                         activation='relu'))\n        model.add(MaxPooling1D(pool_size=2))\n        model.add(Flatten())\n        model.add(Dense(1, activation='linear'))\n    \n    elif model_type == 'lstm':\n        model = Sequential()\n        model.add(Input(shape=(input_shape, 1)))  # Use Input layer for LSTM input shape\n        model.add(LSTM(units=trial.suggest_int('lstm_units', 16, 64, log=True)))\n        model.add(Dense(1, activation='linear'))\n    \n    elif model_type == 'transformer':\n        inputs = Input(shape=(input_shape, 1))\n        attention = MultiHeadAttention(num_heads=trial.suggest_int('num_heads', 2, 8), key_dim=trial.suggest_int('key_dim', 16, 64))(inputs, inputs)\n        attention = Add()([inputs, attention])\n        attention = LayerNormalization(epsilon=1e-6)(attention)\n        outputs = Flatten()(attention)\n        outputs = Dense(1, activation='linear')(outputs)\n        model = Model(inputs=inputs, outputs=outputs)\n    \n    # elif model_type == 'tabnet':\n    #     tabnet_model = TabNetRegressor(\n    #         n_d=trial.suggest_int('n_d', 8, 64),\n    #         n_a=trial.suggest_int('n_a', 8, 64),\n    #         n_steps=trial.suggest_int('n_steps', 3, 10),\n    #         gamma=trial.suggest_float('gamma', 1.0, 2.0),\n    #         lambda_sparse=trial.suggest_float('lambda_sparse', 1e-6, 1e-3)\n    #     )\n    #     tabnet_model.fit(\n    #         X_train, y_train,\n    #         eval_set=[(X_val, y_val)],\n    #         eval_metric=['mae'],\n    #         max_epochs=100,\n    #         patience=10,\n    #         batch_size=256,\n    #         virtual_batch_size=128\n    #     )\n    #     return tabnet_model\n\n    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-2, log=True)\n    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mean_squared_error', metrics=['mean_absolute_error'])\n\n    return model\n\n# Define the objective function for Optuna\ndef objective(trial):\n    model = create_model(trial)\n\n    if isinstance(model, TabNetRegressor):\n        model.fit(\n            X_train, y_train,\n            eval_set=[(X_val, y_val)],\n            eval_metric=['mae'],\n            max_epochs=100,\n            patience=10,\n            batch_size=256,\n            virtual_batch_size=128\n        )\n        y_pred = model.predict(X_val)\n        mae = mean_absolute_error(y_val, y_pred)\n    else:\n        model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10, batch_size=32, verbose=0)\n        _, mae = model.evaluate(X_val, y_val, verbose=0)\n\n    model_type = trial.params['model_type']\n    model_performance[model_type] = mae\n    return mae\n\n# Run the Bayesian optimization\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=10)  # Set n_trials as needed\n\n# Get the best hyperparameters\nbest_params = study.best_params\nprint(f\"Best hyperparameters: {best_params}\")\n\n# Display performance of each model\nprint(\"Model performance (MAE):\")\nfor model_type, mae in model_performance.items():\n    print(f\"{model_type}: {mae}\")\n\n# Plot the model performance\nplt.figure(figsize=(10, 5))\nplt.bar(model_performance.keys(), model_performance.values(), color='skyblue')\nplt.xlabel('Model Type')\nplt.ylabel('Mean Absolute Error')\nplt.title('Model Performance Comparison')\nplt.show()\n",
  "history_output" : "Variables in the .npz file: ['X_train', 'X_test', 'y_train', 'y_test']\n[I 2024-09-27 16:53:29,509] A new study created in memory with name: no-name-db119983-7dfa-4e80-beed-596ff60b5cc3\n[I 2024-09-27 16:53:30,660] Trial 0 finished with value: 0.09153387695550919 and parameters: {'model_type': 'dense', 'num_layers': 5, 'num_units_l0': 29, 'dropout_rate_l0': 0.18090882041350675, 'num_units_l1': 127, 'dropout_rate_l1': 0.1808952872461676, 'num_units_l2': 45, 'dropout_rate_l2': 0.2927939759596686, 'num_units_l3': 110, 'dropout_rate_l3': 0.3766646851739901, 'num_units_l4': 48, 'dropout_rate_l4': 0.18802486722143838, 'learning_rate': 0.0007106763994832153}. Best is trial 0 with value: 0.09153387695550919.\n[I 2024-09-27 16:53:32,009] Trial 1 finished with value: 0.03434019163250923 and parameters: {'model_type': 'lstm', 'lstm_units': 57, 'learning_rate': 0.00465677512463791}. Best is trial 1 with value: 0.03434019163250923.\n[I 2024-09-27 16:53:33,497] Trial 2 finished with value: 0.09966926276683807 and parameters: {'model_type': 'transformer', 'num_heads': 5, 'key_dim': 47, 'learning_rate': 0.006999635775961927}. Best is trial 1 with value: 0.03434019163250923.\n[I 2024-09-27 16:53:34,851] Trial 3 finished with value: 0.033702388405799866 and parameters: {'model_type': 'lstm', 'lstm_units': 59, 'learning_rate': 0.00496729611977395}. Best is trial 3 with value: 0.033702388405799866.\n[I 2024-09-27 16:53:36,080] Trial 4 finished with value: 0.07656506448984146 and parameters: {'model_type': 'lstm', 'lstm_units': 30, 'learning_rate': 0.00012912537687376468}. Best is trial 3 with value: 0.033702388405799866.\n[I 2024-09-27 16:53:37,341] Trial 5 finished with value: 0.16348564624786377 and parameters: {'model_type': 'lstm', 'lstm_units': 38, 'learning_rate': 3.548437201449682e-05}. Best is trial 3 with value: 0.033702388405799866.\n[I 2024-09-27 16:53:37,937] Trial 6 finished with value: 0.05634583160281181 and parameters: {'model_type': 'cnn', 'filters': 41, 'kernel_size': 4, 'learning_rate': 0.0009776907386183328}. Best is trial 3 with value: 0.033702388405799866.\n[I 2024-09-27 16:53:38,538] Trial 7 finished with value: 0.3238662779331207 and parameters: {'model_type': 'cnn', 'filters': 60, 'kernel_size': 3, 'learning_rate': 1.2203023837449677e-05}. Best is trial 3 with value: 0.033702388405799866.\n[I 2024-09-27 16:53:40,364] Trial 8 finished with value: 0.09967289119958878 and parameters: {'model_type': 'transformer', 'num_heads': 8, 'key_dim': 43, 'learning_rate': 0.00045626374070076314}. Best is trial 3 with value: 0.033702388405799866.\n[I 2024-09-27 16:53:41,070] Trial 9 finished with value: 0.047298580408096313 and parameters: {'model_type': 'dense', 'num_layers': 2, 'num_units_l0': 73, 'dropout_rate_l0': 0.47352499087121985, 'num_units_l1': 63, 'dropout_rate_l1': 0.3841590009736188, 'learning_rate': 0.008687493099346449}. Best is trial 3 with value: 0.033702388405799866.\nBest hyperparameters: {'model_type': 'lstm', 'lstm_units': 59, 'learning_rate': 0.00496729611977395}\nModel performance (MAE):\ndense: 0.047298580408096313\nlstm: 0.16348564624786377\ntransformer: 0.09967289119958878\ncnn: 0.3238662779331207\n2024-09-27 16:53:41.630 python[37636:5810971] +[IMKClient subclass]: chose IMKClient_Legacy\n2024-09-27 16:53:41.630 python[37636:5810971] +[IMKInputSession subclass]: chose IMKInputSession_Legacy\n",
  "history_begin_time" : 1727470405082,
  "history_end_time" : 1727470434991,
  "history_notes" : null,
  "history_process" : "vnrvoi",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "oSpBcL9XxajF",
  "history_input" : "import numpy as np\nimport optuna\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import Dense, Dropout, Conv1D, MaxPooling1D, Flatten, LSTM, Input, MultiHeadAttention, LayerNormalization, Add\nfrom tensorflow.keras.optimizers import Adam\nfrom pytorch_tabnet.tab_model import TabNetRegressor\n\n# Load preprocessed data\ndata_path = '/Users/vishesh/gw-workspace/L6gDfkNJofh8/preprocessed_data.npz'\ndata = np.load(data_path)\n\n# List all variable names in the .npz file to identify contents\nprint(\"Variables in the .npz file:\", data.files)\n\n# Load data based on contents of the .npz file\nX_train = data['X_train']\ny_train = data['y_train']\n\n# Check if 'X_val' and 'y_val' exist; if not, create validation split\nif 'X_val' in data.files and 'y_val' in data.files:\n    X_val = data['X_val']\n    y_val = data['y_val']\nelse:\n    # Split the data into training and validation sets\n    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n\n# Initialize a dictionary to store model performance\nmodel_performance = {}\n\n# Define the function to create various models\ndef create_model(trial):\n    input_shape = X_train.shape[1]\n    model_type = trial.suggest_categorical('model_type', ['dense', 'cnn', 'lstm', 'transformer'])\n    # model_type = trial.suggest_categorical('model_type', ['dense', 'cnn', 'lstm', 'transformer', 'tabnet'])\n\n    if model_type == 'dense':\n        model = Sequential()\n        model.add(Input(shape=(input_shape,)))  # Use Input layer to specify input shape\n        num_layers = trial.suggest_int('num_layers', 1, 5)\n        for i in range(num_layers):\n            num_units = trial.suggest_int(f'num_units_l{i}', 16, 128, log=True)\n            model.add(Dense(num_units, activation='relu'))\n            dropout_rate = trial.suggest_float(f'dropout_rate_l{i}', 0.1, 0.5)\n            model.add(Dropout(dropout_rate))\n        model.add(Dense(1, activation='linear'))\n    \n    elif model_type == 'cnn':\n        model = Sequential()\n        model.add(Input(shape=(input_shape, 1)))  # Use Input layer for CNN input shape\n        model.add(Conv1D(filters=trial.suggest_int('filters', 16, 64, log=True),\n                         kernel_size=trial.suggest_int('kernel_size', 3, 5),\n                         activation='relu'))\n        model.add(MaxPooling1D(pool_size=2))\n        model.add(Flatten())\n        model.add(Dense(1, activation='linear'))\n    \n    elif model_type == 'lstm':\n        model = Sequential()\n        model.add(Input(shape=(input_shape, 1)))  # Use Input layer for LSTM input shape\n        model.add(LSTM(units=trial.suggest_int('lstm_units', 16, 64, log=True)))\n        model.add(Dense(1, activation='linear'))\n    \n    elif model_type == 'transformer':\n        inputs = Input(shape=(input_shape, 1))\n        attention = MultiHeadAttention(num_heads=trial.suggest_int('num_heads', 2, 8), key_dim=trial.suggest_int('key_dim', 16, 64))(inputs, inputs)\n        attention = Add()([inputs, attention])\n        attention = LayerNormalization(epsilon=1e-6)(attention)\n        outputs = Flatten()(attention)\n        outputs = Dense(1, activation='linear')(outputs)\n        model = Model(inputs=inputs, outputs=outputs)\n    \n    # elif model_type == 'tabnet':\n    #     tabnet_model = TabNetRegressor(\n    #         n_d=trial.suggest_int('n_d', 8, 64),\n    #         n_a=trial.suggest_int('n_a', 8, 64),\n    #         n_steps=trial.suggest_int('n_steps', 3, 10),\n    #         gamma=trial.suggest_float('gamma', 1.0, 2.0),\n    #         lambda_sparse=trial.suggest_float('lambda_sparse', 1e-6, 1e-3)\n    #     )\n    #     tabnet_model.fit(\n    #         X_train, y_train,\n    #         eval_set=[(X_val, y_val)],\n    #         eval_metric=['mae'],\n    #         max_epochs=100,\n    #         patience=10,\n    #         batch_size=256,\n    #         virtual_batch_size=128\n    #     )\n    #     return tabnet_model\n\n    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-2, log=True)\n    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mean_squared_error', metrics=['mean_absolute_error'])\n\n    return model\n\n# Define the objective function for Optuna\ndef objective(trial):\n    model = create_model(trial)\n\n    if isinstance(model, TabNetRegressor):\n        model.fit(\n            X_train, y_train,\n            eval_set=[(X_val, y_val)],\n            eval_metric=['mae'],\n            max_epochs=100,\n            patience=10,\n            batch_size=256,\n            virtual_batch_size=128\n        )\n        y_pred = model.predict(X_val)\n        mae = mean_absolute_error(y_val, y_pred)\n    else:\n        model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10, batch_size=32, verbose=0)\n        _, mae = model.evaluate(X_val, y_val, verbose=0)\n\n    model_type = trial.params['model_type']\n    model_performance[model_type] = mae\n    return mae\n\n# Run the Bayesian optimization\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=10)  # Set n_trials as needed\n\n# Get the best hyperparameters\nbest_params = study.best_params\nprint(f\"Best hyperparameters: {best_params}\")\n\n# Display performance of each model\nprint(\"Model performance (MAE):\")\nfor model_type, mae in model_performance.items():\n    print(f\"{model_type}: {mae}\")\n\n# Plot the model performance\nplt.figure(figsize=(10, 5))\nplt.bar(model_performance.keys(), model_performance.values(), color='skyblue')\nplt.xlabel('Model Type')\nplt.ylabel('Mean Absolute Error')\nplt.title('Model Performance Comparison')\nplt.show()\n",
  "history_output" : "Variables in the .npz file: ['X_train', 'X_test', 'y_train', 'y_test']\n[I 2024-09-27 16:47:16,468] A new study created in memory with name: no-name-e6e65f6a-36c5-4ee0-ab3e-fd13a88696a4\n[I 2024-09-27 16:47:17,288] Trial 0 finished with value: 0.24583496153354645 and parameters: {'model_type': 'cnn', 'filters': 59, 'kernel_size': 4, 'learning_rate': 1.9436049492527987e-05}. Best is trial 0 with value: 0.24583496153354645.\n[I 2024-09-27 16:47:17,879] Trial 1 finished with value: 0.3583269715309143 and parameters: {'model_type': 'cnn', 'filters': 44, 'kernel_size': 5, 'learning_rate': 1.374752748946862e-05}. Best is trial 0 with value: 0.24583496153354645.\n[I 2024-09-27 16:47:18,476] Trial 2 finished with value: 0.07513163238763809 and parameters: {'model_type': 'cnn', 'filters': 23, 'kernel_size': 5, 'learning_rate': 0.0001417081768323726}. Best is trial 2 with value: 0.07513163238763809.\n[I 2024-09-27 16:47:19,279] Trial 3 finished with value: 0.2407858967781067 and parameters: {'model_type': 'dense', 'num_layers': 3, 'num_units_l0': 20, 'dropout_rate_l0': 0.3200184674494547, 'num_units_l1': 32, 'dropout_rate_l1': 0.2965492297470236, 'num_units_l2': 73, 'dropout_rate_l2': 0.11808428242088374, 'learning_rate': 1.2399445931985813e-05}. Best is trial 2 with value: 0.07513163238763809.\n[I 2024-09-27 16:47:20,233] Trial 4 finished with value: 0.08998613804578781 and parameters: {'model_type': 'dense', 'num_layers': 3, 'num_units_l0': 20, 'dropout_rate_l0': 0.41737396915475855, 'num_units_l1': 25, 'dropout_rate_l1': 0.2734224067415065, 'num_units_l2': 23, 'dropout_rate_l2': 0.14111706999027493, 'learning_rate': 0.006034744365524649}. Best is trial 2 with value: 0.07513163238763809.\n[I 2024-09-27 16:47:20,839] Trial 5 finished with value: 0.11954103410243988 and parameters: {'model_type': 'cnn', 'filters': 64, 'kernel_size': 4, 'learning_rate': 3.225035369635825e-05}. Best is trial 2 with value: 0.07513163238763809.\n[I 2024-09-27 16:47:21,477] Trial 6 finished with value: 0.09558197855949402 and parameters: {'model_type': 'dense', 'num_layers': 1, 'num_units_l0': 38, 'dropout_rate_l0': 0.4280409362787576, 'learning_rate': 0.00010977615114796075}. Best is trial 2 with value: 0.07513163238763809.\n[I 2024-09-27 16:47:22,051] Trial 7 finished with value: 0.1337452083826065 and parameters: {'model_type': 'dense', 'num_layers': 1, 'num_units_l0': 20, 'dropout_rate_l0': 0.36644260652046334, 'learning_rate': 3.423795657941601e-05}. Best is trial 2 with value: 0.07513163238763809.\n[I 2024-09-27 16:47:22,741] Trial 8 finished with value: 0.32347026467323303 and parameters: {'model_type': 'dense', 'num_layers': 2, 'num_units_l0': 18, 'dropout_rate_l0': 0.29769642803235546, 'num_units_l1': 63, 'dropout_rate_l1': 0.32632227437095046, 'learning_rate': 2.3824424163666448e-05}. Best is trial 2 with value: 0.07513163238763809.\n[I 2024-09-27 16:47:24,175] Trial 9 finished with value: 0.3933374881744385 and parameters: {'model_type': 'transformer', 'num_heads': 6, 'key_dim': 54, 'learning_rate': 1.0467136582573417e-05}. Best is trial 2 with value: 0.07513163238763809.\nBest hyperparameters: {'model_type': 'cnn', 'filters': 23, 'kernel_size': 5, 'learning_rate': 0.0001417081768323726}\nModel performance (MAE):\ncnn: 0.11954103410243988\ndense: 0.32347026467323303\ntransformer: 0.3933374881744385\n2024-09-27 16:47:24.727 python[37416:5799073] +[IMKClient subclass]: chose IMKClient_Legacy\n2024-09-27 16:47:24.727 python[37416:5799073] +[IMKInputSession subclass]: chose IMKInputSession_Legacy\n",
  "history_begin_time" : 1727470031927,
  "history_end_time" : 1727470056268,
  "history_notes" : null,
  "history_process" : "vnrvoi",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "I2YwK6VsHAS8",
  "history_input" : "import numpy as np\nimport optuna\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import Dense, Dropout, Conv1D, MaxPooling1D, Flatten, LSTM, Input, MultiHeadAttention, LayerNormalization, Add\nfrom tensorflow.keras.optimizers import Adam\nfrom pytorch_tabnet.tab_model import TabNetRegressor\n\n# Load preprocessed data\ndata_path = '/Users/vishesh/gw-workspace/L6gDfkNJofh8/preprocessed_data.npz'\ndata = np.load(data_path)\n\n# List all variable names in the .npz file to identify contents\nprint(\"Variables in the .npz file:\", data.files)\n\n# Load data based on contents of the .npz file\nX_train = data['X_train']\ny_train = data['y_train']\n\n# Check if 'X_val' and 'y_val' exist; if not, create validation split\nif 'X_val' in data.files and 'y_val' in data.files:\n    X_val = data['X_val']\n    y_val = data['y_val']\nelse:\n    # Split the data into training and validation sets\n    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n\n# Initialize a dictionary to store model performance\nmodel_performance = {}\n\n# Define the function to create various models\ndef create_model(trial):\n    input_shape = X_train.shape[1]\n    model_type = trial.suggest_categorical('model_type', ['dense', 'cnn', 'lstm', 'transformer', 'tabnet'])\n\n    if model_type == 'dense':\n        model = Sequential()\n        model.add(Input(shape=(input_shape,)))  # Use Input layer to specify input shape\n        num_layers = trial.suggest_int('num_layers', 1, 5)\n        for i in range(num_layers):\n            num_units = trial.suggest_int(f'num_units_l{i}', 16, 128, log=True)\n            model.add(Dense(num_units, activation='relu'))\n            dropout_rate = trial.suggest_float(f'dropout_rate_l{i}', 0.1, 0.5)\n            model.add(Dropout(dropout_rate))\n        model.add(Dense(1, activation='linear'))\n    \n    elif model_type == 'cnn':\n        model = Sequential()\n        model.add(Input(shape=(input_shape, 1)))  # Use Input layer for CNN input shape\n        model.add(Conv1D(filters=trial.suggest_int('filters', 16, 64, log=True),\n                         kernel_size=trial.suggest_int('kernel_size', 3, 5),\n                         activation='relu'))\n        model.add(MaxPooling1D(pool_size=2))\n        model.add(Flatten())\n        model.add(Dense(1, activation='linear'))\n    \n    elif model_type == 'lstm':\n        model = Sequential()\n        model.add(Input(shape=(input_shape, 1)))  # Use Input layer for LSTM input shape\n        model.add(LSTM(units=trial.suggest_int('lstm_units', 16, 64, log=True)))\n        model.add(Dense(1, activation='linear'))\n    \n    elif model_type == 'transformer':\n        inputs = Input(shape=(input_shape, 1))\n        attention = MultiHeadAttention(num_heads=trial.suggest_int('num_heads', 2, 8), key_dim=trial.suggest_int('key_dim', 16, 64))(inputs, inputs)\n        attention = Add()([inputs, attention])\n        attention = LayerNormalization(epsilon=1e-6)(attention)\n        outputs = Flatten()(attention)\n        outputs = Dense(1, activation='linear')(outputs)\n        model = Model(inputs=inputs, outputs=outputs)\n    \n    # elif model_type == 'tabnet':\n    #     tabnet_model = TabNetRegressor(\n    #         n_d=trial.suggest_int('n_d', 8, 64),\n    #         n_a=trial.suggest_int('n_a', 8, 64),\n    #         n_steps=trial.suggest_int('n_steps', 3, 10),\n    #         gamma=trial.suggest_float('gamma', 1.0, 2.0),\n    #         lambda_sparse=trial.suggest_float('lambda_sparse', 1e-6, 1e-3)\n    #     )\n    #     tabnet_model.fit(\n    #         X_train, y_train,\n    #         eval_set=[(X_val, y_val)],\n    #         eval_metric=['mae'],\n    #         max_epochs=100,\n    #         patience=10,\n    #         batch_size=256,\n    #         virtual_batch_size=128\n    #     )\n    #     return tabnet_model\n\n    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-2, log=True)\n    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mean_squared_error', metrics=['mean_absolute_error'])\n\n    return model\n\n# Define the objective function for Optuna\ndef objective(trial):\n    model = create_model(trial)\n\n    if isinstance(model, TabNetRegressor):\n        model.fit(\n            X_train, y_train,\n            eval_set=[(X_val, y_val)],\n            eval_metric=['mae'],\n            max_epochs=100,\n            patience=10,\n            batch_size=256,\n            virtual_batch_size=128\n        )\n        y_pred = model.predict(X_val)\n        mae = mean_absolute_error(y_val, y_pred)\n    else:\n        model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10, batch_size=32, verbose=0)\n        _, mae = model.evaluate(X_val, y_val, verbose=0)\n\n    model_type = trial.params['model_type']\n    model_performance[model_type] = mae\n    return mae\n\n# Run the Bayesian optimization\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=10)  # Set n_trials as needed\n\n# Get the best hyperparameters\nbest_params = study.best_params\nprint(f\"Best hyperparameters: {best_params}\")\n\n# Display performance of each model\nprint(\"Model performance (MAE):\")\nfor model_type, mae in model_performance.items():\n    print(f\"{model_type}: {mae}\")\n\n# Plot the model performance\nplt.figure(figsize=(10, 5))\nplt.bar(model_performance.keys(), model_performance.values(), color='skyblue')\nplt.xlabel('Model Type')\nplt.ylabel('Mean Absolute Error')\nplt.title('Model Performance Comparison')\nplt.show()\n",
  "history_output" : "Variables in the .npz file: ['X_train', 'X_test', 'y_train', 'y_test']\n[I 2024-09-27 16:46:41,857] A new study created in memory with name: no-name-5998a7b6-63ea-4611-ab07-66be759ada2c\n[I 2024-09-27 16:46:42,574] Trial 0 finished with value: 0.22631774842739105 and parameters: {'model_type': 'cnn', 'filters': 43, 'kernel_size': 5, 'learning_rate': 1.0473903525739045e-05}. Best is trial 0 with value: 0.22631774842739105.\n[W 2024-09-27 16:46:42,574] Trial 1 failed with parameters: {'model_type': 'tabnet', 'learning_rate': 0.0005354545623036245} because of the following error: UnboundLocalError(\"cannot access local variable 'model' where it is not associated with a value\").\nTraceback (most recent call last):\n  File \"/opt/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n    value_or_values = func(trial)\n                      ^^^^^^^^^^^\n  File \"/Users/vishesh/gw-workspace/I2YwK6VsHAS8/auto_ml.py\", line 100, in objective\n    model = create_model(trial)\n            ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/vishesh/gw-workspace/I2YwK6VsHAS8/auto_ml.py\", line 94, in create_model\n    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mean_squared_error', metrics=['mean_absolute_error'])\n    ^^^^^\nUnboundLocalError: cannot access local variable 'model' where it is not associated with a value\n[W 2024-09-27 16:46:42,575] Trial 1 failed with value None.\nTraceback (most recent call last):\n  File \"/Users/vishesh/gw-workspace/I2YwK6VsHAS8/auto_ml.py\", line 124, in <module>\n    study.optimize(objective, n_trials=10)  # Set n_trials as needed\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/optuna/study/study.py\", line 475, in optimize\n    _optimize(\n  File \"/opt/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 63, in _optimize\n    _optimize_sequential(\n  File \"/opt/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 160, in _optimize_sequential\n    frozen_trial = _run_trial(study, func, catch)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 248, in _run_trial\n    raise func_err\n  File \"/opt/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n    value_or_values = func(trial)\n                      ^^^^^^^^^^^\n  File \"/Users/vishesh/gw-workspace/I2YwK6VsHAS8/auto_ml.py\", line 100, in objective\n    model = create_model(trial)\n            ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/vishesh/gw-workspace/I2YwK6VsHAS8/auto_ml.py\", line 94, in create_model\n    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mean_squared_error', metrics=['mean_absolute_error'])\n    ^^^^^\nUnboundLocalError: cannot access local variable 'model' where it is not associated with a value\n",
  "history_begin_time" : 1727469997099,
  "history_end_time" : 1727470003083,
  "history_notes" : null,
  "history_process" : "vnrvoi",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "MFhT8e2GK7Bl",
  "history_input" : "import numpy as np\nimport optuna\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import Dense, Dropout, Conv1D, MaxPooling1D, Flatten, LSTM, Input, MultiHeadAttention, LayerNormalization, Add\nfrom tensorflow.keras.optimizers import Adam\nfrom pytorch_tabnet.tab_model import TabNetRegressor\n\n# Load preprocessed data\ndata_path = '/Users/vishesh/gw-workspace/L6gDfkNJofh8/preprocessed_data.npz'\ndata = np.load(data_path)\n\n# List all variable names in the .npz file to identify contents\nprint(\"Variables in the .npz file:\", data.files)\n\n# Load data based on contents of the .npz file\nX_train = data['X_train']\ny_train = data['y_train']\n\n# Check if 'X_val' and 'y_val' exist; if not, create validation split\nif 'X_val' in data.files and 'y_val' in data.files:\n    X_val = data['X_val']\n    y_val = data['y_val']\nelse:\n    # Split the data into training and validation sets\n    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n\n# Initialize a dictionary to store model performance\nmodel_performance = {}\n\n# Define the function to create various models\ndef create_model(trial):\n    input_shape = X_train.shape[1]\n    model_type = trial.suggest_categorical('model_type', ['dense', 'cnn', 'lstm', 'transformer', 'tabnet'])\n\n    if model_type == 'dense':\n        model = Sequential()\n        model.add(Input(shape=(input_shape,)))  # Use Input layer to specify input shape\n        num_layers = trial.suggest_int('num_layers', 1, 5)\n        for i in range(num_layers):\n            num_units = trial.suggest_int(f'num_units_l{i}', 16, 128, log=True)\n            model.add(Dense(num_units, activation='relu'))\n            dropout_rate = trial.suggest_float(f'dropout_rate_l{i}', 0.1, 0.5)\n            model.add(Dropout(dropout_rate))\n        model.add(Dense(1, activation='linear'))\n    \n    elif model_type == 'cnn':\n        model = Sequential()\n        model.add(Input(shape=(input_shape, 1)))  # Use Input layer for CNN input shape\n        model.add(Conv1D(filters=trial.suggest_int('filters', 16, 64, log=True),\n                         kernel_size=trial.suggest_int('kernel_size', 3, 5),\n                         activation='relu'))\n        model.add(MaxPooling1D(pool_size=2))\n        model.add(Flatten())\n        model.add(Dense(1, activation='linear'))\n    \n    elif model_type == 'lstm':\n        model = Sequential()\n        model.add(Input(shape=(input_shape, 1)))  # Use Input layer for LSTM input shape\n        model.add(LSTM(units=trial.suggest_int('lstm_units', 16, 64, log=True)))\n        model.add(Dense(1, activation='linear'))\n    \n    elif model_type == 'transformer':\n        inputs = Input(shape=(input_shape, 1))\n        attention = MultiHeadAttention(num_heads=trial.suggest_int('num_heads', 2, 8), key_dim=trial.suggest_int('key_dim', 16, 64))(inputs, inputs)\n        attention = Add()([inputs, attention])\n        attention = LayerNormalization(epsilon=1e-6)(attention)\n        outputs = Flatten()(attention)\n        outputs = Dense(1, activation='linear')(outputs)\n        model = Model(inputs=inputs, outputs=outputs)\n    \n    elif model_type == 'tabnet':\n        tabnet_model = TabNetRegressor(\n            n_d=trial.suggest_int('n_d', 8, 64),\n            n_a=trial.suggest_int('n_a', 8, 64),\n            n_steps=trial.suggest_int('n_steps', 3, 10),\n            gamma=trial.suggest_float('gamma', 1.0, 2.0),\n            lambda_sparse=trial.suggest_float('lambda_sparse', 1e-6, 1e-3)\n        )\n        tabnet_model.fit(\n            X_train, y_train,\n            eval_set=[(X_val, y_val)],\n            eval_metric=['mae'],\n            max_epochs=100,\n            patience=10,\n            batch_size=256,\n            virtual_batch_size=128\n        )\n        return tabnet_model\n\n    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-2, log=True)\n    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mean_squared_error', metrics=['mean_absolute_error'])\n\n    return model\n\n# Define the objective function for Optuna\ndef objective(trial):\n    model = create_model(trial)\n\n    if isinstance(model, TabNetRegressor):\n        model.fit(\n            X_train, y_train,\n            eval_set=[(X_val, y_val)],\n            eval_metric=['mae'],\n            max_epochs=100,\n            patience=10,\n            batch_size=256,\n            virtual_batch_size=128\n        )\n        y_pred = model.predict(X_val)\n        mae = mean_absolute_error(y_val, y_pred)\n    else:\n        model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10, batch_size=32, verbose=0)\n        _, mae = model.evaluate(X_val, y_val, verbose=0)\n\n    model_type = trial.params['model_type']\n    model_performance[model_type] = mae\n    return mae\n\n# Run the Bayesian optimization\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=10)  # Set n_trials as needed\n\n# Get the best hyperparameters\nbest_params = study.best_params\nprint(f\"Best hyperparameters: {best_params}\")\n\n# Display performance of each model\nprint(\"Model performance (MAE):\")\nfor model_type, mae in model_performance.items():\n    print(f\"{model_type}: {mae}\")\n\n# Plot the model performance\nplt.figure(figsize=(10, 5))\nplt.bar(model_performance.keys(), model_performance.values(), color='skyblue')\nplt.xlabel('Model Type')\nplt.ylabel('Mean Absolute Error')\nplt.title('Model Performance Comparison')\nplt.show()\n",
  "history_output" : "Variables in the .npz file: ['X_train', 'X_test', 'y_train', 'y_test']\n[I 2024-09-27 16:46:07,056] A new study created in memory with name: no-name-38fb9bf4-4679-4a8b-9dd8-9ce952667759\n[I 2024-09-27 16:46:07,740] Trial 0 finished with value: 0.05649206414818764 and parameters: {'model_type': 'cnn', 'filters': 17, 'kernel_size': 4, 'learning_rate': 0.0027950716302502614}. Best is trial 0 with value: 0.05649206414818764.\n/opt/anaconda3/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n  warnings.warn(f\"Device used : {self.device}\")\n[W 2024-09-27 16:46:07,744] Trial 1 failed with parameters: {'model_type': 'tabnet', 'n_d': 47, 'n_a': 18, 'n_steps': 6, 'gamma': 1.2004762545254049, 'lambda_sparse': 0.0007632038082672867} because of the following error: ValueError('Found array with dim 3. None expected <= 2.').\nTraceback (most recent call last):\n  File \"/opt/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n    value_or_values = func(trial)\n                      ^^^^^^^^^^^\n  File \"/Users/vishesh/gw-workspace/MFhT8e2GK7Bl/auto_ml.py\", line 100, in objective\n    model = create_model(trial)\n            ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/vishesh/gw-workspace/MFhT8e2GK7Bl/auto_ml.py\", line 82, in create_model\n    tabnet_model.fit(\n  File \"/opt/anaconda3/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py\", line 217, in fit\n    check_input(X_train)\n  File \"/opt/anaconda3/lib/python3.11/site-packages/pytorch_tabnet/utils.py\", line 507, in check_input\n    check_array(X, accept_sparse=True)\n  File \"/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py\", line 915, in check_array\n    raise ValueError(\nValueError: Found array with dim 3. None expected <= 2.\n[W 2024-09-27 16:46:07,746] Trial 1 failed with value None.\nTraceback (most recent call last):\n  File \"/Users/vishesh/gw-workspace/MFhT8e2GK7Bl/auto_ml.py\", line 124, in <module>\n    study.optimize(objective, n_trials=10)  # Set n_trials as needed\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/optuna/study/study.py\", line 475, in optimize\n    _optimize(\n  File \"/opt/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 63, in _optimize\n    _optimize_sequential(\n  File \"/opt/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 160, in _optimize_sequential\n    frozen_trial = _run_trial(study, func, catch)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 248, in _run_trial\n    raise func_err\n  File \"/opt/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n    value_or_values = func(trial)\n                      ^^^^^^^^^^^\n  File \"/Users/vishesh/gw-workspace/MFhT8e2GK7Bl/auto_ml.py\", line 100, in objective\n    model = create_model(trial)\n            ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/vishesh/gw-workspace/MFhT8e2GK7Bl/auto_ml.py\", line 82, in create_model\n    tabnet_model.fit(\n  File \"/opt/anaconda3/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py\", line 217, in fit\n    check_input(X_train)\n  File \"/opt/anaconda3/lib/python3.11/site-packages/pytorch_tabnet/utils.py\", line 507, in check_input\n    check_array(X, accept_sparse=True)\n  File \"/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py\", line 915, in check_array\n    raise ValueError(\nValueError: Found array with dim 3. None expected <= 2.\n",
  "history_begin_time" : 1727469962508,
  "history_end_time" : 1727469968255,
  "history_notes" : null,
  "history_process" : "vnrvoi",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "vBa3adCGKPRh",
  "history_input" : "import numpy as np\nimport optuna\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import Dense, Dropout, Conv1D, MaxPooling1D, Flatten, LSTM, Input, MultiHeadAttention, LayerNormalization, Add\nfrom tensorflow.keras.optimizers import Adam\nfrom pytorch_tabnet.tab_model import TabNetRegressor\n\n# Load preprocessed data\ndata_path = '/Users/vishesh/gw-workspace/L6gDfkNJofh8/preprocessed_data.npz'\ndata = np.load(data_path)\n\n# List all variable names in the .npz file to identify contents\nprint(\"Variables in the .npz file:\", data.files)\n\n# Load data based on contents of the .npz file\nX_train = data['X_train']\ny_train = data['y_train']\n\n# Check if 'X_val' and 'y_val' exist; if not, create validation split\nif 'X_val' in data.files and 'y_val' in data.files:\n    X_val = data['X_val']\n    y_val = data['y_val']\nelse:\n    # Split the data into training and validation sets\n    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n\n# Initialize a dictionary to store model performance\nmodel_performance = {}\n\n# Define the function to create various models\ndef create_model(trial):\n    input_shape = X_train.shape[1]\n    model_type = trial.suggest_categorical('model_type', ['dense', 'cnn', 'lstm', 'transformer', 'tabnet'])\n\n    if model_type == 'dense':\n        model = Sequential()\n        model.add(Input(shape=(input_shape,)))  # Use Input layer to specify input shape\n        num_layers = trial.suggest_int('num_layers', 1, 5)\n        for i in range(num_layers):\n            num_units = trial.suggest_int(f'num_units_l{i}', 16, 128, log=True)\n            model.add(Dense(num_units, activation='relu'))\n            dropout_rate = trial.suggest_float(f'dropout_rate_l{i}', 0.1, 0.5)\n            model.add(Dropout(dropout_rate))\n        model.add(Dense(1, activation='linear'))\n    \n    elif model_type == 'cnn':\n        model = Sequential()\n        model.add(Input(shape=(input_shape, 1)))  # Use Input layer for CNN input shape\n        model.add(Conv1D(filters=trial.suggest_int('filters', 16, 64, log=True),\n                         kernel_size=trial.suggest_int('kernel_size', 3, 5),\n                         activation='relu'))\n        model.add(MaxPooling1D(pool_size=2))\n        model.add(Flatten())\n        model.add(Dense(1, activation='linear'))\n    \n    elif model_type == 'lstm':\n        model = Sequential()\n        model.add(Input(shape=(input_shape, 1)))  # Use Input layer for LSTM input shape\n        model.add(LSTM(units=trial.suggest_int('lstm_units', 16, 64, log=True)))\n        model.add(Dense(1, activation='linear'))\n    \n    elif model_type == 'transformer':\n        inputs = Input(shape=(input_shape, 1))\n        attention = MultiHeadAttention(num_heads=trial.suggest_int('num_heads', 2, 8), key_dim=trial.suggest_int('key_dim', 16, 64))(inputs, inputs)\n        attention = Add()([inputs, attention])\n        attention = LayerNormalization(epsilon=1e-6)(attention)\n        outputs = Flatten()(attention)\n        outputs = Dense(1, activation='linear')(outputs)\n        model = Model(inputs=inputs, outputs=outputs)\n    \n    elif model_type == 'tabnet':\n        tabnet_model = TabNetRegressor(\n            n_d=trial.suggest_int('n_d', 8, 64),\n            n_a=trial.suggest_int('n_a', 8, 64),\n            n_steps=trial.suggest_int('n_steps', 3, 10),\n            gamma=trial.suggest_float('gamma', 1.0, 2.0),\n            lambda_sparse=trial.suggest_float('lambda_sparse', 1e-6, 1e-3)\n        )\n        tabnet_model.fit(\n            X_train, y_train,\n            eval_set=[(X_val, y_val)],\n            eval_metric=['mae'],\n            max_epochs=100,\n            patience=10,\n            batch_size=256,\n            virtual_batch_size=128\n        )\n        return tabnet_model\n\n    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-2, log=True)\n    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mean_squared_error', metrics=['mean_absolute_error'])\n\n    return model\n\n# Define the objective function for Optuna\ndef objective(trial):\n    model = create_model(trial)\n\n    if isinstance(model, TabNetRegressor):\n        model.fit(\n            X_train, y_train,\n            eval_set=[(X_val, y_val)],\n            eval_metric=['mae'],\n            max_epochs=100,\n            patience=10,\n            batch_size=256,\n            virtual_batch_size=128\n        )\n        y_pred = model.predict(X_val)\n        mae = mean_absolute_error(y_val, y_pred)\n    else:\n        model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10, batch_size=32, verbose=0)\n        _, mae = model.evaluate(X_val, y_val, verbose=0)\n\n    model_type = trial.params['model_type']\n    model_performance[model_type] = mae\n    return mae\n\n# Run the Bayesian optimization\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=10)  # Set n_trials as needed\n\n# Get the best hyperparameters\nbest_params = study.best_params\nprint(f\"Best hyperparameters: {best_params}\")\n\n# Display performance of each model\nprint(\"Model performance (MAE):\")\nfor model_type, mae in model_performance.items():\n    print(f\"{model_type}: {mae}\")\n\n# Plot the model performance\nplt.figure(figsize=(10, 5))\nplt.bar(model_performance.keys(), model_performance.values(), color='skyblue')\nplt.xlabel('Model Type')\nplt.ylabel('Mean Absolute Error')\nplt.title('Model Performance Comparison')\nplt.show()\n",
  "history_output" : "Variables in the .npz file: ['X_train', 'X_test', 'y_train', 'y_test']\n[I 2024-09-27 16:43:31,561] A new study created in memory with name: no-name-8bd72a29-942a-4334-8eed-c3cb3dc1ee44\n/opt/anaconda3/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n  warnings.warn(f\"Device used : {self.device}\")\n[W 2024-09-27 16:43:31,564] Trial 0 failed with parameters: {'model_type': 'tabnet', 'n_d': 29, 'n_a': 14, 'n_steps': 4, 'gamma': 1.9640073844190873, 'lambda_sparse': 0.0005219189519108192} because of the following error: ValueError('Found array with dim 3. None expected <= 2.').\nTraceback (most recent call last):\n  File \"/opt/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n    value_or_values = func(trial)\n                      ^^^^^^^^^^^\n  File \"/Users/vishesh/gw-workspace/vBa3adCGKPRh/auto_ml.py\", line 100, in objective\n    model = create_model(trial)\n            ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/vishesh/gw-workspace/vBa3adCGKPRh/auto_ml.py\", line 82, in create_model\n    tabnet_model.fit(\n  File \"/opt/anaconda3/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py\", line 217, in fit\n    check_input(X_train)\n  File \"/opt/anaconda3/lib/python3.11/site-packages/pytorch_tabnet/utils.py\", line 507, in check_input\n    check_array(X, accept_sparse=True)\n  File \"/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py\", line 915, in check_array\n    raise ValueError(\nValueError: Found array with dim 3. None expected <= 2.\n[W 2024-09-27 16:43:31,566] Trial 0 failed with value None.\nTraceback (most recent call last):\n  File \"/Users/vishesh/gw-workspace/vBa3adCGKPRh/auto_ml.py\", line 124, in <module>\n    study.optimize(objective, n_trials=10)  # Set n_trials as needed\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/optuna/study/study.py\", line 475, in optimize\n    _optimize(\n  File \"/opt/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 63, in _optimize\n    _optimize_sequential(\n  File \"/opt/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 160, in _optimize_sequential\n    frozen_trial = _run_trial(study, func, catch)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 248, in _run_trial\n    raise func_err\n  File \"/opt/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n    value_or_values = func(trial)\n                      ^^^^^^^^^^^\n  File \"/Users/vishesh/gw-workspace/vBa3adCGKPRh/auto_ml.py\", line 100, in objective\n    model = create_model(trial)\n            ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/vishesh/gw-workspace/vBa3adCGKPRh/auto_ml.py\", line 82, in create_model\n    tabnet_model.fit(\n  File \"/opt/anaconda3/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py\", line 217, in fit\n    check_input(X_train)\n  File \"/opt/anaconda3/lib/python3.11/site-packages/pytorch_tabnet/utils.py\", line 507, in check_input\n    check_array(X, accept_sparse=True)\n  File \"/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py\", line 915, in check_array\n    raise ValueError(\nValueError: Found array with dim 3. None expected <= 2.\n",
  "history_begin_time" : 1727469807047,
  "history_end_time" : 1727469812051,
  "history_notes" : null,
  "history_process" : "vnrvoi",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "DWxzl0JiZEEN",
  "history_input" : "import numpy as np\nimport optuna\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import Dense, Dropout, Conv1D, MaxPooling1D, Flatten, LSTM, Input, MultiHeadAttention, LayerNormalization, Add\nfrom tensorflow.keras.optimizers import Adam\nfrom pytorch_tabnet.tab_model import TabNetRegressor\n\n# Load preprocessed data\ndata_path = '/Users/vishesh/gw-workspace/L6gDfkNJofh8/preprocessed_data.npz'\ndata = np.load(data_path)\n\n# List all variable names in the .npz file to identify contents\nprint(\"Variables in the .npz file:\", data.files)\n\n# Load data based on contents of the .npz file\nX_train = data['X_train']\ny_train = data['y_train']\n\n# Check if 'X_val' and 'y_val' exist; if not, create validation split\nif 'X_val' in data.files and 'y_val' in data.files:\n    X_val = data['X_val']\n    y_val = data['y_val']\nelse:\n    # Split the data into training and validation sets\n    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n\n# Initialize a dictionary to store model performance\nmodel_performance = {}\n\n# Define the function to create various models\ndef create_model(trial):\n    input_shape = X_train.shape[1]\n    model_type = trial.suggest_categorical('model_type', ['dense', 'cnn', 'lstm', 'transformer', 'tabnet'])\n\n    if model_type == 'dense':\n        model = Sequential()\n        model.add(Input(shape=(input_shape,)))  # Use Input layer to specify input shape\n        num_layers = trial.suggest_int('num_layers', 1, 5)\n        for i in range(num_layers):\n            num_units = trial.suggest_int(f'num_units_l{i}', 16, 128, log=True)\n            model.add(Dense(num_units, activation='relu'))\n            dropout_rate = trial.suggest_float(f'dropout_rate_l{i}', 0.1, 0.5)\n            model.add(Dropout(dropout_rate))\n        model.add(Dense(1, activation='linear'))\n    \n    elif model_type == 'cnn':\n        model = Sequential()\n        model.add(Input(shape=(input_shape, 1)))  # Use Input layer for CNN input shape\n        model.add(Conv1D(filters=trial.suggest_int('filters', 16, 64, log=True),\n                         kernel_size=trial.suggest_int('kernel_size', 3, 5),\n                         activation='relu'))\n        model.add(MaxPooling1D(pool_size=2))\n        model.add(Flatten())\n        model.add(Dense(1, activation='linear'))\n    \n    elif model_type == 'lstm':\n        model = Sequential()\n        model.add(Input(shape=(input_shape, 1)))  # Use Input layer for LSTM input shape\n        model.add(LSTM(units=trial.suggest_int('lstm_units', 16, 64, log=True)))\n        model.add(Dense(1, activation='linear'))\n    \n    elif model_type == 'transformer':\n        inputs = Input(shape=(input_shape, 1))\n        attention = MultiHeadAttention(num_heads=trial.suggest_int('num_heads', 2, 8), key_dim=trial.suggest_int('key_dim', 16, 64))(inputs, inputs)\n        attention = Add()([inputs, attention])\n        attention = LayerNormalization(epsilon=1e-6)(attention)\n        outputs = Flatten()(attention)\n        outputs = Dense(1, activation='linear')(outputs)\n        model = Model(inputs=inputs, outputs=outputs)\n    \n\n    elif model_type == 'tabnet':\n        # Reshape the data to 2D if it has 3 dimensions\n        if len(X_train.shape) == 3:\n            X_train_tabnet = X_train.reshape(X_train.shape[0], -1)\n            X_val_tabnet = X_val.reshape(X_val.shape[0], -1)\n        else:\n            X_train_tabnet = X_train\n            X_val_tabnet = X_val\n        \n        tabnet_model = TabNetRegressor(\n            n_d=trial.suggest_int('n_d', 8, 64),\n            n_a=trial.suggest_int('n_a', 8, 64),\n            n_steps=trial.suggest_int('n_steps', 3, 10),\n            gamma=trial.suggest_float('gamma', 1.0, 2.0),\n            lambda_sparse=trial.suggest_float('lambda_sparse', 1e-6, 1e-3)\n        )\n        tabnet_model.fit(\n            X_train_tabnet, y_train,\n            eval_set=[(X_val_tabnet, y_val)],\n            eval_metric=['mae'],\n            max_epochs=100,\n            patience=10,\n            batch_size=256,\n            virtual_batch_size=128\n        )\n        # return tabnet_model\n\n\n    # elif model_type == 'tabnet':\n    #     tabnet_model = TabNetRegressor(\n    #         n_d=trial.suggest_int('n_d', 8, 64),\n    #         n_a=trial.suggest_int('n_a', 8, 64),\n    #         n_steps=trial.suggest_int('n_steps', 3, 10),\n    #         gamma=trial.suggest_float('gamma', 1.0, 2.0),\n    #         lambda_sparse=trial.suggest_float('lambda_sparse', 1e-6, 1e-3)\n    #     )\n    #     tabnet_model.fit(\n    #         X_train, y_train,\n    #         eval_set=[(X_val, y_val)],\n    #         eval_metric=['mae'],\n    #         max_epochs=100,\n    #         patience=10,\n    #         batch_size=256,\n    #         virtual_batch_size=128\n    #     )\n    #     return tabnet_model\n\n    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-2, log=True)\n    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mean_squared_error', metrics=['mean_absolute_error'])\n\n    return model\n\n# Define the objective function for Optuna\n# Define the objective function for Optuna\ndef objective(trial):\n    model = create_model(trial)\n\n    # Check if the model is an instance of TabNet and reshape input data if necessary\n    if isinstance(model, TabNetRegressor):\n        # Reshape to 2D if the input has 3 dimensions\n        if len(X_train.shape) == 3:\n            X_train_tabnet = X_train.reshape(X_train.shape[0], -1)\n            X_val_tabnet = X_val.reshape(X_val.shape[0], -1)\n        else:\n            X_train_tabnet = X_train\n            X_val_tabnet = X_val\n\n        model.fit(\n            X_train_tabnet, y_train,\n            eval_set=[(X_val_tabnet, y_val)],\n            eval_metric=['mae'],\n            max_epochs=100,\n            patience=10,\n            batch_size=256,\n            virtual_batch_size=128\n        )\n        y_pred = model.predict(X_val_tabnet)\n        mae = mean_absolute_error(y_val, y_pred)\n    else:\n        # For other models, use the original data\n        model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10, batch_size=32, verbose=0)\n        _, mae = model.evaluate(X_val, y_val, verbose=0)\n\n    model_type = trial.params['model_type']\n    model_performance[model_type] = mae\n    return mae\n\n\n# Run the Bayesian optimization\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=10)  # Set n_trials as needed\n\n# Get the best hyperparameters\nbest_params = study.best_params\nprint(f\"Best hyperparameters: {best_params}\")\n\n# Display performance of each model\nprint(\"Model performance (MAE):\")\nfor model_type, mae in model_performance.items():\n    print(f\"{model_type}: {mae}\")\n\n# Plot the model performance\nplt.figure(figsize=(10, 5))\nplt.bar(model_performance.keys(), model_performance.values(), color='skyblue')\nplt.xlabel('Model Type')\nplt.ylabel('Mean Absolute Error')\nplt.title('Model Performance Comparison')\nplt.show()\n",
  "history_output" : "Variables in the .npz file: ['X_train', 'X_test', 'y_train', 'y_test']\n[I 2024-09-27 16:42:48,547] A new study created in memory with name: no-name-fcec1958-f535-4ee8-a7e2-e02bb3f53880\n[I 2024-09-27 16:42:50,260] Trial 0 finished with value: 0.23408494889736176 and parameters: {'model_type': 'transformer', 'num_heads': 8, 'key_dim': 61, 'learning_rate': 0.00016869934219203146}. Best is trial 0 with value: 0.23408494889736176.\n[I 2024-09-27 16:42:51,608] Trial 1 finished with value: 0.3872363865375519 and parameters: {'model_type': 'transformer', 'num_heads': 5, 'key_dim': 26, 'learning_rate': 1.2843565120994378e-05}. Best is trial 0 with value: 0.23408494889736176.\n/opt/anaconda3/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n  warnings.warn(f\"Device used : {self.device}\")\nepoch 0  | loss: 9.51095 | val_0_mae: 2.26953 |  0:00:00s\nepoch 1  | loss: 6.1923  | val_0_mae: 0.37129 |  0:00:00s\nepoch 2  | loss: 3.5754  | val_0_mae: 1.36676 |  0:00:00s\nepoch 3  | loss: 4.59807 | val_0_mae: 1.52793 |  0:00:00s\nepoch 4  | loss: 3.05434 | val_0_mae: 0.72668 |  0:00:00s\nepoch 5  | loss: 2.40832 | val_0_mae: 0.43286 |  0:00:00s\nepoch 6  | loss: 2.78007 | val_0_mae: 0.3293  |  0:00:00s\nepoch 7  | loss: 1.25775 | val_0_mae: 0.39859 |  0:00:00s\nepoch 8  | loss: 0.90286 | val_0_mae: 0.29906 |  0:00:00s\nepoch 9  | loss: 0.82526 | val_0_mae: 0.29855 |  0:00:00s\nepoch 10 | loss: 0.51325 | val_0_mae: 0.31255 |  0:00:01s\nepoch 11 | loss: 0.29402 | val_0_mae: 0.29418 |  0:00:01s\nepoch 12 | loss: 0.27719 | val_0_mae: 0.19169 |  0:00:01s\nepoch 13 | loss: 0.29484 | val_0_mae: 0.15865 |  0:00:01s\nepoch 14 | loss: 0.13014 | val_0_mae: 0.17024 |  0:00:01s\nepoch 15 | loss: 0.18113 | val_0_mae: 0.1246  |  0:00:01s\nepoch 16 | loss: 0.12247 | val_0_mae: 0.10909 |  0:00:01s\nepoch 17 | loss: 0.1175  | val_0_mae: 0.13917 |  0:00:02s\nepoch 18 | loss: 0.09653 | val_0_mae: 0.11241 |  0:00:02s\nepoch 19 | loss: 0.06164 | val_0_mae: 0.10152 |  0:00:02s\nepoch 20 | loss: 0.08348 | val_0_mae: 0.21262 |  0:00:02s\nepoch 21 | loss: 0.27995 | val_0_mae: 0.12733 |  0:00:02s\nepoch 22 | loss: 0.09795 | val_0_mae: 0.19649 |  0:00:02s\nepoch 23 | loss: 0.13624 | val_0_mae: 0.10796 |  0:00:02s\nepoch 24 | loss: 0.07067 | val_0_mae: 0.16883 |  0:00:02s\nepoch 25 | loss: 0.09582 | val_0_mae: 0.21506 |  0:00:02s\nepoch 26 | loss: 0.11105 | val_0_mae: 0.11358 |  0:00:02s\nepoch 27 | loss: 0.09167 | val_0_mae: 0.19044 |  0:00:02s\nepoch 28 | loss: 0.09461 | val_0_mae: 0.09339 |  0:00:03s\nepoch 29 | loss: 0.10494 | val_0_mae: 0.10738 |  0:00:03s\nepoch 30 | loss: 0.04251 | val_0_mae: 0.08524 |  0:00:03s\nepoch 31 | loss: 0.02792 | val_0_mae: 0.11296 |  0:00:03s\nepoch 32 | loss: 0.02111 | val_0_mae: 0.07793 |  0:00:03s\nepoch 33 | loss: 0.02676 | val_0_mae: 0.08931 |  0:00:03s\nepoch 34 | loss: 0.01691 | val_0_mae: 0.08467 |  0:00:03s\nepoch 35 | loss: 0.01209 | val_0_mae: 0.09001 |  0:00:03s\nepoch 36 | loss: 0.01437 | val_0_mae: 0.08659 |  0:00:03s\nepoch 37 | loss: 0.01356 | val_0_mae: 0.10615 |  0:00:03s\nepoch 38 | loss: 0.01831 | val_0_mae: 0.10317 |  0:00:03s\nepoch 39 | loss: 0.01297 | val_0_mae: 0.10116 |  0:00:04s\nepoch 40 | loss: 0.01422 | val_0_mae: 0.10375 |  0:00:04s\nepoch 41 | loss: 0.01379 | val_0_mae: 0.11925 |  0:00:04s\nepoch 42 | loss: 0.00978 | val_0_mae: 0.09414 |  0:00:04s\nEarly stopping occurred at epoch 42 with best_epoch = 32 and best_val_0_mae = 0.07793\n/opt/anaconda3/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n  warnings.warn(wrn_msg)\n[W 2024-09-27 16:42:56,452] Trial 2 failed with parameters: {'model_type': 'tabnet', 'n_d': 45, 'n_a': 59, 'n_steps': 8, 'gamma': 1.5844948920479092, 'lambda_sparse': 0.0006339580255664025, 'learning_rate': 0.00010834288952067688} because of the following error: UnboundLocalError(\"cannot access local variable 'model' where it is not associated with a value\").\nTraceback (most recent call last):\n  File \"/opt/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n    value_or_values = func(trial)\n                      ^^^^^^^^^^^\n  File \"/Users/vishesh/gw-workspace/DWxzl0JiZEEN/auto_ml.py\", line 130, in objective\n    model = create_model(trial)\n            ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/vishesh/gw-workspace/DWxzl0JiZEEN/auto_ml.py\", line 123, in create_model\n    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mean_squared_error', metrics=['mean_absolute_error'])\n    ^^^^^\nUnboundLocalError: cannot access local variable 'model' where it is not associated with a value\n[W 2024-09-27 16:42:56,453] Trial 2 failed with value None.\nTraceback (most recent call last):\n  File \"/Users/vishesh/gw-workspace/DWxzl0JiZEEN/auto_ml.py\", line 165, in <module>\n    study.optimize(objective, n_trials=10)  # Set n_trials as needed\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/optuna/study/study.py\", line 475, in optimize\n    _optimize(\n  File \"/opt/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 63, in _optimize\n    _optimize_sequential(\n  File \"/opt/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 160, in _optimize_sequential\n    frozen_trial = _run_trial(study, func, catch)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 248, in _run_trial\n    raise func_err\n  File \"/opt/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n    value_or_values = func(trial)\n                      ^^^^^^^^^^^\n  File \"/Users/vishesh/gw-workspace/DWxzl0JiZEEN/auto_ml.py\", line 130, in objective\n    model = create_model(trial)\n            ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/vishesh/gw-workspace/DWxzl0JiZEEN/auto_ml.py\", line 123, in create_model\n    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mean_squared_error', metrics=['mean_absolute_error'])\n    ^^^^^\nUnboundLocalError: cannot access local variable 'model' where it is not associated with a value\n",
  "history_begin_time" : 1727469763876,
  "history_end_time" : 1727469777251,
  "history_notes" : null,
  "history_process" : "vnrvoi",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "7FrX9lQKjpby",
  "history_input" : "import numpy as np\nimport optuna\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import Dense, Dropout, Conv1D, MaxPooling1D, Flatten, LSTM, Input, MultiHeadAttention, LayerNormalization, Add\nfrom tensorflow.keras.optimizers import Adam\nfrom pytorch_tabnet.tab_model import TabNetRegressor\n\n# Load preprocessed data\ndata_path = '/Users/vishesh/gw-workspace/L6gDfkNJofh8/preprocessed_data.npz'\ndata = np.load(data_path)\n\n# List all variable names in the .npz file to identify contents\nprint(\"Variables in the .npz file:\", data.files)\n\n# Load data based on contents of the .npz file\nX_train = data['X_train']\ny_train = data['y_train']\n\n# Check if 'X_val' and 'y_val' exist; if not, create validation split\nif 'X_val' in data.files and 'y_val' in data.files:\n    X_val = data['X_val']\n    y_val = data['y_val']\nelse:\n    # Split the data into training and validation sets\n    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n\n# Initialize a dictionary to store model performance\nmodel_performance = {}\n\n# Define the function to create various models\ndef create_model(trial):\n    input_shape = X_train.shape[1]\n    model_type = trial.suggest_categorical('model_type', ['dense', 'cnn', 'lstm', 'transformer', 'tabnet'])\n\n    if model_type == 'dense':\n        model = Sequential()\n        model.add(Input(shape=(input_shape,)))  # Use Input layer to specify input shape\n        num_layers = trial.suggest_int('num_layers', 1, 5)\n        for i in range(num_layers):\n            num_units = trial.suggest_int(f'num_units_l{i}', 16, 128, log=True)\n            model.add(Dense(num_units, activation='relu'))\n            dropout_rate = trial.suggest_float(f'dropout_rate_l{i}', 0.1, 0.5)\n            model.add(Dropout(dropout_rate))\n        model.add(Dense(1, activation='linear'))\n    \n    elif model_type == 'cnn':\n        model = Sequential()\n        model.add(Input(shape=(input_shape, 1)))  # Use Input layer for CNN input shape\n        model.add(Conv1D(filters=trial.suggest_int('filters', 16, 64, log=True),\n                         kernel_size=trial.suggest_int('kernel_size', 3, 5),\n                         activation='relu'))\n        model.add(MaxPooling1D(pool_size=2))\n        model.add(Flatten())\n        model.add(Dense(1, activation='linear'))\n    \n    elif model_type == 'lstm':\n        model = Sequential()\n        model.add(Input(shape=(input_shape, 1)))  # Use Input layer for LSTM input shape\n        model.add(LSTM(units=trial.suggest_int('lstm_units', 16, 64, log=True)))\n        model.add(Dense(1, activation='linear'))\n    \n    elif model_type == 'transformer':\n        inputs = Input(shape=(input_shape, 1))\n        attention = MultiHeadAttention(num_heads=trial.suggest_int('num_heads', 2, 8), key_dim=trial.suggest_int('key_dim', 16, 64))(inputs, inputs)\n        attention = Add()([inputs, attention])\n        attention = LayerNormalization(epsilon=1e-6)(attention)\n        outputs = Flatten()(attention)\n        outputs = Dense(1, activation='linear')(outputs)\n        model = Model(inputs=inputs, outputs=outputs)\n    \n\n    elif model_type == 'tabnet':\n        # Reshape the data to 2D if it has 3 dimensions\n        if len(X_train.shape) == 3:\n            X_train_tabnet = X_train.reshape(X_train.shape[0], -1)\n            X_val_tabnet = X_val.reshape(X_val.shape[0], -1)\n        else:\n            X_train_tabnet = X_train\n            X_val_tabnet = X_val\n        \n        tabnet_model = TabNetRegressor(\n            n_d=trial.suggest_int('n_d', 8, 64),\n            n_a=trial.suggest_int('n_a', 8, 64),\n            n_steps=trial.suggest_int('n_steps', 3, 10),\n            gamma=trial.suggest_float('gamma', 1.0, 2.0),\n            lambda_sparse=trial.suggest_float('lambda_sparse', 1e-6, 1e-3)\n        )\n        tabnet_model.fit(\n            X_train_tabnet, y_train,\n            eval_set=[(X_val_tabnet, y_val)],\n            eval_metric=['mae'],\n            max_epochs=100,\n            patience=10,\n            batch_size=256,\n            virtual_batch_size=128\n        )\n        # return tabnet_model\n\n\n    # elif model_type == 'tabnet':\n    #     tabnet_model = TabNetRegressor(\n    #         n_d=trial.suggest_int('n_d', 8, 64),\n    #         n_a=trial.suggest_int('n_a', 8, 64),\n    #         n_steps=trial.suggest_int('n_steps', 3, 10),\n    #         gamma=trial.suggest_float('gamma', 1.0, 2.0),\n    #         lambda_sparse=trial.suggest_float('lambda_sparse', 1e-6, 1e-3)\n    #     )\n    #     tabnet_model.fit(\n    #         X_train, y_train,\n    #         eval_set=[(X_val, y_val)],\n    #         eval_metric=['mae'],\n    #         max_epochs=100,\n    #         patience=10,\n    #         batch_size=256,\n    #         virtual_batch_size=128\n    #     )\n    #     return tabnet_model\n\n    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-2, log=True)\n    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mean_squared_error', metrics=['mean_absolute_error'])\n\n    return model\n\n# Define the objective function for Optuna\ndef objective(trial):\n    model = create_model(trial)\n\n    if isinstance(model, TabNetRegressor):\n        model.fit(\n            X_train, y_train,\n            eval_set=[(X_val, y_val)],\n            eval_metric=['mae'],\n            max_epochs=100,\n            patience=10,\n            batch_size=256,\n            virtual_batch_size=128\n        )\n        y_pred = model.predict(X_val)\n        mae = mean_absolute_error(y_val, y_pred)\n    else:\n        model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10, batch_size=32, verbose=0)\n        _, mae = model.evaluate(X_val, y_val, verbose=0)\n\n    model_type = trial.params['model_type']\n    model_performance[model_type] = mae\n    return mae\n\n# Run the Bayesian optimization\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=10)  # Set n_trials as needed\n\n# Get the best hyperparameters\nbest_params = study.best_params\nprint(f\"Best hyperparameters: {best_params}\")\n\n# Display performance of each model\nprint(\"Model performance (MAE):\")\nfor model_type, mae in model_performance.items():\n    print(f\"{model_type}: {mae}\")\n\n# Plot the model performance\nplt.figure(figsize=(10, 5))\nplt.bar(model_performance.keys(), model_performance.values(), color='skyblue')\nplt.xlabel('Model Type')\nplt.ylabel('Mean Absolute Error')\nplt.title('Model Performance Comparison')\nplt.show()\n",
  "history_output" : "Variables in the .npz file: ['X_train', 'X_test', 'y_train', 'y_test']\n[I 2024-09-27 16:41:54,371] A new study created in memory with name: no-name-1ddfb7e7-a107-4fc0-a235-5fc3f0830732\n/opt/anaconda3/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n  warnings.warn(f\"Device used : {self.device}\")\nepoch 0  | loss: 5.42825 | val_0_mae: 1.14113 |  0:00:00s\nepoch 1  | loss: 4.4717  | val_0_mae: 0.60053 |  0:00:00s\nepoch 2  | loss: 4.35734 | val_0_mae: 2.07575 |  0:00:00s\nepoch 3  | loss: 4.49463 | val_0_mae: 1.03833 |  0:00:00s\nepoch 4  | loss: 5.6019  | val_0_mae: 0.97545 |  0:00:00s\nepoch 5  | loss: 2.49498 | val_0_mae: 0.99199 |  0:00:00s\nepoch 6  | loss: 6.6045  | val_0_mae: 0.5472  |  0:00:00s\nepoch 7  | loss: 1.54303 | val_0_mae: 0.28392 |  0:00:00s\nepoch 8  | loss: 1.10704 | val_0_mae: 0.49417 |  0:00:00s\nepoch 9  | loss: 0.84193 | val_0_mae: 0.19868 |  0:00:00s\nepoch 10 | loss: 0.70215 | val_0_mae: 0.37562 |  0:00:00s\nepoch 11 | loss: 1.10105 | val_0_mae: 0.45446 |  0:00:01s\nepoch 12 | loss: 2.16454 | val_0_mae: 0.3208  |  0:00:01s\nepoch 13 | loss: 0.93686 | val_0_mae: 0.31469 |  0:00:01s\nepoch 14 | loss: 0.91088 | val_0_mae: 0.23314 |  0:00:01s\nepoch 15 | loss: 0.4012  | val_0_mae: 0.23075 |  0:00:01s\nepoch 16 | loss: 0.37732 | val_0_mae: 0.18058 |  0:00:01s\nepoch 17 | loss: 0.34618 | val_0_mae: 0.19695 |  0:00:01s\nepoch 18 | loss: 0.23348 | val_0_mae: 0.11174 |  0:00:01s\nepoch 19 | loss: 0.20393 | val_0_mae: 0.1319  |  0:00:01s\nepoch 20 | loss: 0.1617  | val_0_mae: 0.17534 |  0:00:01s\nepoch 21 | loss: 0.36276 | val_0_mae: 0.27786 |  0:00:01s\nepoch 22 | loss: 0.1128  | val_0_mae: 0.14198 |  0:00:01s\nepoch 23 | loss: 0.12132 | val_0_mae: 0.23569 |  0:00:01s\nepoch 24 | loss: 0.10265 | val_0_mae: 0.11623 |  0:00:01s\nepoch 25 | loss: 0.21168 | val_0_mae: 0.23119 |  0:00:02s\nepoch 26 | loss: 0.10996 | val_0_mae: 0.19138 |  0:00:02s\nepoch 27 | loss: 0.05598 | val_0_mae: 0.17342 |  0:00:02s\nepoch 28 | loss: 0.0361  | val_0_mae: 0.1007  |  0:00:02s\nepoch 29 | loss: 0.04634 | val_0_mae: 0.17209 |  0:00:02s\nepoch 30 | loss: 0.05539 | val_0_mae: 0.10074 |  0:00:02s\nepoch 31 | loss: 0.03781 | val_0_mae: 0.15317 |  0:00:02s\nepoch 32 | loss: 0.02744 | val_0_mae: 0.08468 |  0:00:02s\nepoch 33 | loss: 0.02394 | val_0_mae: 0.13148 |  0:00:02s\nepoch 34 | loss: 0.02249 | val_0_mae: 0.08978 |  0:00:02s\nepoch 35 | loss: 0.01694 | val_0_mae: 0.09916 |  0:00:02s\nepoch 36 | loss: 0.01394 | val_0_mae: 0.07929 |  0:00:02s\nepoch 37 | loss: 0.0129  | val_0_mae: 0.09221 |  0:00:03s\nepoch 38 | loss: 0.01012 | val_0_mae: 0.10057 |  0:00:03s\nepoch 39 | loss: 0.0117  | val_0_mae: 0.08981 |  0:00:03s\nepoch 40 | loss: 0.01248 | val_0_mae: 0.08888 |  0:00:03s\nepoch 41 | loss: 0.01323 | val_0_mae: 0.0894  |  0:00:03s\nepoch 42 | loss: 0.00888 | val_0_mae: 0.09618 |  0:00:03s\nepoch 43 | loss: 0.00837 | val_0_mae: 0.08759 |  0:00:03s\nepoch 44 | loss: 0.00767 | val_0_mae: 0.09279 |  0:00:03s\nepoch 45 | loss: 0.00713 | val_0_mae: 0.08899 |  0:00:03s\nepoch 46 | loss: 0.00577 | val_0_mae: 0.08605 |  0:00:03s\nEarly stopping occurred at epoch 46 with best_epoch = 36 and best_val_0_mae = 0.07929\n/opt/anaconda3/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n  warnings.warn(wrn_msg)\n[W 2024-09-27 16:41:58,535] Trial 0 failed with parameters: {'model_type': 'tabnet', 'n_d': 41, 'n_a': 11, 'n_steps': 8, 'gamma': 1.6394368577303458, 'lambda_sparse': 0.0008835389588697094, 'learning_rate': 0.004271878675810098} because of the following error: UnboundLocalError(\"cannot access local variable 'model' where it is not associated with a value\").\nTraceback (most recent call last):\n  File \"/opt/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n    value_or_values = func(trial)\n                      ^^^^^^^^^^^\n  File \"/Users/vishesh/gw-workspace/7FrX9lQKjpby/auto_ml.py\", line 129, in objective\n    model = create_model(trial)\n            ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/vishesh/gw-workspace/7FrX9lQKjpby/auto_ml.py\", line 123, in create_model\n    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mean_squared_error', metrics=['mean_absolute_error'])\n    ^^^^^\nUnboundLocalError: cannot access local variable 'model' where it is not associated with a value\n[W 2024-09-27 16:41:58,536] Trial 0 failed with value None.\nTraceback (most recent call last):\n  File \"/Users/vishesh/gw-workspace/7FrX9lQKjpby/auto_ml.py\", line 153, in <module>\n    study.optimize(objective, n_trials=10)  # Set n_trials as needed\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/optuna/study/study.py\", line 475, in optimize\n    _optimize(\n  File \"/opt/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 63, in _optimize\n    _optimize_sequential(\n  File \"/opt/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 160, in _optimize_sequential\n    frozen_trial = _run_trial(study, func, catch)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 248, in _run_trial\n    raise func_err\n  File \"/opt/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n    value_or_values = func(trial)\n                      ^^^^^^^^^^^\n  File \"/Users/vishesh/gw-workspace/7FrX9lQKjpby/auto_ml.py\", line 129, in objective\n    model = create_model(trial)\n            ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/vishesh/gw-workspace/7FrX9lQKjpby/auto_ml.py\", line 123, in create_model\n    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mean_squared_error', metrics=['mean_absolute_error'])\n    ^^^^^\nUnboundLocalError: cannot access local variable 'model' where it is not associated with a value\n",
  "history_begin_time" : 1727469709521,
  "history_end_time" : 1727469719307,
  "history_notes" : null,
  "history_process" : "vnrvoi",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "f5EsJFZYaQy8",
  "history_input" : "import numpy as np\nimport optuna\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import Dense, Dropout, Conv1D, MaxPooling1D, Flatten, LSTM, Input, MultiHeadAttention, LayerNormalization, Add\nfrom tensorflow.keras.optimizers import Adam\nfrom pytorch_tabnet.tab_model import TabNetRegressor\n\n# Load preprocessed data\ndata_path = '/Users/vishesh/gw-workspace/L6gDfkNJofh8/preprocessed_data.npz'\ndata = np.load(data_path)\n\n# List all variable names in the .npz file to identify contents\nprint(\"Variables in the .npz file:\", data.files)\n\n# Load data based on contents of the .npz file\nX_train = data['X_train']\ny_train = data['y_train']\n\n# Check if 'X_val' and 'y_val' exist; if not, create validation split\nif 'X_val' in data.files and 'y_val' in data.files:\n    X_val = data['X_val']\n    y_val = data['y_val']\nelse:\n    # Split the data into training and validation sets\n    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n\n# Initialize a dictionary to store model performance\nmodel_performance = {}\n\n# Define the function to create various models\ndef create_model(trial):\n    input_shape = X_train.shape[1]\n    model_type = trial.suggest_categorical('model_type', ['dense', 'cnn', 'lstm', 'transformer', 'tabnet'])\n\n    if model_type == 'dense':\n        model = Sequential()\n        model.add(Input(shape=(input_shape,)))  # Use Input layer to specify input shape\n        num_layers = trial.suggest_int('num_layers', 1, 5)\n        for i in range(num_layers):\n            num_units = trial.suggest_int(f'num_units_l{i}', 16, 128, log=True)\n            model.add(Dense(num_units, activation='relu'))\n            dropout_rate = trial.suggest_float(f'dropout_rate_l{i}', 0.1, 0.5)\n            model.add(Dropout(dropout_rate))\n        model.add(Dense(1, activation='linear'))\n    \n    elif model_type == 'cnn':\n        model = Sequential()\n        model.add(Input(shape=(input_shape, 1)))  # Use Input layer for CNN input shape\n        model.add(Conv1D(filters=trial.suggest_int('filters', 16, 64, log=True),\n                         kernel_size=trial.suggest_int('kernel_size', 3, 5),\n                         activation='relu'))\n        model.add(MaxPooling1D(pool_size=2))\n        model.add(Flatten())\n        model.add(Dense(1, activation='linear'))\n    \n    elif model_type == 'lstm':\n        model = Sequential()\n        model.add(Input(shape=(input_shape, 1)))  # Use Input layer for LSTM input shape\n        model.add(LSTM(units=trial.suggest_int('lstm_units', 16, 64, log=True)))\n        model.add(Dense(1, activation='linear'))\n    \n    elif model_type == 'transformer':\n        inputs = Input(shape=(input_shape, 1))\n        attention = MultiHeadAttention(num_heads=trial.suggest_int('num_heads', 2, 8), key_dim=trial.suggest_int('key_dim', 16, 64))(inputs, inputs)\n        attention = Add()([inputs, attention])\n        attention = LayerNormalization(epsilon=1e-6)(attention)\n        outputs = Flatten()(attention)\n        outputs = Dense(1, activation='linear')(outputs)\n        model = Model(inputs=inputs, outputs=outputs)\n    \n\n    elif model_type == 'tabnet':\n        # Reshape the data to 2D if it has 3 dimensions\n        if len(X_train.shape) == 3:\n            X_train_tabnet = X_train.reshape(X_train.shape[0], -1)\n            X_val_tabnet = X_val.reshape(X_val.shape[0], -1)\n        else:\n            X_train_tabnet = X_train\n            X_val_tabnet = X_val\n        \n        tabnet_model = TabNetRegressor(\n            n_d=trial.suggest_int('n_d', 8, 64),\n            n_a=trial.suggest_int('n_a', 8, 64),\n            n_steps=trial.suggest_int('n_steps', 3, 10),\n            gamma=trial.suggest_float('gamma', 1.0, 2.0),\n            lambda_sparse=trial.suggest_float('lambda_sparse', 1e-6, 1e-3)\n        )\n        tabnet_model.fit(\n            X_train_tabnet, y_train,\n            eval_set=[(X_val_tabnet, y_val)],\n            eval_metric=['mae'],\n            max_epochs=100,\n            patience=10,\n            batch_size=256,\n            virtual_batch_size=128\n        )\n        return tabnet_model\n\n\n    # elif model_type == 'tabnet':\n    #     tabnet_model = TabNetRegressor(\n    #         n_d=trial.suggest_int('n_d', 8, 64),\n    #         n_a=trial.suggest_int('n_a', 8, 64),\n    #         n_steps=trial.suggest_int('n_steps', 3, 10),\n    #         gamma=trial.suggest_float('gamma', 1.0, 2.0),\n    #         lambda_sparse=trial.suggest_float('lambda_sparse', 1e-6, 1e-3)\n    #     )\n    #     tabnet_model.fit(\n    #         X_train, y_train,\n    #         eval_set=[(X_val, y_val)],\n    #         eval_metric=['mae'],\n    #         max_epochs=100,\n    #         patience=10,\n    #         batch_size=256,\n    #         virtual_batch_size=128\n    #     )\n    #     return tabnet_model\n\n    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-2, log=True)\n    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mean_squared_error', metrics=['mean_absolute_error'])\n\n    return model\n\n# Define the objective function for Optuna\ndef objective(trial):\n    model = create_model(trial)\n\n    if isinstance(model, TabNetRegressor):\n        model.fit(\n            X_train, y_train,\n            eval_set=[(X_val, y_val)],\n            eval_metric=['mae'],\n            max_epochs=100,\n            patience=10,\n            batch_size=256,\n            virtual_batch_size=128\n        )\n        y_pred = model.predict(X_val)\n        mae = mean_absolute_error(y_val, y_pred)\n    else:\n        model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10, batch_size=32, verbose=0)\n        _, mae = model.evaluate(X_val, y_val, verbose=0)\n\n    model_type = trial.params['model_type']\n    model_performance[model_type] = mae\n    return mae\n\n# Run the Bayesian optimization\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=10)  # Set n_trials as needed\n\n# Get the best hyperparameters\nbest_params = study.best_params\nprint(f\"Best hyperparameters: {best_params}\")\n\n# Display performance of each model\nprint(\"Model performance (MAE):\")\nfor model_type, mae in model_performance.items():\n    print(f\"{model_type}: {mae}\")\n\n# Plot the model performance\nplt.figure(figsize=(10, 5))\nplt.bar(model_performance.keys(), model_performance.values(), color='skyblue')\nplt.xlabel('Model Type')\nplt.ylabel('Mean Absolute Error')\nplt.title('Model Performance Comparison')\nplt.show()\n",
  "history_output" : "Variables in the .npz file: ['X_train', 'X_test', 'y_train', 'y_test']\n[I 2024-09-27 16:41:21,860] A new study created in memory with name: no-name-c3bd3cc0-ecea-4536-985c-1824d9586e37\n[I 2024-09-27 16:41:23,115] Trial 0 finished with value: 0.05525391921401024 and parameters: {'model_type': 'lstm', 'lstm_units': 24, 'learning_rate': 0.0008231366340916556}. Best is trial 0 with value: 0.05525391921401024.\n[I 2024-09-27 16:41:24,318] Trial 1 finished with value: 0.09969881922006607 and parameters: {'model_type': 'transformer', 'num_heads': 2, 'key_dim': 58, 'learning_rate': 0.006362328335395278}. Best is trial 0 with value: 0.05525391921401024.\n[I 2024-09-27 16:41:25,841] Trial 2 finished with value: 0.0402572937309742 and parameters: {'model_type': 'lstm', 'lstm_units': 62, 'learning_rate': 0.0011161945579442837}. Best is trial 2 with value: 0.0402572937309742.\n[I 2024-09-27 16:41:27,017] Trial 3 finished with value: 0.06477411091327667 and parameters: {'model_type': 'lstm', 'lstm_units': 27, 'learning_rate': 0.00044405903882725506}. Best is trial 2 with value: 0.0402572937309742.\n/opt/anaconda3/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n  warnings.warn(f\"Device used : {self.device}\")\nepoch 0  | loss: 3.06495 | val_0_mae: 0.63268 |  0:00:00s\nepoch 1  | loss: 0.90308 | val_0_mae: 0.73264 |  0:00:00s\nepoch 2  | loss: 0.54432 | val_0_mae: 0.76505 |  0:00:00s\nepoch 3  | loss: 1.37563 | val_0_mae: 0.55253 |  0:00:00s\nepoch 4  | loss: 0.6326  | val_0_mae: 0.12585 |  0:00:00s\nepoch 5  | loss: 0.27105 | val_0_mae: 0.10964 |  0:00:00s\nepoch 6  | loss: 0.17091 | val_0_mae: 0.1444  |  0:00:00s\nepoch 7  | loss: 0.15333 | val_0_mae: 0.11105 |  0:00:00s\nepoch 8  | loss: 0.11903 | val_0_mae: 0.1265  |  0:00:00s\nepoch 9  | loss: 0.08777 | val_0_mae: 0.13548 |  0:00:00s\nepoch 10 | loss: 0.09347 | val_0_mae: 0.12165 |  0:00:00s\nepoch 11 | loss: 0.065   | val_0_mae: 0.13893 |  0:00:00s\nepoch 12 | loss: 0.04713 | val_0_mae: 0.11519 |  0:00:00s\nepoch 13 | loss: 0.03176 | val_0_mae: 0.11    |  0:00:00s\nepoch 14 | loss: 0.03661 | val_0_mae: 0.10904 |  0:00:00s\nepoch 15 | loss: 0.0243  | val_0_mae: 0.09445 |  0:00:00s\nepoch 16 | loss: 0.01751 | val_0_mae: 0.09004 |  0:00:00s\nepoch 17 | loss: 0.0151  | val_0_mae: 0.08875 |  0:00:01s\nepoch 18 | loss: 0.01214 | val_0_mae: 0.08927 |  0:00:01s\nepoch 19 | loss: 0.01268 | val_0_mae: 0.08364 |  0:00:01s\nepoch 20 | loss: 0.01135 | val_0_mae: 0.09558 |  0:00:01s\nepoch 21 | loss: 0.01213 | val_0_mae: 0.09599 |  0:00:01s\nepoch 22 | loss: 0.00918 | val_0_mae: 0.08484 |  0:00:01s\nepoch 23 | loss: 0.01011 | val_0_mae: 0.08792 |  0:00:01s\nepoch 24 | loss: 0.00853 | val_0_mae: 0.08204 |  0:00:01s\nepoch 25 | loss: 0.00803 | val_0_mae: 0.08477 |  0:00:01s\nepoch 26 | loss: 0.00749 | val_0_mae: 0.07557 |  0:00:01s\nepoch 27 | loss: 0.00898 | val_0_mae: 0.07933 |  0:00:01s\nepoch 28 | loss: 0.00869 | val_0_mae: 0.08003 |  0:00:01s\nepoch 29 | loss: 0.00757 | val_0_mae: 0.07728 |  0:00:01s\nepoch 30 | loss: 0.00719 | val_0_mae: 0.07681 |  0:00:01s\nepoch 31 | loss: 0.00662 | val_0_mae: 0.07036 |  0:00:01s\nepoch 32 | loss: 0.00732 | val_0_mae: 0.0754  |  0:00:01s\nepoch 33 | loss: 0.00617 | val_0_mae: 0.07563 |  0:00:01s\nepoch 34 | loss: 0.00703 | val_0_mae: 0.07742 |  0:00:02s\nepoch 35 | loss: 0.00601 | val_0_mae: 0.07514 |  0:00:02s\nepoch 36 | loss: 0.00665 | val_0_mae: 0.07925 |  0:00:02s\nepoch 37 | loss: 0.00763 | val_0_mae: 0.08555 |  0:00:02s\nepoch 38 | loss: 0.00786 | val_0_mae: 0.09346 |  0:00:02s\nepoch 39 | loss: 0.00818 | val_0_mae: 0.08859 |  0:00:02s\nepoch 40 | loss: 0.0078  | val_0_mae: 0.0885  |  0:00:02s\nepoch 41 | loss: 0.00766 | val_0_mae: 0.08755 |  0:00:02s\nEarly stopping occurred at epoch 41 with best_epoch = 31 and best_val_0_mae = 0.07036\n/opt/anaconda3/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n  warnings.warn(wrn_msg)\n[W 2024-09-27 16:41:29,956] Trial 4 failed with parameters: {'model_type': 'tabnet', 'n_d': 13, 'n_a': 41, 'n_steps': 6, 'gamma': 1.8823116753943032, 'lambda_sparse': 0.0006864363246650971} because of the following error: ValueError('Found array with dim 3. None expected <= 2.').\nTraceback (most recent call last):\n  File \"/opt/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n    value_or_values = func(trial)\n                      ^^^^^^^^^^^\n  File \"/Users/vishesh/gw-workspace/f5EsJFZYaQy8/auto_ml.py\", line 132, in objective\n    model.fit(\n  File \"/opt/anaconda3/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py\", line 217, in fit\n    check_input(X_train)\n  File \"/opt/anaconda3/lib/python3.11/site-packages/pytorch_tabnet/utils.py\", line 507, in check_input\n    check_array(X, accept_sparse=True)\n  File \"/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py\", line 915, in check_array\n    raise ValueError(\nValueError: Found array with dim 3. None expected <= 2.\n[W 2024-09-27 16:41:29,957] Trial 4 failed with value None.\nTraceback (most recent call last):\n  File \"/Users/vishesh/gw-workspace/f5EsJFZYaQy8/auto_ml.py\", line 153, in <module>\n    study.optimize(objective, n_trials=10)  # Set n_trials as needed\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/optuna/study/study.py\", line 475, in optimize\n    _optimize(\n  File \"/opt/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 63, in _optimize\n    _optimize_sequential(\n  File \"/opt/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 160, in _optimize_sequential\n    frozen_trial = _run_trial(study, func, catch)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 248, in _run_trial\n    raise func_err\n  File \"/opt/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n    value_or_values = func(trial)\n                      ^^^^^^^^^^^\n  File \"/Users/vishesh/gw-workspace/f5EsJFZYaQy8/auto_ml.py\", line 132, in objective\n    model.fit(\n  File \"/opt/anaconda3/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py\", line 217, in fit\n    check_input(X_train)\n  File \"/opt/anaconda3/lib/python3.11/site-packages/pytorch_tabnet/utils.py\", line 507, in check_input\n    check_array(X, accept_sparse=True)\n  File \"/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py\", line 915, in check_array\n    raise ValueError(\nValueError: Found array with dim 3. None expected <= 2.\n",
  "history_begin_time" : 1727469677037,
  "history_end_time" : 1727469690754,
  "history_notes" : null,
  "history_process" : "vnrvoi",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "R2jf7evKMmdc",
  "history_input" : "import numpy as np\nimport optuna\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import Dense, Dropout, Conv1D, MaxPooling1D, Flatten, LSTM, Input, MultiHeadAttention, LayerNormalization, Add\nfrom tensorflow.keras.optimizers import Adam\nfrom pytorch_tabnet.tab_model import TabNetRegressor\n\n# Load preprocessed data\ndata_path = '/Users/vishesh/gw-workspace/L6gDfkNJofh8/preprocessed_data.npz'\ndata = np.load(data_path)\n\n# List all variable names in the .npz file to identify contents\nprint(\"Variables in the .npz file:\", data.files)\n\n# Load data based on contents of the .npz file\nX_train = data['X_train']\ny_train = data['y_train']\n\n# Check if 'X_val' and 'y_val' exist; if not, create validation split\nif 'X_val' in data.files and 'y_val' in data.files:\n    X_val = data['X_val']\n    y_val = data['y_val']\nelse:\n    # Split the data into training and validation sets\n    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n\n# Initialize a dictionary to store model performance\nmodel_performance = {}\n\n# Define the function to create various models\ndef create_model(trial):\n    input_shape = X_train.shape[1]\n    model_type = trial.suggest_categorical('model_type', ['dense', 'cnn', 'lstm', 'transformer', 'tabnet'])\n\n    if model_type == 'dense':\n        model = Sequential()\n        model.add(Input(shape=(input_shape,)))  # Use Input layer to specify input shape\n        num_layers = trial.suggest_int('num_layers', 1, 5)\n        for i in range(num_layers):\n            num_units = trial.suggest_int(f'num_units_l{i}', 16, 128, log=True)\n            model.add(Dense(num_units, activation='relu'))\n            dropout_rate = trial.suggest_float(f'dropout_rate_l{i}', 0.1, 0.5)\n            model.add(Dropout(dropout_rate))\n        model.add(Dense(1, activation='linear'))\n    \n    elif model_type == 'cnn':\n        model = Sequential()\n        model.add(Input(shape=(input_shape, 1)))  # Use Input layer for CNN input shape\n        model.add(Conv1D(filters=trial.suggest_int('filters', 16, 64, log=True),\n                         kernel_size=trial.suggest_int('kernel_size', 3, 5),\n                         activation='relu'))\n        model.add(MaxPooling1D(pool_size=2))\n        model.add(Flatten())\n        model.add(Dense(1, activation='linear'))\n    \n    elif model_type == 'lstm':\n        model = Sequential()\n        model.add(Input(shape=(input_shape, 1)))  # Use Input layer for LSTM input shape\n        model.add(LSTM(units=trial.suggest_int('lstm_units', 16, 64, log=True)))\n        model.add(Dense(1, activation='linear'))\n    \n    elif model_type == 'transformer':\n        inputs = Input(shape=(input_shape, 1))\n        attention = MultiHeadAttention(num_heads=trial.suggest_int('num_heads', 2, 8), key_dim=trial.suggest_int('key_dim', 16, 64))(inputs, inputs)\n        attention = Add()([inputs, attention])\n        attention = LayerNormalization(epsilon=1e-6)(attention)\n        outputs = Flatten()(attention)\n        outputs = Dense(1, activation='linear')(outputs)\n        model = Model(inputs=inputs, outputs=outputs)\n    \n    elif model_type == 'tabnet':\n        tabnet_model = TabNetRegressor(\n            n_d=trial.suggest_int('n_d', 8, 64),\n            n_a=trial.suggest_int('n_a', 8, 64),\n            n_steps=trial.suggest_int('n_steps', 3, 10),\n            gamma=trial.suggest_float('gamma', 1.0, 2.0),\n            lambda_sparse=trial.suggest_float('lambda_sparse', 1e-6, 1e-3)\n        )\n        tabnet_model.fit(\n            X_train, y_train,\n            eval_set=[(X_val, y_val)],\n            eval_metric=['mae'],\n            max_epochs=100,\n            patience=10,\n            batch_size=256,\n            virtual_batch_size=128\n        )\n        return tabnet_model\n\n    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-2, log=True)\n    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mean_squared_error', metrics=['mean_absolute_error'])\n\n    return model\n\n# Define the objective function for Optuna\ndef objective(trial):\n    model = create_model(trial)\n\n    if isinstance(model, TabNetRegressor):\n        model.fit(\n            X_train, y_train,\n            eval_set=[(X_val, y_val)],\n            eval_metric=['mae'],\n            max_epochs=100,\n            patience=10,\n            batch_size=256,\n            virtual_batch_size=128\n        )\n        y_pred = model.predict(X_val)\n        mae = mean_absolute_error(y_val, y_pred)\n    else:\n        model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10, batch_size=32, verbose=0)\n        _, mae = model.evaluate(X_val, y_val, verbose=0)\n\n    model_type = trial.params['model_type']\n    model_performance[model_type] = mae\n    return mae\n\n# Run the Bayesian optimization\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=10)  # Set n_trials as needed\n\n# Get the best hyperparameters\nbest_params = study.best_params\nprint(f\"Best hyperparameters: {best_params}\")\n\n# Display performance of each model\nprint(\"Model performance (MAE):\")\nfor model_type, mae in model_performance.items():\n    print(f\"{model_type}: {mae}\")\n\n# Plot the model performance\nplt.figure(figsize=(10, 5))\nplt.bar(model_performance.keys(), model_performance.values(), color='skyblue')\nplt.xlabel('Model Type')\nplt.ylabel('Mean Absolute Error')\nplt.title('Model Performance Comparison')\nplt.show()\n",
  "history_output" : "Variables in the .npz file: ['X_train', 'X_test', 'y_train', 'y_test']\n[I 2024-09-27 16:40:34,351] A new study created in memory with name: no-name-e0c20963-17b7-4920-af3b-de01921f0a88\n/opt/anaconda3/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n  warnings.warn(f\"Device used : {self.device}\")\n[W 2024-09-27 16:40:34,354] Trial 0 failed with parameters: {'model_type': 'tabnet', 'n_d': 63, 'n_a': 57, 'n_steps': 9, 'gamma': 1.5113072468767585, 'lambda_sparse': 0.0006655447752303417} because of the following error: ValueError('Found array with dim 3. None expected <= 2.').\nTraceback (most recent call last):\n  File \"/opt/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n    value_or_values = func(trial)\n                      ^^^^^^^^^^^\n  File \"/Users/vishesh/gw-workspace/R2jf7evKMmdc/auto_ml.py\", line 100, in objective\n    model = create_model(trial)\n            ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/vishesh/gw-workspace/R2jf7evKMmdc/auto_ml.py\", line 82, in create_model\n    tabnet_model.fit(\n  File \"/opt/anaconda3/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py\", line 217, in fit\n    check_input(X_train)\n  File \"/opt/anaconda3/lib/python3.11/site-packages/pytorch_tabnet/utils.py\", line 507, in check_input\n    check_array(X, accept_sparse=True)\n  File \"/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py\", line 915, in check_array\n    raise ValueError(\nValueError: Found array with dim 3. None expected <= 2.\n[W 2024-09-27 16:40:34,355] Trial 0 failed with value None.\nTraceback (most recent call last):\n  File \"/Users/vishesh/gw-workspace/R2jf7evKMmdc/auto_ml.py\", line 124, in <module>\n    study.optimize(objective, n_trials=10)  # Set n_trials as needed\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/optuna/study/study.py\", line 475, in optimize\n    _optimize(\n  File \"/opt/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 63, in _optimize\n    _optimize_sequential(\n  File \"/opt/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 160, in _optimize_sequential\n    frozen_trial = _run_trial(study, func, catch)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 248, in _run_trial\n    raise func_err\n  File \"/opt/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n    value_or_values = func(trial)\n                      ^^^^^^^^^^^\n  File \"/Users/vishesh/gw-workspace/R2jf7evKMmdc/auto_ml.py\", line 100, in objective\n    model = create_model(trial)\n            ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/vishesh/gw-workspace/R2jf7evKMmdc/auto_ml.py\", line 82, in create_model\n    tabnet_model.fit(\n  File \"/opt/anaconda3/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py\", line 217, in fit\n    check_input(X_train)\n  File \"/opt/anaconda3/lib/python3.11/site-packages/pytorch_tabnet/utils.py\", line 507, in check_input\n    check_array(X, accept_sparse=True)\n  File \"/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py\", line 915, in check_array\n    raise ValueError(\nValueError: Found array with dim 3. None expected <= 2.\n",
  "history_begin_time" : 1727469630270,
  "history_end_time" : 1727469634818,
  "history_notes" : null,
  "history_process" : "vnrvoi",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "OsKBh3i9Jmwe",
  "history_input" : "import numpy as np\nimport optuna\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import Dense, Dropout, Conv1D, MaxPooling1D, Flatten, LSTM, Input, MultiHeadAttention, LayerNormalization, Add\nfrom tensorflow.keras.optimizers import Adam\nfrom pytorch_tabnet.tab_model import TabNetRegressor\n\n# Load preprocessed data\ndata_path = '/Users/vishesh/gw-workspace/L6gDfkNJofh8/preprocessed_data.npz'\ndata = np.load(data_path)\n\n# List all variable names in the .npz file to identify contents\nprint(\"Variables in the .npz file:\", data.files)\n\n# Load data based on contents of the .npz file\nX_train = data['X_train']\ny_train = data['y_train']\n\n# Check if 'X_val' and 'y_val' exist; if not, create validation split\nif 'X_val' in data.files and 'y_val' in data.files:\n    X_val = data['X_val']\n    y_val = data['y_val']\nelse:\n    # Split the data into training and validation sets\n    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n\n# Initialize a dictionary to store model performance\nmodel_performance = {}\n\n# Define the function to create various models\ndef create_model(trial):\n    input_shape = X_train.shape[1]\n    model_type = trial.suggest_categorical('model_type', ['dense', 'cnn', 'lstm', 'transformer', 'tabnet'])\n\n    if model_type == 'dense':\n        model = Sequential()\n        model.add(Input(shape=(input_shape,)))  # Use Input layer to specify input shape\n        num_layers = trial.suggest_int('num_layers', 1, 5)\n        for i in range(num_layers):\n            num_units = trial.suggest_int(f'num_units_l{i}', 16, 128, log=True)\n            model.add(Dense(num_units, activation='relu'))\n            dropout_rate = trial.suggest_float(f'dropout_rate_l{i}', 0.1, 0.5)\n            model.add(Dropout(dropout_rate))\n        model.add(Dense(1, activation='linear'))\n    \n    elif model_type == 'cnn':\n        model = Sequential()\n        model.add(Input(shape=(input_shape, 1)))  # Use Input layer for CNN input shape\n        model.add(Conv1D(filters=trial.suggest_int('filters', 16, 64, log=True),\n                         kernel_size=trial.suggest_int('kernel_size', 3, 5),\n                         activation='relu'))\n        model.add(MaxPooling1D(pool_size=2))\n        model.add(Flatten())\n        model.add(Dense(1, activation='linear'))\n    \n    elif model_type == 'lstm':\n        model = Sequential()\n        model.add(Input(shape=(input_shape, 1)))  # Use Input layer for LSTM input shape\n        model.add(LSTM(units=trial.suggest_int('lstm_units', 16, 64, log=True)))\n        model.add(Dense(1, activation='linear'))\n    \n    elif model_type == 'transformer':\n        inputs = Input(shape=(input_shape, 1))\n        attention = MultiHeadAttention(num_heads=trial.suggest_int('num_heads', 2, 8), key_dim=trial.suggest_int('key_dim', 16, 64))(inputs, inputs)\n        attention = Add()([inputs, attention])\n        attention = LayerNormalization(epsilon=1e-6)(attention)\n        outputs = Flatten()(attention)\n        outputs = Dense(1, activation='linear')(outputs)\n        model = Model(inputs=inputs, outputs=outputs)\n    \n    elif model_type == 'tabnet':\n        tabnet_model = TabNetRegressor(\n            n_d=trial.suggest_int('n_d', 8, 64),\n            n_a=trial.suggest_int('n_a', 8, 64),\n            n_steps=trial.suggest_int('n_steps', 3, 10),\n            gamma=trial.suggest_float('gamma', 1.0, 2.0),\n            lambda_sparse=trial.suggest_float('lambda_sparse', 1e-6, 1e-3)\n        )\n        tabnet_model.fit(\n            X_train, y_train,\n            eval_set=[(X_val, y_val)],\n            eval_metric=['mae'],\n            max_epochs=100,\n            patience=10,\n            batch_size=256,\n            virtual_batch_size=128\n        )\n        return tabnet_model\n\n    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-2, log=True)\n    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mean_squared_error', metrics=['mean_absolute_error'])\n\n    return model\n\n# Define the objective function for Optuna\ndef objective(trial):\n    model = create_model(trial)\n\n    if isinstance(model, TabNetRegressor):\n        model.fit(\n            X_train, y_train,\n            eval_set=[(X_val, y_val)],\n            eval_metric=['mae'],\n            max_epochs=100,\n            patience=10,\n            batch_size=256,\n            virtual_batch_size=128\n        )\n        y_pred = model.predict(X_val)\n        mae = mean_absolute_error(y_val, y_pred)\n    else:\n        model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10, batch_size=32, verbose=0)\n        _, mae = model.evaluate(X_val, y_val, verbose=0)\n\n    model_type = trial.params['model_type']\n    model_performance[model_type] = mae\n    return mae\n\n# Run the Bayesian optimization\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=10)  # Set n_trials as needed\n\n# Get the best hyperparameters\nbest_params = study.best_params\nprint(f\"Best hyperparameters: {best_params}\")\n\n# Display performance of each model\nprint(\"Model performance (MAE):\")\nfor model_type, mae in model_performance.items():\n    print(f\"{model_type}: {mae}\")\n\n# Plot the model performance\nplt.figure(figsize=(10, 5))\nplt.bar(model_performance.keys(), model_performance.values(), color='skyblue')\nplt.xlabel('Model Type')\nplt.ylabel('Mean Absolute Error')\nplt.title('Model Performance Comparison')\nplt.show()\n",
  "history_output" : "Variables in the .npz file: ['X_train', 'X_test', 'y_train', 'y_test']\n[I 2024-09-27 16:38:58,932] A new study created in memory with name: no-name-12ef966c-98ce-4104-bdd6-07ead26cf605\n/opt/anaconda3/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n  warnings.warn(f\"Device used : {self.device}\")\n[W 2024-09-27 16:38:58,935] Trial 0 failed with parameters: {'model_type': 'tabnet', 'n_d': 45, 'n_a': 47, 'n_steps': 9, 'gamma': 1.9024164078477264, 'lambda_sparse': 0.00037972911398464435} because of the following error: ValueError('Found array with dim 3. None expected <= 2.').\nTraceback (most recent call last):\n  File \"/opt/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n    value_or_values = func(trial)\n                      ^^^^^^^^^^^\n  File \"/Users/vishesh/gw-workspace/OsKBh3i9Jmwe/auto_ml.py\", line 100, in objective\n    model = create_model(trial)\n            ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/vishesh/gw-workspace/OsKBh3i9Jmwe/auto_ml.py\", line 82, in create_model\n    tabnet_model.fit(\n  File \"/opt/anaconda3/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py\", line 217, in fit\n    check_input(X_train)\n  File \"/opt/anaconda3/lib/python3.11/site-packages/pytorch_tabnet/utils.py\", line 507, in check_input\n    check_array(X, accept_sparse=True)\n  File \"/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py\", line 915, in check_array\n    raise ValueError(\nValueError: Found array with dim 3. None expected <= 2.\n[W 2024-09-27 16:38:58,936] Trial 0 failed with value None.\nTraceback (most recent call last):\n  File \"/Users/vishesh/gw-workspace/OsKBh3i9Jmwe/auto_ml.py\", line 124, in <module>\n    study.optimize(objective, n_trials=10)  # Set n_trials as needed\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/optuna/study/study.py\", line 475, in optimize\n    _optimize(\n  File \"/opt/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 63, in _optimize\n    _optimize_sequential(\n  File \"/opt/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 160, in _optimize_sequential\n    frozen_trial = _run_trial(study, func, catch)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 248, in _run_trial\n    raise func_err\n  File \"/opt/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n    value_or_values = func(trial)\n                      ^^^^^^^^^^^\n  File \"/Users/vishesh/gw-workspace/OsKBh3i9Jmwe/auto_ml.py\", line 100, in objective\n    model = create_model(trial)\n            ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/vishesh/gw-workspace/OsKBh3i9Jmwe/auto_ml.py\", line 82, in create_model\n    tabnet_model.fit(\n  File \"/opt/anaconda3/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py\", line 217, in fit\n    check_input(X_train)\n  File \"/opt/anaconda3/lib/python3.11/site-packages/pytorch_tabnet/utils.py\", line 507, in check_input\n    check_array(X, accept_sparse=True)\n  File \"/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py\", line 915, in check_array\n    raise ValueError(\nValueError: Found array with dim 3. None expected <= 2.\n",
  "history_begin_time" : 1727469534449,
  "history_end_time" : 1727469539492,
  "history_notes" : null,
  "history_process" : "vnrvoi",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "halX59oKGToc",
  "history_input" : "import numpy as np\nimport optuna\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import Dense, Dropout, Conv1D, MaxPooling1D, Flatten, LSTM, Input, MultiHeadAttention, LayerNormalization, Add\nfrom tensorflow.keras.optimizers import Adam\nfrom pytorch_tabnet.tab_model import TabNetRegressor\n\n# Load preprocessed data\ndata_path = '/Users/vishesh/gw-workspace/L6gDfkNJofh8/preprocessed_data.npz'\ndata = np.load(data_path)\n\n# List all variable names in the .npz file to identify contents\nprint(\"Variables in the .npz file:\", data.files)\n\n# Load data based on contents of the .npz file\nX_train = data['X_train']\ny_train = data['y_train']\n\n# Check if 'X_val' and 'y_val' exist; if not, create validation split\nif 'X_val' in data.files and 'y_val' in data.files:\n    X_val = data['X_val']\n    y_val = data['y_val']\nelse:\n    # Split the data into training and validation sets\n    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n\n# Initialize a dictionary to store model performance\nmodel_performance = {}\n\n# Define the function to create various models\ndef create_model(trial):\n    input_shape = X_train.shape[1]\n    model_type = trial.suggest_categorical('model_type', ['dense', 'cnn', 'lstm', 'transformer', 'tabnet'])\n\n    if model_type == 'dense':\n        model = Sequential()\n        model.add(Input(shape=(input_shape,)))  # Use Input layer to specify input shape\n        num_layers = trial.suggest_int('num_layers', 1, 5)\n        for i in range(num_layers):\n            num_units = trial.suggest_int(f'num_units_l{i}', 16, 128, log=True)\n            model.add(Dense(num_units, activation='relu'))\n            dropout_rate = trial.suggest_float(f'dropout_rate_l{i}', 0.1, 0.5)\n            model.add(Dropout(dropout_rate))\n        model.add(Dense(1, activation='linear'))\n    \n    elif model_type == 'cnn':\n        model = Sequential()\n        model.add(Input(shape=(input_shape, 1)))  # Use Input layer for CNN input shape\n        model.add(Conv1D(filters=trial.suggest_int('filters', 16, 64, log=True),\n                         kernel_size=trial.suggest_int('kernel_size', 3, 5),\n                         activation='relu'))\n        model.add(MaxPooling1D(pool_size=2))\n        model.add(Flatten())\n        model.add(Dense(1, activation='linear'))\n    \n    elif model_type == 'lstm':\n        model = Sequential()\n        model.add(Input(shape=(input_shape, 1)))  # Use Input layer for LSTM input shape\n        model.add(LSTM(units=trial.suggest_int('lstm_units', 16, 64, log=True)))\n        model.add(Dense(1, activation='linear'))\n    \n    elif model_type == 'transformer':\n        inputs = Input(shape=(input_shape, 1))\n        attention = MultiHeadAttention(num_heads=trial.suggest_int('num_heads', 2, 8), key_dim=trial.suggest_int('key_dim', 16, 64))(inputs, inputs)\n        attention = Add()([inputs, attention])\n        attention = LayerNormalization(epsilon=1e-6)(attention)\n        outputs = Flatten()(attention)\n        outputs = Dense(1, activation='linear')(outputs)\n        model = Model(inputs=inputs, outputs=outputs)\n    \n    elif model_type == 'tabnet':\n        tabnet_model = TabNetRegressor(\n            n_d=trial.suggest_int('n_d', 8, 64),\n            n_a=trial.suggest_int('n_a', 8, 64),\n            n_steps=trial.suggest_int('n_steps', 3, 10),\n            gamma=trial.suggest_float('gamma', 1.0, 2.0),\n            lambda_sparse=trial.suggest_float('lambda_sparse', 1e-6, 1e-3)\n        )\n        tabnet_model.fit(\n            X_train, y_train,\n            eval_set=[(X_val, y_val)],\n            eval_metric=['mae'],\n            max_epochs=100,\n            patience=10,\n            batch_size=256,\n            virtual_batch_size=128\n        )\n        return tabnet_model\n\n    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-2, log=True)\n    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mean_squared_error', metrics=['mean_absolute_error'])\n\n    return model\n\n# Define the objective function for Optuna\ndef objective(trial):\n    model = create_model(trial)\n\n    if isinstance(model, TabNetRegressor):\n        model.fit(\n            X_train, y_train,\n            eval_set=[(X_val, y_val)],\n            eval_metric=['mae'],\n            max_epochs=100,\n            patience=10,\n            batch_size=256,\n            virtual_batch_size=128\n        )\n        y_pred = model.predict(X_val)\n        mae = mean_absolute_error(y_val, y_pred)\n    else:\n        model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10, batch_size=32, verbose=0)\n        _, mae = model.evaluate(X_val, y_val, verbose=0)\n\n    model_type = trial.params['model_type']\n    model_performance[model_type] = mae\n    return mae\n\n# Run the Bayesian optimization\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=10)  # Set n_trials as needed\n\n# Get the best hyperparameters\nbest_params = study.best_params\nprint(f\"Best hyperparameters: {best_params}\")\n\n# Display performance of each model\nprint(\"Model performance (MAE):\")\nfor model_type, mae in model_performance.items():\n    print(f\"{model_type}: {mae}\")\n\n# Plot the model performance\nplt.figure(figsize=(10, 5))\nplt.bar(model_performance.keys(), model_performance.values(), color='skyblue')\nplt.xlabel('Model Type')\nplt.ylabel('Mean Absolute Error')\nplt.title('Model Performance Comparison')\nplt.show()\n",
  "history_output" : "Variables in the .npz file: ['X_train', 'X_test', 'y_train', 'y_test']\n[I 2024-09-27 16:37:15,334] A new study created in memory with name: no-name-13c10792-0ce9-4da1-8c3f-c6e27e089d7b\n[I 2024-09-27 16:37:16,489] Trial 0 finished with value: 0.2010091245174408 and parameters: {'model_type': 'dense', 'num_layers': 5, 'num_units_l0': 31, 'dropout_rate_l0': 0.2996611744898274, 'num_units_l1': 36, 'dropout_rate_l1': 0.18337947324674964, 'num_units_l2': 74, 'dropout_rate_l2': 0.40971845158469317, 'num_units_l3': 28, 'dropout_rate_l3': 0.473119918069945, 'num_units_l4': 16, 'dropout_rate_l4': 0.47713053749138723, 'learning_rate': 0.00010269557690082855}. Best is trial 0 with value: 0.2010091245174408.\n[I 2024-09-27 16:37:17,775] Trial 1 finished with value: 0.037778694182634354 and parameters: {'model_type': 'lstm', 'lstm_units': 42, 'learning_rate': 0.0021341043957174544}. Best is trial 1 with value: 0.037778694182634354.\n[I 2024-09-27 16:37:18,484] Trial 2 finished with value: 0.056918323040008545 and parameters: {'model_type': 'dense', 'num_layers': 1, 'num_units_l0': 79, 'dropout_rate_l0': 0.3338067394620643, 'learning_rate': 0.00021899150834146243}. Best is trial 1 with value: 0.037778694182634354.\n/opt/anaconda3/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n  warnings.warn(f\"Device used : {self.device}\")\n[W 2024-09-27 16:37:18,488] Trial 3 failed with parameters: {'model_type': 'tabnet', 'n_d': 25, 'n_a': 46, 'n_steps': 5, 'gamma': 1.4565066264545905, 'lambda_sparse': 0.00035514214050578867} because of the following error: ValueError('Found array with dim 3. None expected <= 2.').\nTraceback (most recent call last):\n  File \"/opt/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n    value_or_values = func(trial)\n                      ^^^^^^^^^^^\n  File \"/Users/vishesh/gw-workspace/halX59oKGToc/auto_ml.py\", line 100, in objective\n    model = create_model(trial)\n            ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/vishesh/gw-workspace/halX59oKGToc/auto_ml.py\", line 82, in create_model\n    tabnet_model.fit(\n  File \"/opt/anaconda3/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py\", line 217, in fit\n    check_input(X_train)\n  File \"/opt/anaconda3/lib/python3.11/site-packages/pytorch_tabnet/utils.py\", line 507, in check_input\n    check_array(X, accept_sparse=True)\n  File \"/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py\", line 915, in check_array\n    raise ValueError(\nValueError: Found array with dim 3. None expected <= 2.\n[W 2024-09-27 16:37:18,489] Trial 3 failed with value None.\nTraceback (most recent call last):\n  File \"/Users/vishesh/gw-workspace/halX59oKGToc/auto_ml.py\", line 124, in <module>\n    study.optimize(objective, n_trials=10)  # Set n_trials as needed\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/optuna/study/study.py\", line 475, in optimize\n    _optimize(\n  File \"/opt/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 63, in _optimize\n    _optimize_sequential(\n  File \"/opt/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 160, in _optimize_sequential\n    frozen_trial = _run_trial(study, func, catch)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 248, in _run_trial\n    raise func_err\n  File \"/opt/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n    value_or_values = func(trial)\n                      ^^^^^^^^^^^\n  File \"/Users/vishesh/gw-workspace/halX59oKGToc/auto_ml.py\", line 100, in objective\n    model = create_model(trial)\n            ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/vishesh/gw-workspace/halX59oKGToc/auto_ml.py\", line 82, in create_model\n    tabnet_model.fit(\n  File \"/opt/anaconda3/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py\", line 217, in fit\n    check_input(X_train)\n  File \"/opt/anaconda3/lib/python3.11/site-packages/pytorch_tabnet/utils.py\", line 507, in check_input\n    check_array(X, accept_sparse=True)\n  File \"/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py\", line 915, in check_array\n    raise ValueError(\nValueError: Found array with dim 3. None expected <= 2.\n",
  "history_begin_time" : 1727469430988,
  "history_end_time" : 1727469439060,
  "history_notes" : null,
  "history_process" : "vnrvoi",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "EuT18BMklIm2",
  "history_input" : "import numpy as np\nimport optuna\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import Dense, Dropout, Conv1D, MaxPooling1D, Flatten, LSTM, Input, MultiHeadAttention, LayerNormalization, Add\nfrom tensorflow.keras.optimizers import Adam\nfrom pytorch_tabnet.tab_model import TabNetRegressor\n\n# Load preprocessed data\ndata_path = '/Users/vishesh/gw-workspace/L6gDfkNJofh8/preprocessed_data.npz'\ndata = np.load(data_path)\n\n# List all variable names in the .npz file to identify contents\nprint(\"Variables in the .npz file:\", data.files)\n\n# Load data based on contents of the .npz file\nX_train = data['X_train']\ny_train = data['y_train']\n\n# Check if 'X_val' and 'y_val' exist; if not, create validation split\nif 'X_val' in data.files and 'y_val' in data.files:\n    X_val = data['X_val']\n    y_val = data['y_val']\nelse:\n    # Split the data into training and validation sets\n    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n\n# Initialize a dictionary to store model performance\nmodel_performance = {}\n\n# Define the function to create various models\ndef create_model(trial):\n    input_shape = X_train.shape[1]\n    model_type = trial.suggest_categorical('model_type', ['dense', 'cnn', 'lstm', 'transformer', 'tabnet'])\n\n    if model_type == 'dense':\n        model = Sequential()\n        model.add(Input(shape=(input_shape,)))  # Use Input layer to specify input shape\n        num_layers = trial.suggest_int('num_layers', 1, 5)\n        for i in range(num_layers):\n            num_units = trial.suggest_int(f'num_units_l{i}', 16, 128, log=True)\n            model.add(Dense(num_units, activation='relu'))\n            dropout_rate = trial.suggest_float(f'dropout_rate_l{i}', 0.1, 0.5)\n            model.add(Dropout(dropout_rate))\n        model.add(Dense(1, activation='linear'))\n    \n    elif model_type == 'cnn':\n        model = Sequential()\n        model.add(Input(shape=(input_shape, 1)))  # Use Input layer for CNN input shape\n        model.add(Conv1D(filters=trial.suggest_int('filters', 16, 64, log=True),\n                         kernel_size=trial.suggest_int('kernel_size', 3, 5),\n                         activation='relu'))\n        model.add(MaxPooling1D(pool_size=2))\n        model.add(Flatten())\n        model.add(Dense(1, activation='linear'))\n    \n    elif model_type == 'lstm':\n        model = Sequential()\n        model.add(Input(shape=(input_shape, 1)))  # Use Input layer for LSTM input shape\n        model.add(LSTM(units=trial.suggest_int('lstm_units', 16, 64, log=True)))\n        model.add(Dense(1, activation='linear'))\n    \n    elif model_type == 'transformer':\n        inputs = Input(shape=(input_shape, 1))\n        attention = MultiHeadAttention(num_heads=trial.suggest_int('num_heads', 2, 8), key_dim=trial.suggest_int('key_dim', 16, 64))(inputs, inputs)\n        attention = Add()([inputs, attention])\n        attention = LayerNormalization(epsilon=1e-6)(attention)\n        outputs = Flatten()(attention)\n        outputs = Dense(1, activation='linear')(outputs)\n        model = Model(inputs=inputs, outputs=outputs)\n    \n    elif model_type == 'tabnet':\n        tabnet_model = TabNetRegressor(\n            n_d=trial.suggest_int('n_d', 8, 64),\n            n_a=trial.suggest_int('n_a', 8, 64),\n            n_steps=trial.suggest_int('n_steps', 3, 10),\n            gamma=trial.suggest_float('gamma', 1.0, 2.0),\n            lambda_sparse=trial.suggest_float('lambda_sparse', 1e-6, 1e-3)\n        )\n        tabnet_model.fit(\n            X_train, y_train,\n            eval_set=[(X_val, y_val)],\n            eval_metric=['mae'],\n            max_epochs=100,\n            patience=10,\n            batch_size=256,\n            virtual_batch_size=128\n        )\n        return tabnet_model\n\n\n\n    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-2, log=True)\n    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mean_squared_error', metrics=['mean_absolute_error'])\n\n    return model\n\n# Define the objective function for Optuna\ndef objective(trial):\n    model = create_model(trial)\n\n    if isinstance(model, TabNetRegressor):\n        model.fit(\n            X_train, y_train,\n            eval_set=[(X_val, y_val)],\n            eval_metric=['mae'],\n            max_epochs=100,\n            patience=10,\n            batch_size=256,\n            virtual_batch_size=128\n        )\n        y_pred = model.predict(X_val)\n        mae = mean_absolute_error(y_val, y_pred)\n    else:\n        model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10, batch_size=32, verbose=0)\n        _, mae = model.evaluate(X_val, y_val, verbose=0)\n\n    model_type = trial.params['model_type']\n    model_performance[model_type] = mae\n    return mae\n\n# Run the Bayesian optimization\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=10)  # Set n_trials as needed\n\n# Get the best hyperparameters\nbest_params = study.best_params\nprint(f\"Best hyperparameters: {best_params}\")\n\n# Display performance of each model\nprint(\"Model performance (MAE):\")\nfor model_type, mae in model_performance.items():\n    print(f\"{model_type}: {mae}\")\n\n# Plot the model performance\nplt.figure(figsize=(10, 5))\nplt.bar(model_performance.keys(), model_performance.values(), color='skyblue')\nplt.xlabel('Model Type')\nplt.ylabel('Mean Absolute Error')\nplt.title('Model Performance Comparison')\nplt.show()\n",
  "history_output" : "Variables in the .npz file: ['X_train', 'X_test', 'y_train', 'y_test']\n[I 2024-09-27 16:36:53,544] A new study created in memory with name: no-name-1fdcd463-429a-4cb4-b4df-10f1973f0107\n[I 2024-09-27 16:36:54,274] Trial 0 finished with value: 0.04954395815730095 and parameters: {'model_type': 'cnn', 'filters': 35, 'kernel_size': 4, 'learning_rate': 0.009797480813147248}. Best is trial 0 with value: 0.04954395815730095.\n[I 2024-09-27 16:36:54,863] Trial 1 finished with value: 0.055542267858982086 and parameters: {'model_type': 'cnn', 'filters': 26, 'kernel_size': 3, 'learning_rate': 0.0006084176355862676}. Best is trial 0 with value: 0.04954395815730095.\n[I 2024-09-27 16:36:55,670] Trial 2 finished with value: 0.09473460167646408 and parameters: {'model_type': 'dense', 'num_layers': 3, 'num_units_l0': 36, 'dropout_rate_l0': 0.43050440850038424, 'num_units_l1': 96, 'dropout_rate_l1': 0.34724237782311457, 'num_units_l2': 56, 'dropout_rate_l2': 0.14558491136389612, 'learning_rate': 7.740009925895374e-05}. Best is trial 0 with value: 0.04954395815730095.\n/opt/anaconda3/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n  warnings.warn(f\"Device used : {self.device}\")\n[W 2024-09-27 16:36:55,673] Trial 3 failed with parameters: {'model_type': 'tabnet', 'n_d': 10, 'n_a': 32, 'n_steps': 8, 'gamma': 1.299660913993013, 'lambda_sparse': 0.0008548800081805192} because of the following error: ValueError('Found array with dim 3. None expected <= 2.').\nTraceback (most recent call last):\n  File \"/opt/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n    value_or_values = func(trial)\n                      ^^^^^^^^^^^\n  File \"/Users/vishesh/gw-workspace/EuT18BMklIm2/auto_ml.py\", line 102, in objective\n    model = create_model(trial)\n            ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/vishesh/gw-workspace/EuT18BMklIm2/auto_ml.py\", line 82, in create_model\n    tabnet_model.fit(\n  File \"/opt/anaconda3/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py\", line 217, in fit\n    check_input(X_train)\n  File \"/opt/anaconda3/lib/python3.11/site-packages/pytorch_tabnet/utils.py\", line 507, in check_input\n    check_array(X, accept_sparse=True)\n  File \"/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py\", line 915, in check_array\n    raise ValueError(\nValueError: Found array with dim 3. None expected <= 2.\n[W 2024-09-27 16:36:55,674] Trial 3 failed with value None.\nTraceback (most recent call last):\n  File \"/Users/vishesh/gw-workspace/EuT18BMklIm2/auto_ml.py\", line 126, in <module>\n    study.optimize(objective, n_trials=10)  # Set n_trials as needed\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/optuna/study/study.py\", line 475, in optimize\n    _optimize(\n  File \"/opt/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 63, in _optimize\n    _optimize_sequential(\n  File \"/opt/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 160, in _optimize_sequential\n    frozen_trial = _run_trial(study, func, catch)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 248, in _run_trial\n    raise func_err\n  File \"/opt/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n    value_or_values = func(trial)\n                      ^^^^^^^^^^^\n  File \"/Users/vishesh/gw-workspace/EuT18BMklIm2/auto_ml.py\", line 102, in objective\n    model = create_model(trial)\n            ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/vishesh/gw-workspace/EuT18BMklIm2/auto_ml.py\", line 82, in create_model\n    tabnet_model.fit(\n  File \"/opt/anaconda3/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py\", line 217, in fit\n    check_input(X_train)\n  File \"/opt/anaconda3/lib/python3.11/site-packages/pytorch_tabnet/utils.py\", line 507, in check_input\n    check_array(X, accept_sparse=True)\n  File \"/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py\", line 915, in check_array\n    raise ValueError(\nValueError: Found array with dim 3. None expected <= 2.\n",
  "history_begin_time" : 1727469408810,
  "history_end_time" : 1727469416327,
  "history_notes" : null,
  "history_process" : "vnrvoi",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "Nf571le1bjSE",
  "history_input" : "import numpy as np\nimport optuna\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import Dense, Dropout, Conv1D, MaxPooling1D, Flatten, LSTM, Input, MultiHeadAttention, LayerNormalization, Add\nfrom tensorflow.keras.optimizers import Adam\nfrom pytorch_tabnet.tab_model import TabNetRegressor\n\n# Load preprocessed data\ndata_path = '/Users/vishesh/gw-workspace/L6gDfkNJofh8/preprocessed_data.npz'\ndata = np.load(data_path)\n\n# List all variable names in the .npz file to identify contents\nprint(\"Variables in the .npz file:\", data.files)\n\n# Load data based on contents of the .npz file\nX_train = data['X_train']\ny_train = data['y_train']\n\n# Check if 'X_val' and 'y_val' exist; if not, create validation split\nif 'X_val' in data.files and 'y_val' in data.files:\n    X_val = data['X_val']\n    y_val = data['y_val']\nelse:\n    # Split the data into training and validation sets\n    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n\n# Initialize a dictionary to store model performance\nmodel_performance = {}\n\n# Define the function to create various models\ndef create_model(trial):\n    input_shape = X_train.shape[1]\n    model_type = trial.suggest_categorical('model_type', ['dense', 'cnn', 'lstm', 'transformer', 'tabnet'])\n\n    if model_type == 'dense':\n        model = Sequential()\n        model.add(Input(shape=(input_shape,)))  # Use Input layer to specify input shape\n        num_layers = trial.suggest_int('num_layers', 1, 5)\n        for i in range(num_layers):\n            num_units = trial.suggest_int(f'num_units_l{i}', 16, 128, log=True)\n            model.add(Dense(num_units, activation='relu'))\n            dropout_rate = trial.suggest_float(f'dropout_rate_l{i}', 0.1, 0.5)\n            model.add(Dropout(dropout_rate))\n        model.add(Dense(1, activation='linear'))\n    \n    elif model_type == 'cnn':\n        model = Sequential()\n        model.add(Input(shape=(input_shape, 1)))  # Use Input layer for CNN input shape\n        model.add(Conv1D(filters=trial.suggest_int('filters', 16, 64, log=True),\n                         kernel_size=trial.suggest_int('kernel_size', 3, 5),\n                         activation='relu'))\n        model.add(MaxPooling1D(pool_size=2))\n        model.add(Flatten())\n        model.add(Dense(1, activation='linear'))\n    \n    elif model_type == 'lstm':\n        model = Sequential()\n        model.add(Input(shape=(input_shape, 1)))  # Use Input layer for LSTM input shape\n        model.add(LSTM(units=trial.suggest_int('lstm_units', 16, 64, log=True)))\n        model.add(Dense(1, activation='linear'))\n    \n    elif model_type == 'transformer':\n        inputs = Input(shape=(input_shape, 1))\n        attention = MultiHeadAttention(num_heads=trial.suggest_int('num_heads', 2, 8), key_dim=trial.suggest_int('key_dim', 16, 64))(inputs, inputs)\n        attention = Add()([inputs, attention])\n        attention = LayerNormalization(epsilon=1e-6)(attention)\n        outputs = Flatten()(attention)\n        outputs = Dense(1, activation='linear')(outputs)\n        model = Model(inputs=inputs, outputs=outputs)\n    \n    elif model_type == 'tabnet':\n        tabnet_model = TabNetRegressor(\n            n_d=trial.suggest_int('n_d', 8, 64),\n            n_a=trial.suggest_int('n_a', 8, 64),\n            n_steps=trial.suggest_int('n_steps', 3, 10),\n            gamma=trial.suggest_float('gamma', 1.0, 2.0),\n            lambda_sparse=trial.suggest_float('lambda_sparse', 1e-6, 1e-3)\n        )\n        tabnet_model.fit(\n            X_train, y_train,\n            eval_set=[(X_val, y_val)],\n            eval_metric=['mae'],\n            max_epochs=100,\n            patience=10,\n            batch_size=256,\n            virtual_batch_size=128\n        )\n        return tabnet_model\n\n\n\n    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-2, log=True)\n    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mean_squared_error', metrics=['mean_absolute_error'])\n\n    return model\n\n# Define the objective function for Optuna\ndef objective(trial):\n    model = create_model(trial)\n\n    if isinstance(model, TabNetRegressor):\n        model.fit(\n            X_train, y_train,\n            eval_set=[(X_val, y_val)],\n            eval_metric=['mae'],\n            max_epochs=100,\n            patience=10,\n            batch_size=256,\n            virtual_batch_size=128\n        )\n        y_pred = model.predict(X_val)\n        mae = mean_absolute_error(y_val, y_pred)\n    else:\n        model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10, batch_size=32, verbose=0)\n        _, mae = model.evaluate(X_val, y_val, verbose=0)\n\n    model_type = trial.params['model_type']\n    model_performance[model_type] = mae\n    return mae\n\n# Run the Bayesian optimization\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=10)  # Set n_trials as needed\n\n# Get the best hyperparameters\nbest_params = study.best_params\nprint(f\"Best hyperparameters: {best_params}\")\n\n# Display performance of each model\nprint(\"Model performance (MAE):\")\nfor model_type, mae in model_performance.items():\n    print(f\"{model_type}: {mae}\")\n\n# Plot the model performance\nplt.figure(figsize=(10, 5))\nplt.bar(model_performance.keys(), model_performance.values(), color='skyblue')\nplt.xlabel('Model Type')\nplt.ylabel('Mean Absolute Error')\nplt.title('Model Performance Comparison')\nplt.show()\n",
  "history_output" : "Variables in the .npz file: ['X_train', 'X_test', 'y_train', 'y_test']\n[I 2024-09-27 16:30:51,598] A new study created in memory with name: no-name-2823473f-4d8a-4336-92af-4a658bd5e015\n[I 2024-09-27 16:30:52,164] Trial 0 finished with value: 0.05078238621354103 and parameters: {'model_type': 'cnn', 'filters': 18, 'kernel_size': 4, 'learning_rate': 0.008159673088260657}. Best is trial 0 with value: 0.05078238621354103.\n/opt/anaconda3/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n  warnings.warn(f\"Device used : {self.device}\")\n[W 2024-09-27 16:30:52,167] Trial 1 failed with parameters: {'model_type': 'tabnet', 'n_d': 38, 'n_a': 43, 'n_steps': 10, 'gamma': 1.3906000649164825, 'lambda_sparse': 0.0007240416562975596} because of the following error: ValueError('Found array with dim 3. None expected <= 2.').\nTraceback (most recent call last):\n  File \"/opt/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n    value_or_values = func(trial)\n                      ^^^^^^^^^^^\n  File \"/Users/vishesh/gw-workspace/Nf571le1bjSE/auto_ml.py\", line 102, in objective\n    model = create_model(trial)\n            ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/vishesh/gw-workspace/Nf571le1bjSE/auto_ml.py\", line 82, in create_model\n    tabnet_model.fit(\n  File \"/opt/anaconda3/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py\", line 217, in fit\n    check_input(X_train)\n  File \"/opt/anaconda3/lib/python3.11/site-packages/pytorch_tabnet/utils.py\", line 507, in check_input\n    check_array(X, accept_sparse=True)\n  File \"/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py\", line 915, in check_array\n    raise ValueError(\nValueError: Found array with dim 3. None expected <= 2.\n[W 2024-09-27 16:30:52,168] Trial 1 failed with value None.\nTraceback (most recent call last):\n  File \"/Users/vishesh/gw-workspace/Nf571le1bjSE/auto_ml.py\", line 126, in <module>\n    study.optimize(objective, n_trials=10)  # Set n_trials as needed\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/optuna/study/study.py\", line 475, in optimize\n    _optimize(\n  File \"/opt/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 63, in _optimize\n    _optimize_sequential(\n  File \"/opt/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 160, in _optimize_sequential\n    frozen_trial = _run_trial(study, func, catch)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 248, in _run_trial\n    raise func_err\n  File \"/opt/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n    value_or_values = func(trial)\n                      ^^^^^^^^^^^\n  File \"/Users/vishesh/gw-workspace/Nf571le1bjSE/auto_ml.py\", line 102, in objective\n    model = create_model(trial)\n            ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/vishesh/gw-workspace/Nf571le1bjSE/auto_ml.py\", line 82, in create_model\n    tabnet_model.fit(\n  File \"/opt/anaconda3/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py\", line 217, in fit\n    check_input(X_train)\n  File \"/opt/anaconda3/lib/python3.11/site-packages/pytorch_tabnet/utils.py\", line 507, in check_input\n    check_array(X, accept_sparse=True)\n  File \"/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py\", line 915, in check_array\n    raise ValueError(\nValueError: Found array with dim 3. None expected <= 2.\n",
  "history_begin_time" : 1727469046944,
  "history_end_time" : 1727469052640,
  "history_notes" : null,
  "history_process" : "vnrvoi",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "eeZKNVA1B9aq",
  "history_input" : "import numpy as np\nimport optuna\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import Dense, Dropout, Conv1D, MaxPooling1D, Flatten, LSTM, Input, MultiHeadAttention, LayerNormalization, Add\nfrom tensorflow.keras.optimizers import Adam\nfrom pytorch_tabnet.tab_model import TabNetRegressor\n\n# Load preprocessed data\ndata_path = '/Users/vishesh/gw-workspace/L6gDfkNJofh8/preprocessed_data.npz'\ndata = np.load(data_path)\n\n# List all variable names in the .npz file to identify contents\nprint(\"Variables in the .npz file:\", data.files)\n\n# Load data based on contents of the .npz file\nX_train = data['X_train']\ny_train = data['y_train']\n\n# Check if 'X_val' and 'y_val' exist; if not, create validation split\nif 'X_val' in data.files and 'y_val' in data.files:\n    X_val = data['X_val']\n    y_val = data['y_val']\nelse:\n    # Split the data into training and validation sets\n    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n\n# Initialize a dictionary to store model performance\nmodel_performance = {}\n\n# Define the function to create various models\ndef create_model(trial):\n    input_shape = X_train.shape[1]\n    model_type = trial.suggest_categorical('model_type', ['dense', 'cnn', 'lstm', 'transformer', 'tabnet'])\n\n    if model_type == 'dense':\n        model = Sequential()\n        model.add(Input(shape=(input_shape,)))  # Use Input layer to specify input shape\n        num_layers = trial.suggest_int('num_layers', 1, 5)\n        for i in range(num_layers):\n            num_units = trial.suggest_int(f'num_units_l{i}', 16, 128, log=True)\n            model.add(Dense(num_units, activation='relu'))\n            dropout_rate = trial.suggest_float(f'dropout_rate_l{i}', 0.1, 0.5)\n            model.add(Dropout(dropout_rate))\n        model.add(Dense(1, activation='linear'))\n    \n    elif model_type == 'cnn':\n        model = Sequential()\n        model.add(Input(shape=(input_shape, 1)))  # Use Input layer for CNN input shape\n        model.add(Conv1D(filters=trial.suggest_int('filters', 16, 64, log=True),\n                         kernel_size=trial.suggest_int('kernel_size', 3, 5),\n                         activation='relu'))\n        model.add(MaxPooling1D(pool_size=2))\n        model.add(Flatten())\n        model.add(Dense(1, activation='linear'))\n    \n    elif model_type == 'lstm':\n        model = Sequential()\n        model.add(Input(shape=(input_shape, 1)))  # Use Input layer for LSTM input shape\n        model.add(LSTM(units=trial.suggest_int('lstm_units', 16, 64, log=True)))\n        model.add(Dense(1, activation='linear'))\n    \n    elif model_type == 'transformer':\n        inputs = Input(shape=(input_shape, 1))\n        attention = MultiHeadAttention(num_heads=trial.suggest_int('num_heads', 2, 8), key_dim=trial.suggest_int('key_dim', 16, 64))(inputs, inputs)\n        attention = Add()([inputs, attention])\n        attention = LayerNormalization(epsilon=1e-6)(attention)\n        outputs = Flatten()(attention)\n        outputs = Dense(1, activation='linear')(outputs)\n        model = Model(inputs=inputs, outputs=outputs)\n    \n    elif model_type == 'tabnet':\n        tabnet_model = TabNetRegressor(\n            n_d=trial.suggest_int('n_d', 8, 64),\n            n_a=trial.suggest_int('n_a', 8, 64),\n            n_steps=trial.suggest_int('n_steps', 3, 10),\n            gamma=trial.suggest_float('gamma', 1.0, 2.0),\n            lambda_sparse=trial.suggest_float('lambda_sparse', 1e-6, 1e-3)\n        )\n        tabnet_model.fit(\n            X_train, y_train,\n            eval_set=[(X_val, y_val)],\n            eval_metric=['mae'],\n            max_epochs=100,\n            patience=10,\n            batch_size=256,\n            virtual_batch_size=128\n        )\n        return tabnet_model\n\n\n\n    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-2, log=True)\n    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mean_squared_error', metrics=['mean_absolute_error'])\n\n    return model\n\n# Define the objective function for Optuna\ndef objective(trial):\n    model = create_model(trial)\n\n    if isinstance(model, TabNetRegressor):\n        model.fit(\n            X_train, y_train,\n            eval_set=[(X_val, y_val)],\n            eval_metric=['mae'],\n            max_epochs=100,\n            patience=10,\n            batch_size=256,\n            virtual_batch_size=128\n        )\n        y_pred = model.predict(X_val)\n        mae = mean_absolute_error(y_val, y_pred)\n    else:\n        model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10, batch_size=32, verbose=0)\n        _, mae = model.evaluate(X_val, y_val, verbose=0)\n\n    model_type = trial.params['model_type']\n    model_performance[model_type] = mae\n    return mae\n\n# Run the Bayesian optimization\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=10)  # Set n_trials as needed\n\n# Get the best hyperparameters\nbest_params = study.best_params\nprint(f\"Best hyperparameters: {best_params}\")\n\n# Display performance of each model\nprint(\"Model performance (MAE):\")\nfor model_type, mae in model_performance.items():\n    print(f\"{model_type}: {mae}\")\n\n# Plot the model performance\nplt.figure(figsize=(10, 5))\nplt.bar(model_performance.keys(), model_performance.values(), color='skyblue')\nplt.xlabel('Model Type')\nplt.ylabel('Mean Absolute Error')\nplt.title('Model Performance Comparison')\nplt.show()\n",
  "history_output" : "Variables in the .npz file: ['X_train', 'X_test', 'y_train', 'y_test']\n[I 2024-09-27 16:29:36,577] A new study created in memory with name: no-name-b5d180a2-b005-4232-b649-22e3ff3c22f2\n[I 2024-09-27 16:29:37,265] Trial 0 finished with value: 0.06047685816884041 and parameters: {'model_type': 'dense', 'num_layers': 2, 'num_units_l0': 46, 'dropout_rate_l0': 0.21148601776816484, 'num_units_l1': 21, 'dropout_rate_l1': 0.4009729900215614, 'learning_rate': 0.00044807625745217915}. Best is trial 0 with value: 0.06047685816884041.\n[I 2024-09-27 16:29:37,810] Trial 1 finished with value: 0.058301858603954315 and parameters: {'model_type': 'cnn', 'filters': 52, 'kernel_size': 4, 'learning_rate': 0.0008647249185986702}. Best is trial 1 with value: 0.058301858603954315.\n[I 2024-09-27 16:29:39,257] Trial 2 finished with value: 0.3841686546802521 and parameters: {'model_type': 'transformer', 'num_heads': 8, 'key_dim': 55, 'learning_rate': 2.0046959829401398e-05}. Best is trial 1 with value: 0.058301858603954315.\n[I 2024-09-27 16:29:39,850] Trial 3 finished with value: 0.4023188650608063 and parameters: {'model_type': 'cnn', 'filters': 16, 'kernel_size': 5, 'learning_rate': 2.4617126555409217e-05}. Best is trial 1 with value: 0.058301858603954315.\n[I 2024-09-27 16:29:40,948] Trial 4 finished with value: 0.061266109347343445 and parameters: {'model_type': 'lstm', 'lstm_units': 38, 'learning_rate': 0.0003795628117035189}. Best is trial 1 with value: 0.058301858603954315.\n[I 2024-09-27 16:29:41,934] Trial 5 finished with value: 0.07572084665298462 and parameters: {'model_type': 'lstm', 'lstm_units': 17, 'learning_rate': 0.0003892649892512853}. Best is trial 1 with value: 0.058301858603954315.\n[I 2024-09-27 16:29:43,145] Trial 6 finished with value: 0.07029891759157181 and parameters: {'model_type': 'lstm', 'lstm_units': 62, 'learning_rate': 0.00012123641813836207}. Best is trial 1 with value: 0.058301858603954315.\n[I 2024-09-27 16:29:43,815] Trial 7 finished with value: 0.0579344742000103 and parameters: {'model_type': 'cnn', 'filters': 25, 'kernel_size': 4, 'learning_rate': 0.0007586641205507409}. Best is trial 7 with value: 0.0579344742000103.\n/opt/anaconda3/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n  warnings.warn(f\"Device used : {self.device}\")\n[W 2024-09-27 16:29:43,818] Trial 8 failed with parameters: {'model_type': 'tabnet', 'n_d': 9, 'n_a': 27, 'n_steps': 7, 'gamma': 1.034715926280092, 'lambda_sparse': 0.0009879230717088486} because of the following error: ValueError('Found array with dim 3. None expected <= 2.').\nTraceback (most recent call last):\n  File \"/opt/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n    value_or_values = func(trial)\n                      ^^^^^^^^^^^\n  File \"/Users/vishesh/gw-workspace/eeZKNVA1B9aq/auto_ml.py\", line 102, in objective\n    model = create_model(trial)\n            ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/vishesh/gw-workspace/eeZKNVA1B9aq/auto_ml.py\", line 82, in create_model\n    tabnet_model.fit(\n  File \"/opt/anaconda3/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py\", line 217, in fit\n    check_input(X_train)\n  File \"/opt/anaconda3/lib/python3.11/site-packages/pytorch_tabnet/utils.py\", line 507, in check_input\n    check_array(X, accept_sparse=True)\n  File \"/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py\", line 915, in check_array\n    raise ValueError(\nValueError: Found array with dim 3. None expected <= 2.\n[W 2024-09-27 16:29:43,820] Trial 8 failed with value None.\nTraceback (most recent call last):\n  File \"/Users/vishesh/gw-workspace/eeZKNVA1B9aq/auto_ml.py\", line 126, in <module>\n    study.optimize(objective, n_trials=10)  # Set n_trials as needed\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/optuna/study/study.py\", line 475, in optimize\n    _optimize(\n  File \"/opt/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 63, in _optimize\n    _optimize_sequential(\n  File \"/opt/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 160, in _optimize_sequential\n    frozen_trial = _run_trial(study, func, catch)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 248, in _run_trial\n    raise func_err\n  File \"/opt/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n    value_or_values = func(trial)\n                      ^^^^^^^^^^^\n  File \"/Users/vishesh/gw-workspace/eeZKNVA1B9aq/auto_ml.py\", line 102, in objective\n    model = create_model(trial)\n            ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/vishesh/gw-workspace/eeZKNVA1B9aq/auto_ml.py\", line 82, in create_model\n    tabnet_model.fit(\n  File \"/opt/anaconda3/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py\", line 217, in fit\n    check_input(X_train)\n  File \"/opt/anaconda3/lib/python3.11/site-packages/pytorch_tabnet/utils.py\", line 507, in check_input\n    check_array(X, accept_sparse=True)\n  File \"/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py\", line 915, in check_array\n    raise ValueError(\nValueError: Found array with dim 3. None expected <= 2.\n",
  "history_begin_time" : 1727468973065,
  "history_end_time" : 1727468984572,
  "history_notes" : null,
  "history_process" : "vnrvoi",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "Rudx1OTLPPuE",
  "history_input" : "import numpy as np\nimport optuna\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import Dense, Dropout, Conv1D, MaxPooling1D, Flatten, LSTM, Input, MultiHeadAttention, LayerNormalization, Add\nfrom tensorflow.keras.optimizers import Adam\nfrom pytorch_tabnet.tab_model import TabNetRegressor\n\n# Load preprocessed data\ndata_path = '/Users/vishesh/gw-workspace/L6gDfkNJofh8/preprocessed_data.npz'\ndata = np.load(data_path)\n\n# List all variable names in the .npz file to identify contents\nprint(\"Variables in the .npz file:\", data.files)\n\n# Load data based on contents of the .npz file\nX_train = data['X_train']\ny_train = data['y_train']\n\n# Check if 'X_val' and 'y_val' exist; if not, create validation split\nif 'X_val' in data.files and 'y_val' in data.files:\n    X_val = data['X_val']\n    y_val = data['y_val']\nelse:\n    # Split the data into training and validation sets\n    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n\n# Initialize a dictionary to store model performance\nmodel_performance = {}\n\n# Define the function to create various models\ndef create_model(trial):\n    input_shape = X_train.shape[1]\n    model_type = trial.suggest_categorical('model_type', ['dense', 'cnn', 'lstm', 'transformer', 'tabnet'])\n\n    if model_type == 'dense':\n        model = Sequential()\n        model.add(Input(shape=(input_shape,)))  # Use Input layer to specify input shape\n        num_layers = trial.suggest_int('num_layers', 1, 5)\n        for i in range(num_layers):\n            num_units = trial.suggest_int(f'num_units_l{i}', 16, 128, log=True)\n            model.add(Dense(num_units, activation='relu'))\n            dropout_rate = trial.suggest_float(f'dropout_rate_l{i}', 0.1, 0.5)\n            model.add(Dropout(dropout_rate))\n        model.add(Dense(1, activation='linear'))\n    \n    elif model_type == 'cnn':\n        model = Sequential()\n        model.add(Input(shape=(input_shape, 1)))  # Use Input layer for CNN input shape\n        model.add(Conv1D(filters=trial.suggest_int('filters', 16, 64, log=True),\n                         kernel_size=trial.suggest_int('kernel_size', 3, 5),\n                         activation='relu'))\n        model.add(MaxPooling1D(pool_size=2))\n        model.add(Flatten())\n        model.add(Dense(1, activation='linear'))\n    \n    elif model_type == 'lstm':\n        model = Sequential()\n        model.add(Input(shape=(input_shape, 1)))  # Use Input layer for LSTM input shape\n        model.add(LSTM(units=trial.suggest_int('lstm_units', 16, 64, log=True)))\n        model.add(Dense(1, activation='linear'))\n    \n    elif model_type == 'transformer':\n        inputs = Input(shape=(input_shape, 1))\n        attention = MultiHeadAttention(num_heads=trial.suggest_int('num_heads', 2, 8), key_dim=trial.suggest_int('key_dim', 16, 64))(inputs, inputs)\n        attention = Add()([inputs, attention])\n        attention = LayerNormalization(epsilon=1e-6)(attention)\n        outputs = Flatten()(attention)\n        outputs = Dense(1, activation='linear')(outputs)\n        model = Model(inputs=inputs, outputs=outputs)\n    \n    elif model_type == 'tabnet':\n        tabnet_model = TabNetRegressor(\n            n_d=trial.suggest_int('n_d', 8, 64),\n            n_a=trial.suggest_int('n_a', 8, 64),\n            n_steps=trial.suggest_int('n_steps', 3, 10),\n            gamma=trial.suggest_float('gamma', 1.0, 2.0),\n            lambda_sparse=trial.suggest_float('lambda_sparse', 1e-6, 1e-3)\n        )\n        tabnet_model.fit(\n            X_train, y_train,\n            eval_set=[(X_val, y_val)],\n            eval_metric=['mae'],\n            max_epochs=100,\n            patience=10,\n            batch_size=256,\n            virtual_batch_size=128\n        )\n        return tabnet_model\n\n\n\n    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-2, log=True)\n    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mean_squared_error', metrics=['mean_absolute_error'])\n\n    return model\n\n# Define the objective function for Optuna\ndef objective(trial):\n    model = create_model(trial)\n\n    if isinstance(model, TabNetRegressor):\n        model.fit(\n            X_train, y_train,\n            eval_set=[(X_val, y_val)],\n            eval_metric=['mae'],\n            max_epochs=100,\n            patience=10,\n            batch_size=256,\n            virtual_batch_size=128\n        )\n        y_pred = model.predict(X_val)\n        mae = mean_absolute_error(y_val, y_pred)\n    else:\n        model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10, batch_size=32, verbose=0)\n        _, mae = model.evaluate(X_val, y_val, verbose=0)\n\n    model_type = trial.params['model_type']\n    model_performance[model_type] = mae\n    return mae\n\n# Run the Bayesian optimization\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=10)  # Set n_trials as needed\n\n# Get the best hyperparameters\nbest_params = study.best_params\nprint(f\"Best hyperparameters: {best_params}\")\n\n# Display performance of each model\nprint(\"Model performance (MAE):\")\nfor model_type, mae in model_performance.items():\n    print(f\"{model_type}: {mae}\")\n\n# Plot the model performance\nplt.figure(figsize=(10, 5))\nplt.bar(model_performance.keys(), model_performance.values(), color='skyblue')\nplt.xlabel('Model Type')\nplt.ylabel('Mean Absolute Error')\nplt.title('Model Performance Comparison')\nplt.show()\n",
  "history_output" : "Variables in the .npz file: ['X_train', 'X_test', 'y_train', 'y_test']\n[I 2024-09-27 16:29:13,513] A new study created in memory with name: no-name-55865829-e0b4-42a9-9c93-2559913adb7c\n/opt/anaconda3/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n  warnings.warn(f\"Device used : {self.device}\")\n[W 2024-09-27 16:29:13,517] Trial 0 failed with parameters: {'model_type': 'tabnet', 'n_d': 48, 'n_a': 62, 'n_steps': 10, 'gamma': 1.3894925047045095, 'lambda_sparse': 0.0008201575514572279} because of the following error: ValueError('Found array with dim 3. None expected <= 2.').\nTraceback (most recent call last):\n  File \"/opt/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n    value_or_values = func(trial)\n                      ^^^^^^^^^^^\n  File \"/Users/vishesh/gw-workspace/Rudx1OTLPPuE/auto_ml.py\", line 102, in objective\n    model = create_model(trial)\n            ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/vishesh/gw-workspace/Rudx1OTLPPuE/auto_ml.py\", line 82, in create_model\n    tabnet_model.fit(\n  File \"/opt/anaconda3/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py\", line 217, in fit\n    check_input(X_train)\n  File \"/opt/anaconda3/lib/python3.11/site-packages/pytorch_tabnet/utils.py\", line 507, in check_input\n    check_array(X, accept_sparse=True)\n  File \"/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py\", line 915, in check_array\n    raise ValueError(\nValueError: Found array with dim 3. None expected <= 2.\n[W 2024-09-27 16:29:13,518] Trial 0 failed with value None.\nTraceback (most recent call last):\n  File \"/Users/vishesh/gw-workspace/Rudx1OTLPPuE/auto_ml.py\", line 126, in <module>\n    study.optimize(objective, n_trials=10)  # Set n_trials as needed\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/optuna/study/study.py\", line 475, in optimize\n    _optimize(\n  File \"/opt/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 63, in _optimize\n    _optimize_sequential(\n  File \"/opt/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 160, in _optimize_sequential\n    frozen_trial = _run_trial(study, func, catch)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 248, in _run_trial\n    raise func_err\n  File \"/opt/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n    value_or_values = func(trial)\n                      ^^^^^^^^^^^\n  File \"/Users/vishesh/gw-workspace/Rudx1OTLPPuE/auto_ml.py\", line 102, in objective\n    model = create_model(trial)\n            ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/vishesh/gw-workspace/Rudx1OTLPPuE/auto_ml.py\", line 82, in create_model\n    tabnet_model.fit(\n  File \"/opt/anaconda3/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py\", line 217, in fit\n    check_input(X_train)\n  File \"/opt/anaconda3/lib/python3.11/site-packages/pytorch_tabnet/utils.py\", line 507, in check_input\n    check_array(X, accept_sparse=True)\n  File \"/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py\", line 915, in check_array\n    raise ValueError(\nValueError: Found array with dim 3. None expected <= 2.\n",
  "history_begin_time" : 1727468949049,
  "history_end_time" : 1727468953983,
  "history_notes" : null,
  "history_process" : "vnrvoi",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "RRkMWP9wSdH6",
  "history_input" : "import numpy as np\nimport optuna\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import Dense, Dropout, Conv1D, MaxPooling1D, Flatten, LSTM, Input, MultiHeadAttention, LayerNormalization, Add\nfrom tensorflow.keras.optimizers import Adam\nfrom pytorch_tabnet.tab_model import TabNetRegressor\n\n# Load preprocessed data\ndata_path = '/Users/vishesh/gw-workspace/L6gDfkNJofh8/preprocessed_data.npz'\ndata = np.load(data_path)\n\n# List all variable names in the .npz file to identify contents\nprint(\"Variables in the .npz file:\", data.files)\n\n# Load data based on contents of the .npz file\nX_train = data['X_train']\ny_train = data['y_train']\n\n# Check if 'X_val' and 'y_val' exist; if not, create validation split\nif 'X_val' in data.files and 'y_val' in data.files:\n    X_val = data['X_val']\n    y_val = data['y_val']\nelse:\n    # Split the data into training and validation sets\n    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n\n# Initialize a dictionary to store model performance\nmodel_performance = {}\n\n# Define the function to create various models\ndef create_model(trial):\n    input_shape = X_train.shape[1]\n    model_type = trial.suggest_categorical('model_type', ['dense', 'cnn', 'lstm', 'transformer', 'tabnet'])\n\n    if model_type == 'dense':\n        model = Sequential()\n        model.add(Input(shape=(input_shape,)))  # Use Input layer to specify input shape\n        num_layers = trial.suggest_int('num_layers', 1, 5)\n        for i in range(num_layers):\n            num_units = trial.suggest_int(f'num_units_l{i}', 16, 128, log=True)\n            model.add(Dense(num_units, activation='relu'))\n            dropout_rate = trial.suggest_float(f'dropout_rate_l{i}', 0.1, 0.5)\n            model.add(Dropout(dropout_rate))\n        model.add(Dense(1, activation='linear'))\n    \n    elif model_type == 'cnn':\n        model = Sequential()\n        model.add(Input(shape=(input_shape, 1)))  # Use Input layer for CNN input shape\n        model.add(Conv1D(filters=trial.suggest_int('filters', 16, 64, log=True),\n                         kernel_size=trial.suggest_int('kernel_size', 3, 5),\n                         activation='relu'))\n        model.add(MaxPooling1D(pool_size=2))\n        model.add(Flatten())\n        model.add(Dense(1, activation='linear'))\n    \n    elif model_type == 'lstm':\n        model = Sequential()\n        model.add(Input(shape=(input_shape, 1)))  # Use Input layer for LSTM input shape\n        model.add(LSTM(units=trial.suggest_int('lstm_units', 16, 64, log=True)))\n        model.add(Dense(1, activation='linear'))\n    \n    elif model_type == 'transformer':\n        inputs = Input(shape=(input_shape, 1))\n        attention = MultiHeadAttention(num_heads=trial.suggest_int('num_heads', 2, 8), key_dim=trial.suggest_int('key_dim', 16, 64))(inputs, inputs)\n        attention = Add()([inputs, attention])\n        attention = LayerNormalization(epsilon=1e-6)(attention)\n        outputs = Flatten()(attention)\n        outputs = Dense(1, activation='linear')(outputs)\n        model = Model(inputs=inputs, outputs=outputs)\n    \n    elif model_type == 'tabnet':\n        # tabnet_model = TabNetRegressor(\n        #     n_d=trial.suggest_int('n_d', 8, 64),\n        #     n_a=trial.suggest_int('n_a', 8, 64),\n        #     n_steps=trial.suggest_int('n_steps', 3, 10),\n        #     gamma=trial.suggest_float('gamma', 1.0, 2.0),\n        #     lambda_sparse=trial.suggest_float('lambda_sparse', 1e-6, 1e-3)\n        # )\n        # tabnet_model.fit(\n        #     X_train, y_train,\n        #     eval_set=[(X_val, y_val)],\n        #     eval_metric=['mae'],\n        #     max_epochs=100,\n        #     patience=10,\n        #     batch_size=256,\n        #     virtual_batch_size=128\n        # )\n        # return tabnet_model\n\n        X_train_tabnet = X_train.reshape(X_train.shape[0], -1)\n        X_val_tabnet = X_val.reshape(X_val.shape[0], -1)\n        \n        tabnet_model = TabNetRegressor(\n            n_d=trial.suggest_int('n_d', 8, 64),\n            n_a=trial.suggest_int('n_a', 8, 64),\n            n_steps=trial.suggest_int('n_steps', 3, 10),\n            gamma=trial.suggest_float('gamma', 1.0, 2.0),\n            lambda_sparse=trial.suggest_float('lambda_sparse', 1e-6, 1e-3)\n        )\n        tabnet_model.fit(\n            X_train_tabnet, y_train,\n            eval_set=[(X_val_tabnet, y_val)],\n            eval_metric=['mae'],\n            max_epochs=100,\n            patience=10,\n            batch_size=256,\n            virtual_batch_size=128\n        )\n        return tabnet_model\n\n\n    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-2, log=True)\n    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mean_squared_error', metrics=['mean_absolute_error'])\n\n    return model\n\n# Define the objective function for Optuna\ndef objective(trial):\n    model = create_model(trial)\n\n    if isinstance(model, TabNetRegressor):\n        model.fit(\n            X_train, y_train,\n            eval_set=[(X_val, y_val)],\n            eval_metric=['mae'],\n            max_epochs=100,\n            patience=10,\n            batch_size=256,\n            virtual_batch_size=128\n        )\n        y_pred = model.predict(X_val)\n        mae = mean_absolute_error(y_val, y_pred)\n    else:\n        model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10, batch_size=32, verbose=0)\n        _, mae = model.evaluate(X_val, y_val, verbose=0)\n\n    model_type = trial.params['model_type']\n    model_performance[model_type] = mae\n    return mae\n\n# Run the Bayesian optimization\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=10)  # Set n_trials as needed\n\n# Get the best hyperparameters\nbest_params = study.best_params\nprint(f\"Best hyperparameters: {best_params}\")\n\n# Display performance of each model\nprint(\"Model performance (MAE):\")\nfor model_type, mae in model_performance.items():\n    print(f\"{model_type}: {mae}\")\n\n# Plot the model performance\nplt.figure(figsize=(10, 5))\nplt.bar(model_performance.keys(), model_performance.values(), color='skyblue')\nplt.xlabel('Model Type')\nplt.ylabel('Mean Absolute Error')\nplt.title('Model Performance Comparison')\nplt.show()\n",
  "history_output" : "Variables in the .npz file: ['X_train', 'X_test', 'y_train', 'y_test']\n[I 2024-09-27 16:28:00,240] A new study created in memory with name: no-name-39796e1a-9351-4a89-999b-47d4935e8eff\n[I 2024-09-27 16:28:01,044] Trial 0 finished with value: 0.0580129399895668 and parameters: {'model_type': 'dense', 'num_layers': 3, 'num_units_l0': 35, 'dropout_rate_l0': 0.2881353591805701, 'num_units_l1': 56, 'dropout_rate_l1': 0.4379563565121092, 'num_units_l2': 93, 'dropout_rate_l2': 0.21472269622811188, 'learning_rate': 0.00873277713084791}. Best is trial 0 with value: 0.0580129399895668.\n[I 2024-09-27 16:28:02,016] Trial 1 finished with value: 0.08293621987104416 and parameters: {'model_type': 'lstm', 'lstm_units': 16, 'learning_rate': 0.00013019322121322823}. Best is trial 0 with value: 0.0580129399895668.\n[I 2024-09-27 16:28:02,536] Trial 2 finished with value: 0.039228565990924835 and parameters: {'model_type': 'cnn', 'filters': 29, 'kernel_size': 3, 'learning_rate': 0.008018874940851843}. Best is trial 2 with value: 0.039228565990924835.\n/opt/anaconda3/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n  warnings.warn(f\"Device used : {self.device}\")\nepoch 0  | loss: 12.82826| val_0_mae: 1.31511 |  0:00:00s\nepoch 1  | loss: 8.32289 | val_0_mae: 1.84015 |  0:00:00s\nepoch 2  | loss: 7.66733 | val_0_mae: 1.51273 |  0:00:00s\nepoch 3  | loss: 5.93363 | val_0_mae: 0.84571 |  0:00:00s\nepoch 4  | loss: 2.09418 | val_0_mae: 0.3275  |  0:00:00s\nepoch 5  | loss: 1.93948 | val_0_mae: 0.43472 |  0:00:00s\nepoch 6  | loss: 1.26579 | val_0_mae: 0.61343 |  0:00:00s\nepoch 7  | loss: 1.76681 | val_0_mae: 0.24615 |  0:00:00s\nepoch 8  | loss: 0.85694 | val_0_mae: 0.18069 |  0:00:00s\nepoch 9  | loss: 0.91962 | val_0_mae: 0.14552 |  0:00:00s\nepoch 10 | loss: 0.72405 | val_0_mae: 0.23351 |  0:00:00s\nepoch 11 | loss: 0.42819 | val_0_mae: 0.14276 |  0:00:00s\nepoch 12 | loss: 0.35303 | val_0_mae: 0.44271 |  0:00:00s\nepoch 13 | loss: 0.28364 | val_0_mae: 0.22913 |  0:00:01s\nepoch 14 | loss: 0.23978 | val_0_mae: 0.1046  |  0:00:01s\nepoch 15 | loss: 0.29149 | val_0_mae: 0.1336  |  0:00:01s\nepoch 16 | loss: 0.27177 | val_0_mae: 0.14705 |  0:00:01s\nepoch 17 | loss: 0.35083 | val_0_mae: 0.13332 |  0:00:01s\nepoch 18 | loss: 0.20709 | val_0_mae: 0.17497 |  0:00:01s\nepoch 19 | loss: 0.1405  | val_0_mae: 0.11372 |  0:00:01s\nepoch 20 | loss: 0.11247 | val_0_mae: 0.15216 |  0:00:01s\nepoch 21 | loss: 0.10806 | val_0_mae: 0.0986  |  0:00:01s\nepoch 22 | loss: 0.13106 | val_0_mae: 0.09915 |  0:00:01s\nepoch 23 | loss: 0.07145 | val_0_mae: 0.10256 |  0:00:01s\nepoch 24 | loss: 0.05895 | val_0_mae: 0.12026 |  0:00:01s\nepoch 25 | loss: 0.06752 | val_0_mae: 0.0912  |  0:00:01s\nepoch 26 | loss: 0.04759 | val_0_mae: 0.09493 |  0:00:02s\nepoch 27 | loss: 0.05412 | val_0_mae: 0.11659 |  0:00:02s\nepoch 28 | loss: 0.04251 | val_0_mae: 0.10866 |  0:00:02s\nepoch 29 | loss: 0.02722 | val_0_mae: 0.08264 |  0:00:02s\nepoch 30 | loss: 0.02323 | val_0_mae: 0.09363 |  0:00:02s\nepoch 31 | loss: 0.02432 | val_0_mae: 0.10158 |  0:00:02s\nepoch 32 | loss: 0.01863 | val_0_mae: 0.10898 |  0:00:02s\nepoch 33 | loss: 0.0176  | val_0_mae: 0.10587 |  0:00:02s\nepoch 34 | loss: 0.01786 | val_0_mae: 0.09546 |  0:00:02s\nepoch 35 | loss: 0.01238 | val_0_mae: 0.09068 |  0:00:02s\nepoch 36 | loss: 0.01165 | val_0_mae: 0.10075 |  0:00:02s\nepoch 37 | loss: 0.01319 | val_0_mae: 0.1019  |  0:00:02s\nepoch 38 | loss: 0.01691 | val_0_mae: 0.09524 |  0:00:02s\nepoch 39 | loss: 0.02281 | val_0_mae: 0.09639 |  0:00:02s\nEarly stopping occurred at epoch 39 with best_epoch = 29 and best_val_0_mae = 0.08264\n/opt/anaconda3/lib/python3.11/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n  warnings.warn(wrn_msg)\n[W 2024-09-27 16:28:06,826] Trial 3 failed with parameters: {'model_type': 'tabnet', 'n_d': 54, 'n_a': 60, 'n_steps': 6, 'gamma': 1.4528286079067767, 'lambda_sparse': 0.0006066045318287174} because of the following error: ValueError('Found array with dim 3. None expected <= 2.').\nTraceback (most recent call last):\n  File \"/opt/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n    value_or_values = func(trial)\n                      ^^^^^^^^^^^\n  File \"/Users/vishesh/gw-workspace/RRkMWP9wSdH6/auto_ml.py\", line 125, in objective\n    model.fit(\n  File \"/opt/anaconda3/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py\", line 217, in fit\n    check_input(X_train)\n  File \"/opt/anaconda3/lib/python3.11/site-packages/pytorch_tabnet/utils.py\", line 507, in check_input\n    check_array(X, accept_sparse=True)\n  File \"/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py\", line 915, in check_array\n    raise ValueError(\nValueError: Found array with dim 3. None expected <= 2.\n[W 2024-09-27 16:28:06,828] Trial 3 failed with value None.\nTraceback (most recent call last):\n  File \"/Users/vishesh/gw-workspace/RRkMWP9wSdH6/auto_ml.py\", line 146, in <module>\n    study.optimize(objective, n_trials=10)  # Set n_trials as needed\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/optuna/study/study.py\", line 475, in optimize\n    _optimize(\n  File \"/opt/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 63, in _optimize\n    _optimize_sequential(\n  File \"/opt/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 160, in _optimize_sequential\n    frozen_trial = _run_trial(study, func, catch)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 248, in _run_trial\n    raise func_err\n  File \"/opt/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n    value_or_values = func(trial)\n                      ^^^^^^^^^^^\n  File \"/Users/vishesh/gw-workspace/RRkMWP9wSdH6/auto_ml.py\", line 125, in objective\n    model.fit(\n  File \"/opt/anaconda3/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py\", line 217, in fit\n    check_input(X_train)\n  File \"/opt/anaconda3/lib/python3.11/site-packages/pytorch_tabnet/utils.py\", line 507, in check_input\n    check_array(X, accept_sparse=True)\n  File \"/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py\", line 915, in check_array\n    raise ValueError(\nValueError: Found array with dim 3. None expected <= 2.\n",
  "history_begin_time" : 1727468875466,
  "history_end_time" : 1727468887478,
  "history_notes" : null,
  "history_process" : "vnrvoi",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "O3mI3qOXAUvX",
  "history_input" : "import numpy as np\nimport optuna\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import Dense, Dropout, Conv1D, MaxPooling1D, Flatten, LSTM, Input, MultiHeadAttention, LayerNormalization, Add\nfrom tensorflow.keras.optimizers import Adam\nfrom pytorch_tabnet.tab_model import TabNetRegressor\n\n# Load preprocessed data\ndata_path = '/Users/vishesh/gw-workspace/L6gDfkNJofh8/preprocessed_data.npz'\ndata = np.load(data_path)\n\n# List all variable names in the .npz file to identify contents\nprint(\"Variables in the .npz file:\", data.files)\n\n# Load data based on contents of the .npz file\nX_train = data['X_train']\ny_train = data['y_train']\n\n# Check if 'X_val' and 'y_val' exist; if not, create validation split\nif 'X_val' in data.files and 'y_val' in data.files:\n    X_val = data['X_val']\n    y_val = data['y_val']\nelse:\n    # Split the data into training and validation sets\n    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n\n# Initialize a dictionary to store model performance\nmodel_performance = {}\n\n# Define the function to create various models\ndef create_model(trial):\n    input_shape = X_train.shape[1]\n    model_type = trial.suggest_categorical('model_type', ['dense', 'cnn', 'lstm', 'transformer', 'tabnet'])\n\n    if model_type == 'dense':\n        model = Sequential()\n        model.add(Input(shape=(input_shape,)))  # Use Input layer to specify input shape\n        num_layers = trial.suggest_int('num_layers', 1, 5)\n        for i in range(num_layers):\n            num_units = trial.suggest_int(f'num_units_l{i}', 16, 128, log=True)\n            model.add(Dense(num_units, activation='relu'))\n            dropout_rate = trial.suggest_float(f'dropout_rate_l{i}', 0.1, 0.5)\n            model.add(Dropout(dropout_rate))\n        model.add(Dense(1, activation='linear'))\n    \n    elif model_type == 'cnn':\n        model = Sequential()\n        model.add(Input(shape=(input_shape, 1)))  # Use Input layer for CNN input shape\n        model.add(Conv1D(filters=trial.suggest_int('filters', 16, 64, log=True),\n                         kernel_size=trial.suggest_int('kernel_size', 3, 5),\n                         activation='relu'))\n        model.add(MaxPooling1D(pool_size=2))\n        model.add(Flatten())\n        model.add(Dense(1, activation='linear'))\n    \n    elif model_type == 'lstm':\n        model = Sequential()\n        model.add(Input(shape=(input_shape, 1)))  # Use Input layer for LSTM input shape\n        model.add(LSTM(units=trial.suggest_int('lstm_units', 16, 64, log=True)))\n        model.add(Dense(1, activation='linear'))\n    \n    elif model_type == 'transformer':\n        inputs = Input(shape=(input_shape, 1))\n        attention = MultiHeadAttention(num_heads=trial.suggest_int('num_heads', 2, 8), key_dim=trial.suggest_int('key_dim', 16, 64))(inputs, inputs)\n        attention = Add()([inputs, attention])\n        attention = LayerNormalization(epsilon=1e-6)(attention)\n        outputs = Flatten()(attention)\n        outputs = Dense(1, activation='linear')(outputs)\n        model = Model(inputs=inputs, outputs=outputs)\n    \n    elif model_type == 'tabnet':\n        tabnet_model = TabNetRegressor(\n            n_d=trial.suggest_int('n_d', 8, 64),\n            n_a=trial.suggest_int('n_a', 8, 64),\n            n_steps=trial.suggest_int('n_steps', 3, 10),\n            gamma=trial.suggest_float('gamma', 1.0, 2.0),\n            lambda_sparse=trial.suggest_float('lambda_sparse', 1e-6, 1e-3)\n        )\n        tabnet_model.fit(\n            X_train, y_train,\n            eval_set=[(X_val, y_val)],\n            eval_metric=['mae'],\n            max_epochs=100,\n            patience=10,\n            batch_size=256,\n            virtual_batch_size=128\n        )\n        return tabnet_model\n\n\n    # return tabnet_model\n\n    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-2, log=True)\n    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mean_squared_error', metrics=['mean_absolute_error'])\n\n    return model\n\n# Define the objective function for Optuna\ndef objective(trial):\n    model = create_model(trial)\n\n    if isinstance(model, TabNetRegressor):\n        model.fit(\n            X_train, y_train,\n            eval_set=[(X_val, y_val)],\n            eval_metric=['mae'],\n            max_epochs=100,\n            patience=10,\n            batch_size=256,\n            virtual_batch_size=128\n        )\n        y_pred = model.predict(X_val)\n        mae = mean_absolute_error(y_val, y_pred)\n    else:\n        model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10, batch_size=32, verbose=0)\n        _, mae = model.evaluate(X_val, y_val, verbose=0)\n\n    model_type = trial.params['model_type']\n    model_performance[model_type] = mae\n    return mae\n\n# Run the Bayesian optimization\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=10)  # Set n_trials as needed\n\n# Get the best hyperparameters\nbest_params = study.best_params\nprint(f\"Best hyperparameters: {best_params}\")\n\n# Display performance of each model\nprint(\"Model performance (MAE):\")\nfor model_type, mae in model_performance.items():\n    print(f\"{model_type}: {mae}\")\n\n# Plot the model performance\nplt.figure(figsize=(10, 5))\nplt.bar(model_performance.keys(), model_performance.values(), color='skyblue')\nplt.xlabel('Model Type')\nplt.ylabel('Mean Absolute Error')\nplt.title('Model Performance Comparison')\nplt.show()\n",
  "history_output" : "Variables in the .npz file: ['X_train', 'X_test', 'y_train', 'y_test']\n[I 2024-09-27 16:26:38,726] A new study created in memory with name: no-name-a4bb3cab-d680-456c-b8e8-a49487d17ff3\n[I 2024-09-27 16:26:39,501] Trial 0 finished with value: 0.11684118211269379 and parameters: {'model_type': 'dense', 'num_layers': 2, 'num_units_l0': 114, 'dropout_rate_l0': 0.10793441227317246, 'num_units_l1': 32, 'dropout_rate_l1': 0.3299019087385207, 'learning_rate': 2.8061641893591765e-05}. Best is trial 0 with value: 0.11684118211269379.\n[I 2024-09-27 16:26:40,636] Trial 1 finished with value: 0.0996822863817215 and parameters: {'model_type': 'transformer', 'num_heads': 5, 'key_dim': 33, 'learning_rate': 0.00179807105540664}. Best is trial 1 with value: 0.0996822863817215.\n[I 2024-09-27 16:26:41,908] Trial 2 finished with value: 0.03422929346561432 and parameters: {'model_type': 'lstm', 'lstm_units': 60, 'learning_rate': 0.0068642792167915925}. Best is trial 2 with value: 0.03422929346561432.\n[I 2024-09-27 16:26:42,871] Trial 3 finished with value: 0.03602197766304016 and parameters: {'model_type': 'lstm', 'lstm_units': 18, 'learning_rate': 0.0063767908760957726}. Best is trial 2 with value: 0.03422929346561432.\n[I 2024-09-27 16:26:44,053] Trial 4 finished with value: 0.17914508283138275 and parameters: {'model_type': 'lstm', 'lstm_units': 52, 'learning_rate': 3.6552964331078706e-05}. Best is trial 2 with value: 0.03422929346561432.\n[I 2024-09-27 16:26:44,550] Trial 5 finished with value: 0.08290176838636398 and parameters: {'model_type': 'cnn', 'filters': 41, 'kernel_size': 4, 'learning_rate': 0.00019672019580540246}. Best is trial 2 with value: 0.03422929346561432.\n[I 2024-09-27 16:26:45,607] Trial 6 finished with value: 0.06765349954366684 and parameters: {'model_type': 'lstm', 'lstm_units': 42, 'learning_rate': 0.0001194205198902904}. Best is trial 2 with value: 0.03422929346561432.\n[I 2024-09-27 16:26:46,722] Trial 7 finished with value: 0.04022405669093132 and parameters: {'model_type': 'lstm', 'lstm_units': 36, 'learning_rate': 0.002147938216961347}. Best is trial 2 with value: 0.03422929346561432.\n[I 2024-09-27 16:26:47,240] Trial 8 finished with value: 0.10811830312013626 and parameters: {'model_type': 'cnn', 'filters': 56, 'kernel_size': 5, 'learning_rate': 4.600201515871103e-05}. Best is trial 2 with value: 0.03422929346561432.\n/opt/anaconda3/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n  warnings.warn(f\"Device used : {self.device}\")\n[W 2024-09-27 16:26:47,243] Trial 9 failed with parameters: {'model_type': 'tabnet', 'n_d': 41, 'n_a': 62, 'n_steps': 4, 'gamma': 1.2700866894264993, 'lambda_sparse': 0.0004107732902188427} because of the following error: ValueError('Found array with dim 3. None expected <= 2.').\nTraceback (most recent call last):\n  File \"/opt/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n    value_or_values = func(trial)\n                      ^^^^^^^^^^^\n  File \"/Users/vishesh/gw-workspace/O3mI3qOXAUvX/auto_ml.py\", line 103, in objective\n    model = create_model(trial)\n            ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/vishesh/gw-workspace/O3mI3qOXAUvX/auto_ml.py\", line 82, in create_model\n    tabnet_model.fit(\n  File \"/opt/anaconda3/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py\", line 217, in fit\n    check_input(X_train)\n  File \"/opt/anaconda3/lib/python3.11/site-packages/pytorch_tabnet/utils.py\", line 507, in check_input\n    check_array(X, accept_sparse=True)\n  File \"/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py\", line 915, in check_array\n    raise ValueError(\nValueError: Found array with dim 3. None expected <= 2.\n[W 2024-09-27 16:26:47,245] Trial 9 failed with value None.\nTraceback (most recent call last):\n  File \"/Users/vishesh/gw-workspace/O3mI3qOXAUvX/auto_ml.py\", line 127, in <module>\n    study.optimize(objective, n_trials=10)  # Set n_trials as needed\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/optuna/study/study.py\", line 475, in optimize\n    _optimize(\n  File \"/opt/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 63, in _optimize\n    _optimize_sequential(\n  File \"/opt/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 160, in _optimize_sequential\n    frozen_trial = _run_trial(study, func, catch)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 248, in _run_trial\n    raise func_err\n  File \"/opt/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n    value_or_values = func(trial)\n                      ^^^^^^^^^^^\n  File \"/Users/vishesh/gw-workspace/O3mI3qOXAUvX/auto_ml.py\", line 103, in objective\n    model = create_model(trial)\n            ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/vishesh/gw-workspace/O3mI3qOXAUvX/auto_ml.py\", line 82, in create_model\n    tabnet_model.fit(\n  File \"/opt/anaconda3/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py\", line 217, in fit\n    check_input(X_train)\n  File \"/opt/anaconda3/lib/python3.11/site-packages/pytorch_tabnet/utils.py\", line 507, in check_input\n    check_array(X, accept_sparse=True)\n  File \"/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py\", line 915, in check_array\n    raise ValueError(\nValueError: Found array with dim 3. None expected <= 2.\n",
  "history_begin_time" : 1727468793885,
  "history_end_time" : 1727468807960,
  "history_notes" : null,
  "history_process" : "vnrvoi",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "onlzbd3c1ey",
  "history_input" : "import numpy as np\nimport optuna\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import Dense, Dropout, Conv1D, MaxPooling1D, Flatten, LSTM, Input, MultiHeadAttention, LayerNormalization, Add\nfrom tensorflow.keras.optimizers import Adam\nfrom pytorch_tabnet.tab_model import TabNetRegressor\n\n# Load preprocessed data\ndata_path = '/Users/vishesh/gw-workspace/L6gDfkNJofh8/preprocessed_data.npz'\ndata = np.load(data_path)\n\n# List all variable names in the .npz file to identify contents\nprint(\"Variables in the .npz file:\", data.files)\n\n# Load data based on contents of the .npz file\nX_train = data['X_train']\ny_train = data['y_train']\n\n# Check if 'X_val' and 'y_val' exist; if not, create validation split\nif 'X_val' in data.files and 'y_val' in data.files:\n    X_val = data['X_val']\n    y_val = data['y_val']\nelse:\n    # Split the data into training and validation sets\n    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n\n# Initialize a dictionary to store model performance\nmodel_performance = {}\n\n# Define the function to create various models\ndef create_model(trial):\n    input_shape = X_train.shape[1]\n    model_type = trial.suggest_categorical('model_type', ['dense', 'cnn', 'lstm', 'transformer', 'tabnet'])\n\n    if model_type == 'dense':\n        model = Sequential()\n        model.add(Input(shape=(input_shape,)))  # Use Input layer to specify input shape\n        num_layers = trial.suggest_int('num_layers', 1, 5)\n        for i in range(num_layers):\n            num_units = trial.suggest_int(f'num_units_l{i}', 16, 128, log=True)\n            model.add(Dense(num_units, activation='relu'))\n            dropout_rate = trial.suggest_float(f'dropout_rate_l{i}', 0.1, 0.5)\n            model.add(Dropout(dropout_rate))\n        model.add(Dense(1, activation='linear'))\n    \n    elif model_type == 'cnn':\n        model = Sequential()\n        model.add(Input(shape=(input_shape, 1)))  # Use Input layer for CNN input shape\n        model.add(Conv1D(filters=trial.suggest_int('filters', 16, 64, log=True),\n                         kernel_size=trial.suggest_int('kernel_size', 3, 5),\n                         activation='relu'))\n        model.add(MaxPooling1D(pool_size=2))\n        model.add(Flatten())\n        model.add(Dense(1, activation='linear'))\n    \n    elif model_type == 'lstm':\n        model = Sequential()\n        model.add(Input(shape=(input_shape, 1)))  # Use Input layer for LSTM input shape\n        model.add(LSTM(units=trial.suggest_int('lstm_units', 16, 64, log=True)))\n        model.add(Dense(1, activation='linear'))\n    \n    elif model_type == 'transformer':\n        inputs = Input(shape=(input_shape, 1))\n        attention = MultiHeadAttention(num_heads=trial.suggest_int('num_heads', 2, 8), key_dim=trial.suggest_int('key_dim', 16, 64))(inputs, inputs)\n        attention = Add()([inputs, attention])\n        attention = LayerNormalization(epsilon=1e-6)(attention)\n        outputs = Flatten()(attention)\n        outputs = Dense(1, activation='linear')(outputs)\n        model = Model(inputs=inputs, outputs=outputs)\n    \n    elif model_type == 'tabnet':\n        tabnet_model = TabNetRegressor(\n            n_d=trial.suggest_int('n_d', 8, 64),\n            n_a=trial.suggest_int('n_a', 8, 64),\n            n_steps=trial.suggest_int('n_steps', 3, 10),\n            gamma=trial.suggest_float('gamma', 1.0, 2.0),\n            lambda_sparse=trial.suggest_float('lambda_sparse', 1e-6, 1e-3)\n        )\n        tabnet_model.fit(\n            X_train, y_train,\n            eval_set=[(X_val, y_val)],\n            eval_metric=['mae'],\n            max_epochs=100,\n            patience=10,\n            batch_size=256,\n            virtual_batch_size=128\n        )\n        return tabnet_model\n\n\n    return tabnet_model\n\n    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-2, log=True)\n    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mean_squared_error', metrics=['mean_absolute_error'])\n\n    return model\n\n# Define the objective function for Optuna\ndef objective(trial):\n    model = create_model(trial)\n\n    if isinstance(model, TabNetRegressor):\n        model.fit(\n            X_train, y_train,\n            eval_set=[(X_val, y_val)],\n            eval_metric=['mae'],\n            max_epochs=100,\n            patience=10,\n            batch_size=256,\n            virtual_batch_size=128\n        )\n        y_pred = model.predict(X_val)\n        mae = mean_absolute_error(y_val, y_pred)\n    else:\n        model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10, batch_size=32, verbose=0)\n        _, mae = model.evaluate(X_val, y_val, verbose=0)\n\n    model_type = trial.params['model_type']\n    model_performance[model_type] = mae\n    return mae\n\n# Run the Bayesian optimization\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=10)  # Set n_trials as needed\n\n# Get the best hyperparameters\nbest_params = study.best_params\nprint(f\"Best hyperparameters: {best_params}\")\n\n# Display performance of each model\nprint(\"Model performance (MAE):\")\nfor model_type, mae in model_performance.items():\n    print(f\"{model_type}: {mae}\")\n\n# Plot the model performance\nplt.figure(figsize=(10, 5))\nplt.bar(model_performance.keys(), model_performance.values(), color='skyblue')\nplt.xlabel('Model Type')\nplt.ylabel('Mean Absolute Error')\nplt.title('Model Performance Comparison')\nplt.show()\n",
  "history_output" : "Variables in the .npz file: ['X_train', 'X_test', 'y_train', 'y_test']\n[I 2024-09-27 16:25:05,960] A new study created in memory with name: no-name-3d3eea1c-4d23-45fb-a61b-5709e70779a2\n/opt/anaconda3/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n  warnings.warn(f\"Device used : {self.device}\")\n[W 2024-09-27 16:25:05,965] Trial 0 failed with parameters: {'model_type': 'tabnet', 'n_d': 14, 'n_a': 24, 'n_steps': 5, 'gamma': 1.8276160750280501, 'lambda_sparse': 0.0009776327501219428} because of the following error: ValueError('Found array with dim 3. None expected <= 2.').\nTraceback (most recent call last):\n  File \"/opt/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n    value_or_values = func(trial)\n                      ^^^^^^^^^^^\n  File \"/Users/vishesh/gw-workspace/onlzbd3c1ey/auto_ml.py\", line 103, in objective\n    model = create_model(trial)\n            ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/vishesh/gw-workspace/onlzbd3c1ey/auto_ml.py\", line 82, in create_model\n    tabnet_model.fit(\n  File \"/opt/anaconda3/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py\", line 217, in fit\n    check_input(X_train)\n  File \"/opt/anaconda3/lib/python3.11/site-packages/pytorch_tabnet/utils.py\", line 507, in check_input\n    check_array(X, accept_sparse=True)\n  File \"/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py\", line 915, in check_array\n    raise ValueError(\nValueError: Found array with dim 3. None expected <= 2.\n[W 2024-09-27 16:25:05,966] Trial 0 failed with value None.\nTraceback (most recent call last):\n  File \"/Users/vishesh/gw-workspace/onlzbd3c1ey/auto_ml.py\", line 127, in <module>\n    study.optimize(objective, n_trials=10)  # Set n_trials as needed\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/optuna/study/study.py\", line 475, in optimize\n    _optimize(\n  File \"/opt/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 63, in _optimize\n    _optimize_sequential(\n  File \"/opt/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 160, in _optimize_sequential\n    frozen_trial = _run_trial(study, func, catch)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 248, in _run_trial\n    raise func_err\n  File \"/opt/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n    value_or_values = func(trial)\n                      ^^^^^^^^^^^\n  File \"/Users/vishesh/gw-workspace/onlzbd3c1ey/auto_ml.py\", line 103, in objective\n    model = create_model(trial)\n            ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/vishesh/gw-workspace/onlzbd3c1ey/auto_ml.py\", line 82, in create_model\n    tabnet_model.fit(\n  File \"/opt/anaconda3/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py\", line 217, in fit\n    check_input(X_train)\n  File \"/opt/anaconda3/lib/python3.11/site-packages/pytorch_tabnet/utils.py\", line 507, in check_input\n    check_array(X, accept_sparse=True)\n  File \"/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py\", line 915, in check_array\n    raise ValueError(\nValueError: Found array with dim 3. None expected <= 2.\n",
  "history_begin_time" : 1727468702561,
  "history_end_time" : 1727468706485,
  "history_notes" : null,
  "history_process" : "vnrvoi",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "4blepflzv5h",
  "history_input" : "import numpy as np\nimport optuna\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import Dense, Dropout, Conv1D, MaxPooling1D, Flatten, LSTM, Input, MultiHeadAttention, LayerNormalization, Add\nfrom tensorflow.keras.optimizers import Adam\nfrom pytorch_tabnet.tab_model import TabNetRegressor\n\n# Load preprocessed data\ndata_path = '/Users/vishesh/gw-workspace/L6gDfkNJofh8/preprocessed_data.npz'\ndata = np.load(data_path)\n\n# List all variable names in the .npz file to identify contents\nprint(\"Variables in the .npz file:\", data.files)\n\n# Load data based on contents of the .npz file\nX_train = data['X_train']\ny_train = data['y_train']\n\n# Check if 'X_val' and 'y_val' exist; if not, create validation split\nif 'X_val' in data.files and 'y_val' in data.files:\n    X_val = data['X_val']\n    y_val = data['y_val']\nelse:\n    # Split the data into training and validation sets\n    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n\n# Initialize a dictionary to store model performance\nmodel_performance = {}\n\n# Define the function to create various models\ndef create_model(trial):\n    input_shape = X_train.shape[1]\n    model_type = trial.suggest_categorical('model_type', ['dense', 'cnn', 'lstm', 'transformer', 'tabnet'])\n\n    if model_type == 'dense':\n        model = Sequential()\n        model.add(Input(shape=(input_shape,)))  # Use Input layer to specify input shape\n        num_layers = trial.suggest_int('num_layers', 1, 5)\n        for i in range(num_layers):\n            num_units = trial.suggest_int(f'num_units_l{i}', 16, 128, log=True)\n            model.add(Dense(num_units, activation='relu'))\n            dropout_rate = trial.suggest_float(f'dropout_rate_l{i}', 0.1, 0.5)\n            model.add(Dropout(dropout_rate))\n        model.add(Dense(1, activation='linear'))\n    \n    elif model_type == 'cnn':\n        model = Sequential()\n        model.add(Input(shape=(input_shape, 1)))  # Use Input layer for CNN input shape\n        model.add(Conv1D(filters=trial.suggest_int('filters', 16, 64, log=True),\n                         kernel_size=trial.suggest_int('kernel_size', 3, 5),\n                         activation='relu'))\n        model.add(MaxPooling1D(pool_size=2))\n        model.add(Flatten())\n        model.add(Dense(1, activation='linear'))\n    \n    elif model_type == 'lstm':\n        model = Sequential()\n        model.add(Input(shape=(input_shape, 1)))  # Use Input layer for LSTM input shape\n        model.add(LSTM(units=trial.suggest_int('lstm_units', 16, 64, log=True)))\n        model.add(Dense(1, activation='linear'))\n    \n    elif model_type == 'transformer':\n        inputs = Input(shape=(input_shape, 1))\n        attention = MultiHeadAttention(num_heads=trial.suggest_int('num_heads', 2, 8), key_dim=trial.suggest_int('key_dim', 16, 64))(inputs, inputs)\n        attention = Add()([inputs, attention])\n        attention = LayerNormalization(epsilon=1e-6)(attention)\n        outputs = Flatten()(attention)\n        outputs = Dense(1, activation='linear')(outputs)\n        model = Model(inputs=inputs, outputs=outputs)\n    \n    elif model_type == 'tabnet':\n        tabnet_model = TabNetRegressor(\n            n_d=trial.suggest_int('n_d', 8, 64),\n            n_a=trial.suggest_int('n_a', 8, 64),\n            n_steps=trial.suggest_int('n_steps', 3, 10),\n            gamma=trial.suggest_float('gamma', 1.0, 2.0),\n            lambda_sparse=trial.suggest_float('lambda_sparse', 1e-6, 1e-3)\n        )\n        tabnet_model.fit(\n            X_train, y_train,\n            eval_set=[(X_val, y_val)],\n            eval_metric=['mae'],\n            max_epochs=100,\n            patience=10,\n            batch_size=256,\n            virtual_batch_size=128\n        )\n        return tabnet_model\n\n\n    return tabnet_model\n\n    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-2, log=True)\n    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mean_squared_error', metrics=['mean_absolute_error'])\n\n    return model\n\n# Define the objective function for Optuna\ndef objective(trial):\n    model = create_model(trial)\n\n    if isinstance(model, TabNetRegressor):\n        model.fit(\n            X_train, y_train,\n            eval_set=[(X_val, y_val)],\n            eval_metric=['mae'],\n            max_epochs=100,\n            patience=10,\n            batch_size=256,\n            virtual_batch_size=128\n        )\n        y_pred = model.predict(X_val)\n        mae = mean_absolute_error(y_val, y_pred)\n    else:\n        model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10, batch_size=32, verbose=0)\n        _, mae = model.evaluate(X_val, y_val, verbose=0)\n\n    model_type = trial.params['model_type']\n    model_performance[model_type] = mae\n    return mae\n\n# Run the Bayesian optimization\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=10)  # Set n_trials as needed\n\n# Get the best hyperparameters\nbest_params = study.best_params\nprint(f\"Best hyperparameters: {best_params}\")\n\n# Display performance of each model\nprint(\"Model performance (MAE):\")\nfor model_type, mae in model_performance.items():\n    print(f\"{model_type}: {mae}\")\n\n# Plot the model performance\nplt.figure(figsize=(10, 5))\nplt.bar(model_performance.keys(), model_performance.values(), color='skyblue')\nplt.xlabel('Model Type')\nplt.ylabel('Mean Absolute Error')\nplt.title('Model Performance Comparison')\nplt.show()\n",
  "history_output" : "Variables in the .npz file: ['X_train', 'X_test', 'y_train', 'y_test']\n[I 2024-09-27 16:24:27,728] A new study created in memory with name: no-name-db911d65-3dec-47b2-b7be-530a33181e12\n[W 2024-09-27 16:24:27,788] Trial 0 failed with parameters: {'model_type': 'cnn', 'filters': 19, 'kernel_size': 3} because of the following error: UnboundLocalError(\"cannot access local variable 'tabnet_model' where it is not associated with a value\").\nTraceback (most recent call last):\n  File \"/opt/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n    value_or_values = func(trial)\n                      ^^^^^^^^^^^\n  File \"/Users/vishesh/gw-workspace/4blepflzv5h/auto_ml.py\", line 103, in objective\n    model = create_model(trial)\n            ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/vishesh/gw-workspace/4blepflzv5h/auto_ml.py\", line 94, in create_model\n    return tabnet_model\n           ^^^^^^^^^^^^\nUnboundLocalError: cannot access local variable 'tabnet_model' where it is not associated with a value\n[W 2024-09-27 16:24:27,788] Trial 0 failed with value None.\nTraceback (most recent call last):\n  File \"/Users/vishesh/gw-workspace/4blepflzv5h/auto_ml.py\", line 127, in <module>\n    study.optimize(objective, n_trials=10)  # Set n_trials as needed\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/optuna/study/study.py\", line 475, in optimize\n    _optimize(\n  File \"/opt/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 63, in _optimize\n    _optimize_sequential(\n  File \"/opt/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 160, in _optimize_sequential\n    frozen_trial = _run_trial(study, func, catch)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 248, in _run_trial\n    raise func_err\n  File \"/opt/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n    value_or_values = func(trial)\n                      ^^^^^^^^^^^\n  File \"/Users/vishesh/gw-workspace/4blepflzv5h/auto_ml.py\", line 103, in objective\n    model = create_model(trial)\n            ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/vishesh/gw-workspace/4blepflzv5h/auto_ml.py\", line 94, in create_model\n    return tabnet_model\n           ^^^^^^^^^^^^\nUnboundLocalError: cannot access local variable 'tabnet_model' where it is not associated with a value\n",
  "history_begin_time" : 1727468664270,
  "history_end_time" : 1727468668268,
  "history_notes" : null,
  "history_process" : "vnrvoi",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "76olr3www99",
  "history_input" : "import numpy as np\nimport optuna\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import Dense, Dropout, Conv1D, MaxPooling1D, Flatten, LSTM, Input, MultiHeadAttention, LayerNormalization, Add\nfrom tensorflow.keras.optimizers import Adam\nfrom pytorch_tabnet.tab_model import TabNetRegressor\n\n# Load preprocessed data\ndata_path = '/Users/vishesh/gw-workspace/L6gDfkNJofh8/preprocessed_data.npz'\ndata = np.load(data_path)\n\n# List all variable names in the .npz file to identify contents\nprint(\"Variables in the .npz file:\", data.files)\n\n# Load data based on contents of the .npz file\nX_train = data['X_train']\ny_train = data['y_train']\n\n# Check if 'X_val' and 'y_val' exist; if not, create validation split\nif 'X_val' in data.files and 'y_val' in data.files:\n    X_val = data['X_val']\n    y_val = data['y_val']\nelse:\n    # Split the data into training and validation sets\n    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n\n# Initialize a dictionary to store model performance\nmodel_performance = {}\n\n# Define the function to create various models\ndef create_model(trial):\n    input_shape = X_train.shape[1]\n    model_type = trial.suggest_categorical('model_type', ['dense', 'cnn', 'lstm', 'transformer', 'tabnet'])\n\n    if model_type == 'dense':\n        model = Sequential()\n        model.add(Input(shape=(input_shape,)))  # Use Input layer to specify input shape\n        num_layers = trial.suggest_int('num_layers', 1, 5)\n        for i in range(num_layers):\n            num_units = trial.suggest_int(f'num_units_l{i}', 16, 128, log=True)\n            model.add(Dense(num_units, activation='relu'))\n            dropout_rate = trial.suggest_float(f'dropout_rate_l{i}', 0.1, 0.5)\n            model.add(Dropout(dropout_rate))\n        model.add(Dense(1, activation='linear'))\n    \n    elif model_type == 'cnn':\n        model = Sequential()\n        model.add(Input(shape=(input_shape, 1)))  # Use Input layer for CNN input shape\n        model.add(Conv1D(filters=trial.suggest_int('filters', 16, 64, log=True),\n                         kernel_size=trial.suggest_int('kernel_size', 3, 5),\n                         activation='relu'))\n        model.add(MaxPooling1D(pool_size=2))\n        model.add(Flatten())\n        model.add(Dense(1, activation='linear'))\n    \n    elif model_type == 'lstm':\n        model = Sequential()\n        model.add(Input(shape=(input_shape, 1)))  # Use Input layer for LSTM input shape\n        model.add(LSTM(units=trial.suggest_int('lstm_units', 16, 64, log=True)))\n        model.add(Dense(1, activation='linear'))\n    \n    elif model_type == 'transformer':\n        inputs = Input(shape=(input_shape, 1))\n        attention = MultiHeadAttention(num_heads=trial.suggest_int('num_heads', 2, 8), key_dim=trial.suggest_int('key_dim', 16, 64))(inputs, inputs)\n        attention = Add()([inputs, attention])\n        attention = LayerNormalization(epsilon=1e-6)(attention)\n        outputs = Flatten()(attention)\n        outputs = Dense(1, activation='linear')(outputs)\n        model = Model(inputs=inputs, outputs=outputs)\n    \n    elif model_type == 'tabnet':\n        tabnet_model = TabNetRegressor(\n            n_d=trial.suggest_int('n_d', 8, 64),\n            n_a=trial.suggest_int('n_a', 8, 64),\n            n_steps=trial.suggest_int('n_steps', 3, 10),\n            gamma=trial.suggest_float('gamma', 1.0, 2.0),\n            lambda_sparse=trial.suggest_float('lambda_sparse', 1e-6, 1e-3)\n        )\n        tabnet_model.fit(\n            X_train, y_train,\n            eval_set=[(X_val, y_val)],\n            eval_metric=['mae'],\n            max_epochs=100,\n            patience=10,\n            batch_size=256,\n            virtual_batch_size=128\n        )\n        return tabnet_model\n        Reshape the data to 2D for TabNet\n\n    return tabnet_model\n\n    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-2, log=True)\n    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mean_squared_error', metrics=['mean_absolute_error'])\n\n    return model\n\n# Define the objective function for Optuna\ndef objective(trial):\n    model = create_model(trial)\n\n    if isinstance(model, TabNetRegressor):\n        model.fit(\n            X_train, y_train,\n            eval_set=[(X_val, y_val)],\n            eval_metric=['mae'],\n            max_epochs=100,\n            patience=10,\n            batch_size=256,\n            virtual_batch_size=128\n        )\n        y_pred = model.predict(X_val)\n        mae = mean_absolute_error(y_val, y_pred)\n    else:\n        model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10, batch_size=32, verbose=0)\n        _, mae = model.evaluate(X_val, y_val, verbose=0)\n\n    model_type = trial.params['model_type']\n    model_performance[model_type] = mae\n    return mae\n\n# Run the Bayesian optimization\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=10)  # Set n_trials as needed\n\n# Get the best hyperparameters\nbest_params = study.best_params\nprint(f\"Best hyperparameters: {best_params}\")\n\n# Display performance of each model\nprint(\"Model performance (MAE):\")\nfor model_type, mae in model_performance.items():\n    print(f\"{model_type}: {mae}\")\n\n# Plot the model performance\nplt.figure(figsize=(10, 5))\nplt.bar(model_performance.keys(), model_performance.values(), color='skyblue')\nplt.xlabel('Model Type')\nplt.ylabel('Mean Absolute Error')\nplt.title('Model Performance Comparison')\nplt.show()\n",
  "history_output" : "  File \"/Users/vishesh/gw-workspace/76olr3www99/auto_ml.py\", line 92\n    Reshape the data to 2D for TabNet\n                        ^\nSyntaxError: invalid decimal literal\n",
  "history_begin_time" : 1727468627503,
  "history_end_time" : 1727468627586,
  "history_notes" : null,
  "history_process" : "vnrvoi",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "w4bbn3wt3s6",
  "history_input" : "import numpy as np\nimport optuna\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import Dense, Dropout, Conv1D, MaxPooling1D, Flatten, LSTM, Input, MultiHeadAttention, LayerNormalization, Add\nfrom tensorflow.keras.optimizers import Adam\nfrom pytorch_tabnet.tab_model import TabNetRegressor\n\n# Load preprocessed data\ndata_path = '/Users/vishesh/gw-workspace/L6gDfkNJofh8/preprocessed_data.npz'\ndata = np.load(data_path)\n\n# List all variable names in the .npz file to identify contents\nprint(\"Variables in the .npz file:\", data.files)\n\n# Load data based on contents of the .npz file\nX_train = data['X_train']\ny_train = data['y_train']\n\n# Check if 'X_val' and 'y_val' exist; if not, create validation split\nif 'X_val' in data.files and 'y_val' in data.files:\n    X_val = data['X_val']\n    y_val = data['y_val']\nelse:\n    # Split the data into training and validation sets\n    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n\n# Initialize a dictionary to store model performance\nmodel_performance = {}\n\n# Define the function to create various models\ndef create_model(trial):\n    input_shape = X_train.shape[1]\n    model_type = trial.suggest_categorical('model_type', ['dense', 'cnn', 'lstm', 'transformer', 'tabnet'])\n\n    if model_type == 'dense':\n        model = Sequential()\n        model.add(Input(shape=(input_shape,)))  # Use Input layer to specify input shape\n        num_layers = trial.suggest_int('num_layers', 1, 5)\n        for i in range(num_layers):\n            num_units = trial.suggest_int(f'num_units_l{i}', 16, 128, log=True)\n            model.add(Dense(num_units, activation='relu'))\n            dropout_rate = trial.suggest_float(f'dropout_rate_l{i}', 0.1, 0.5)\n            model.add(Dropout(dropout_rate))\n        model.add(Dense(1, activation='linear'))\n    \n    elif model_type == 'cnn':\n        model = Sequential()\n        model.add(Input(shape=(input_shape, 1)))  # Use Input layer for CNN input shape\n        model.add(Conv1D(filters=trial.suggest_int('filters', 16, 64, log=True),\n                         kernel_size=trial.suggest_int('kernel_size', 3, 5),\n                         activation='relu'))\n        model.add(MaxPooling1D(pool_size=2))\n        model.add(Flatten())\n        model.add(Dense(1, activation='linear'))\n    \n    elif model_type == 'lstm':\n        model = Sequential()\n        model.add(Input(shape=(input_shape, 1)))  # Use Input layer for LSTM input shape\n        model.add(LSTM(units=trial.suggest_int('lstm_units', 16, 64, log=True)))\n        model.add(Dense(1, activation='linear'))\n    \n    elif model_type == 'transformer':\n        inputs = Input(shape=(input_shape, 1))\n        attention = MultiHeadAttention(num_heads=trial.suggest_int('num_heads', 2, 8), key_dim=trial.suggest_int('key_dim', 16, 64))(inputs, inputs)\n        attention = Add()([inputs, attention])\n        attention = LayerNormalization(epsilon=1e-6)(attention)\n        outputs = Flatten()(attention)\n        outputs = Dense(1, activation='linear')(outputs)\n        model = Model(inputs=inputs, outputs=outputs)\n    \n    elif model_type == 'tabnet':\n        # tabnet_model = TabNetRegressor(\n        #     n_d=trial.suggest_int('n_d', 8, 64),\n        #     n_a=trial.suggest_int('n_a', 8, 64),\n        #     n_steps=trial.suggest_int('n_steps', 3, 10),\n        #     gamma=trial.suggest_float('gamma', 1.0, 2.0),\n        #     lambda_sparse=trial.suggest_float('lambda_sparse', 1e-6, 1e-3)\n        # )\n        # tabnet_model.fit(\n        #     X_train, y_train,\n        #     eval_set=[(X_val, y_val)],\n        #     eval_metric=['mae'],\n        #     max_epochs=100,\n        #     patience=10,\n        #     batch_size=256,\n        #     virtual_batch_size=128\n        # )\n        # return tabnet_model\n        # Reshape the data to 2D for TabNet\n    X_train_tabnet = X_train.reshape(X_train.shape[0], -1)\n    X_val_tabnet = X_val.reshape(X_val.shape[0], -1)\n    \n    tabnet_model = TabNetRegressor(\n        n_d=trial.suggest_int('n_d', 8, 64),\n        n_a=trial.suggest_int('n_a', 8, 64),\n        n_steps=trial.suggest_int('n_steps', 3, 10),\n        gamma=trial.suggest_float('gamma', 1.0, 2.0),\n        lambda_sparse=trial.suggest_float('lambda_sparse', 1e-6, 1e-3)\n    )\n    tabnet_model.fit(\n        X_train_tabnet, y_train,\n        eval_set=[(X_val_tabnet, y_val)],\n        eval_metric=['mae'],\n        max_epochs=100,\n        patience=10,\n        batch_size=256,\n        virtual_batch_size=128\n    )\n    return tabnet_model\n\n    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-2, log=True)\n    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mean_squared_error', metrics=['mean_absolute_error'])\n\n    return model\n\n# Define the objective function for Optuna\ndef objective(trial):\n    model = create_model(trial)\n\n    if isinstance(model, TabNetRegressor):\n        model.fit(\n            X_train, y_train,\n            eval_set=[(X_val, y_val)],\n            eval_metric=['mae'],\n            max_epochs=100,\n            patience=10,\n            batch_size=256,\n            virtual_batch_size=128\n        )\n        y_pred = model.predict(X_val)\n        mae = mean_absolute_error(y_val, y_pred)\n    else:\n        model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10, batch_size=32, verbose=0)\n        _, mae = model.evaluate(X_val, y_val, verbose=0)\n\n    model_type = trial.params['model_type']\n    model_performance[model_type] = mae\n    return mae\n\n# Run the Bayesian optimization\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=10)  # Set n_trials as needed\n\n# Get the best hyperparameters\nbest_params = study.best_params\nprint(f\"Best hyperparameters: {best_params}\")\n\n# Display performance of each model\nprint(\"Model performance (MAE):\")\nfor model_type, mae in model_performance.items():\n    print(f\"{model_type}: {mae}\")\n\n# Plot the model performance\nplt.figure(figsize=(10, 5))\nplt.bar(model_performance.keys(), model_performance.values(), color='skyblue')\nplt.xlabel('Model Type')\nplt.ylabel('Mean Absolute Error')\nplt.title('Model Performance Comparison')\nplt.show()\n",
  "history_output" : "  File \"/Users/vishesh/gw-workspace/w4bbn3wt3s6/auto_ml.py\", line 93\n    X_train_tabnet = X_train.reshape(X_train.shape[0], -1)\n    ^\nIndentationError: expected an indented block after 'elif' statement on line 74\n",
  "history_begin_time" : 1727468509298,
  "history_end_time" : 1727468509405,
  "history_notes" : null,
  "history_process" : "vnrvoi",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "s0eg04dpv2v",
  "history_input" : "import numpy as np\nimport optuna\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import Dense, Dropout, Conv1D, MaxPooling1D, Flatten, LSTM, Input, MultiHeadAttention, LayerNormalization, Add\nfrom tensorflow.keras.optimizers import Adam\nfrom pytorch_tabnet.tab_model import TabNetRegressor\n\n# Load preprocessed data\ndata_path = '/Users/vishesh/gw-workspace/L6gDfkNJofh8/preprocessed_data.npz'\ndata = np.load(data_path)\n\n# List all variable names in the .npz file to identify contents\nprint(\"Variables in the .npz file:\", data.files)\n\n# Load data based on contents of the .npz file\nX_train = data['X_train']\ny_train = data['y_train']\n\n# Check if 'X_val' and 'y_val' exist; if not, create validation split\nif 'X_val' in data.files and 'y_val' in data.files:\n    X_val = data['X_val']\n    y_val = data['y_val']\nelse:\n    # Split the data into training and validation sets\n    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n\n# Initialize a dictionary to store model performance\nmodel_performance = {}\n\n# Define the function to create various models\ndef create_model(trial):\n    input_shape = X_train.shape[1]\n    model_type = trial.suggest_categorical('model_type', ['dense', 'cnn', 'lstm', 'transformer', 'tabnet'])\n\n    if model_type == 'dense':\n        model = Sequential()\n        model.add(Input(shape=(input_shape,)))  # Use Input layer to specify input shape\n        num_layers = trial.suggest_int('num_layers', 1, 5)\n        for i in range(num_layers):\n            num_units = trial.suggest_int(f'num_units_l{i}', 16, 128, log=True)\n            model.add(Dense(num_units, activation='relu'))\n            dropout_rate = trial.suggest_float(f'dropout_rate_l{i}', 0.1, 0.5)\n            model.add(Dropout(dropout_rate))\n        model.add(Dense(1, activation='linear'))\n    \n    elif model_type == 'cnn':\n        model = Sequential()\n        model.add(Input(shape=(input_shape, 1)))  # Use Input layer for CNN input shape\n        model.add(Conv1D(filters=trial.suggest_int('filters', 16, 64, log=True),\n                         kernel_size=trial.suggest_int('kernel_size', 3, 5),\n                         activation='relu'))\n        model.add(MaxPooling1D(pool_size=2))\n        model.add(Flatten())\n        model.add(Dense(1, activation='linear'))\n    \n    elif model_type == 'lstm':\n        model = Sequential()\n        model.add(Input(shape=(input_shape, 1)))  # Use Input layer for LSTM input shape\n        model.add(LSTM(units=trial.suggest_int('lstm_units', 16, 64, log=True)))\n        model.add(Dense(1, activation='linear'))\n    \n    elif model_type == 'transformer':\n        inputs = Input(shape=(input_shape, 1))\n        attention = MultiHeadAttention(num_heads=trial.suggest_int('num_heads', 2, 8), key_dim=trial.suggest_int('key_dim', 16, 64))(inputs, inputs)\n        attention = Add()([inputs, attention])\n        attention = LayerNormalization(epsilon=1e-6)(attention)\n        outputs = Flatten()(attention)\n        outputs = Dense(1, activation='linear')(outputs)\n        model = Model(inputs=inputs, outputs=outputs)\n    \n    elif model_type == 'tabnet':\n        tabnet_model = TabNetRegressor(\n            n_d=trial.suggest_int('n_d', 8, 64),\n            n_a=trial.suggest_int('n_a', 8, 64),\n            n_steps=trial.suggest_int('n_steps', 3, 10),\n            gamma=trial.suggest_float('gamma', 1.0, 2.0),\n            lambda_sparse=trial.suggest_float('lambda_sparse', 1e-6, 1e-3)\n        )\n        tabnet_model.fit(\n            X_train, y_train,\n            eval_set=[(X_val, y_val)],\n            eval_metric=['mae'],\n            max_epochs=100,\n            patience=10,\n            batch_size=256,\n            virtual_batch_size=128\n        )\n        return tabnet_model\n\n    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-2, log=True)\n    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mean_squared_error', metrics=['mean_absolute_error'])\n\n    return model\n\n# Define the objective function for Optuna\ndef objective(trial):\n    model = create_model(trial)\n\n    if isinstance(model, TabNetRegressor):\n        model.fit(\n            X_train, y_train,\n            eval_set=[(X_val, y_val)],\n            eval_metric=['mae'],\n            max_epochs=100,\n            patience=10,\n            batch_size=256,\n            virtual_batch_size=128\n        )\n        y_pred = model.predict(X_val)\n        mae = mean_absolute_error(y_val, y_pred)\n    else:\n        model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10, batch_size=32, verbose=0)\n        _, mae = model.evaluate(X_val, y_val, verbose=0)\n\n    model_type = trial.params['model_type']\n    model_performance[model_type] = mae\n    return mae\n\n# Run the Bayesian optimization\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=10)  # Set n_trials as needed\n\n# Get the best hyperparameters\nbest_params = study.best_params\nprint(f\"Best hyperparameters: {best_params}\")\n\n# Display performance of each model\nprint(\"Model performance (MAE):\")\nfor model_type, mae in model_performance.items():\n    print(f\"{model_type}: {mae}\")\n\n# Plot the model performance\nplt.figure(figsize=(10, 5))\nplt.bar(model_performance.keys(), model_performance.values(), color='skyblue')\nplt.xlabel('Model Type')\nplt.ylabel('Mean Absolute Error')\nplt.title('Model Performance Comparison')\nplt.show()\n",
  "history_output" : "Variables in the .npz file: ['X_train', 'X_test', 'y_train', 'y_test']\n[I 2024-09-27 16:19:37,143] A new study created in memory with name: no-name-67072c35-a556-4562-8504-c9ff67217379\n[I 2024-09-27 16:19:37,747] Trial 0 finished with value: 0.0449388213455677 and parameters: {'model_type': 'dense', 'num_layers': 1, 'num_units_l0': 60, 'dropout_rate_l0': 0.14686138411278696, 'learning_rate': 0.0008496021206082136}. Best is trial 0 with value: 0.0449388213455677.\n[I 2024-09-27 16:19:38,887] Trial 1 finished with value: 0.31271544098854065 and parameters: {'model_type': 'transformer', 'num_heads': 4, 'key_dim': 60, 'learning_rate': 0.00011239198060713093}. Best is trial 0 with value: 0.0449388213455677.\n[I 2024-09-27 16:19:39,411] Trial 2 finished with value: 0.04936045780777931 and parameters: {'model_type': 'cnn', 'filters': 53, 'kernel_size': 4, 'learning_rate': 0.0033878796948567788}. Best is trial 0 with value: 0.0449388213455677.\n[I 2024-09-27 16:19:40,642] Trial 3 finished with value: 0.09967638552188873 and parameters: {'model_type': 'transformer', 'num_heads': 6, 'key_dim': 36, 'learning_rate': 0.0005487583696645022}. Best is trial 0 with value: 0.0449388213455677.\n[I 2024-09-27 16:19:41,458] Trial 4 finished with value: 0.06228038668632507 and parameters: {'model_type': 'dense', 'num_layers': 4, 'num_units_l0': 28, 'dropout_rate_l0': 0.11442556816131698, 'num_units_l1': 19, 'dropout_rate_l1': 0.1837025189683541, 'num_units_l2': 28, 'dropout_rate_l2': 0.3043043827472813, 'num_units_l3': 44, 'dropout_rate_l3': 0.2975610526197204, 'learning_rate': 0.0009736690321857154}. Best is trial 0 with value: 0.0449388213455677.\n[I 2024-09-27 16:19:42,744] Trial 5 finished with value: 0.09982165694236755 and parameters: {'model_type': 'transformer', 'num_heads': 7, 'key_dim': 53, 'learning_rate': 0.0024062138871676917}. Best is trial 0 with value: 0.0449388213455677.\n[I 2024-09-27 16:19:43,759] Trial 6 finished with value: 0.2032669633626938 and parameters: {'model_type': 'lstm', 'lstm_units': 22, 'learning_rate': 3.4913565367779823e-05}. Best is trial 0 with value: 0.0449388213455677.\n/opt/anaconda3/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n  warnings.warn(f\"Device used : {self.device}\")\n[W 2024-09-27 16:19:43,762] Trial 7 failed with parameters: {'model_type': 'tabnet', 'n_d': 39, 'n_a': 17, 'n_steps': 8, 'gamma': 1.6718383771259564, 'lambda_sparse': 0.0005228912641464338} because of the following error: ValueError('Found array with dim 3. None expected <= 2.').\nTraceback (most recent call last):\n  File \"/opt/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n    value_or_values = func(trial)\n                      ^^^^^^^^^^^\n  File \"/Users/vishesh/gw-workspace/s0eg04dpv2v/auto_ml.py\", line 100, in objective\n    model = create_model(trial)\n            ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/vishesh/gw-workspace/s0eg04dpv2v/auto_ml.py\", line 82, in create_model\n    tabnet_model.fit(\n  File \"/opt/anaconda3/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py\", line 217, in fit\n    check_input(X_train)\n  File \"/opt/anaconda3/lib/python3.11/site-packages/pytorch_tabnet/utils.py\", line 507, in check_input\n    check_array(X, accept_sparse=True)\n  File \"/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py\", line 915, in check_array\n    raise ValueError(\nValueError: Found array with dim 3. None expected <= 2.\n[W 2024-09-27 16:19:43,763] Trial 7 failed with value None.\nTraceback (most recent call last):\n  File \"/Users/vishesh/gw-workspace/s0eg04dpv2v/auto_ml.py\", line 124, in <module>\n    study.optimize(objective, n_trials=10)  # Set n_trials as needed\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/optuna/study/study.py\", line 475, in optimize\n    _optimize(\n  File \"/opt/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 63, in _optimize\n    _optimize_sequential(\n  File \"/opt/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 160, in _optimize_sequential\n    frozen_trial = _run_trial(study, func, catch)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 248, in _run_trial\n    raise func_err\n  File \"/opt/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n    value_or_values = func(trial)\n                      ^^^^^^^^^^^\n  File \"/Users/vishesh/gw-workspace/s0eg04dpv2v/auto_ml.py\", line 100, in objective\n    model = create_model(trial)\n            ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/vishesh/gw-workspace/s0eg04dpv2v/auto_ml.py\", line 82, in create_model\n    tabnet_model.fit(\n  File \"/opt/anaconda3/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py\", line 217, in fit\n    check_input(X_train)\n  File \"/opt/anaconda3/lib/python3.11/site-packages/pytorch_tabnet/utils.py\", line 507, in check_input\n    check_array(X, accept_sparse=True)\n  File \"/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py\", line 915, in check_array\n    raise ValueError(\nValueError: Found array with dim 3. None expected <= 2.\n",
  "history_begin_time" : 1727468372881,
  "history_end_time" : 1727468384412,
  "history_notes" : null,
  "history_process" : "vnrvoi",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "k3vv45p5mqc",
  "history_input" : "import numpy as np\nimport optuna\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import Dense, Dropout, Conv1D, MaxPooling1D, Flatten, LSTM, Input, MultiHeadAttention, LayerNormalization, Add\nfrom tensorflow.keras.optimizers import Adam\nfrom pytorch_tabnet.tab_model import TabNetRegressor\n\n# Load preprocessed data\ndata_path = '/Users/vishesh/gw-workspace/L6gDfkNJofh8/preprocessed_data.npz'\ndata = np.load(data_path)\n\n# List all variable names in the .npz file to identify contents\nprint(\"Variables in the .npz file:\", data.files)\n\n# Load data based on contents of the .npz file\nX_train = data['X_train']\ny_train = data['y_train']\n\n# Check if 'X_val' and 'y_val' exist; if not, create validation split\nif 'X_val' in data.files and 'y_val' in data.files:\n    X_val = data['X_val']\n    y_val = data['y_val']\nelse:\n    # Split the data into training and validation sets\n    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n\n# Initialize a dictionary to store model performance\nmodel_performance = {}\n\n# Define the function to create various models\ndef create_model(trial):\n    input_shape = X_train.shape[1]\n    model_type = trial.suggest_categorical('model_type', ['dense', 'cnn', 'lstm', 'transformer', 'tabnet'])\n\n    if model_type == 'dense':\n        model = Sequential()\n        model.add(Input(shape=(input_shape,)))  # Use Input layer to specify input shape\n        num_layers = trial.suggest_int('num_layers', 1, 5)\n        for i in range(num_layers):\n            num_units = trial.suggest_int(f'num_units_l{i}', 16, 128, log=True)\n            model.add(Dense(num_units, activation='relu'))\n            dropout_rate = trial.suggest_float(f'dropout_rate_l{i}', 0.1, 0.5)\n            model.add(Dropout(dropout_rate))\n        model.add(Dense(1, activation='linear'))\n    \n    elif model_type == 'cnn':\n        model = Sequential()\n        model.add(Input(shape=(input_shape, 1)))  # Use Input layer for CNN input shape\n        model.add(Conv1D(filters=trial.suggest_int('filters', 16, 64, log=True),\n                         kernel_size=trial.suggest_int('kernel_size', 3, 5),\n                         activation='relu'))\n        model.add(MaxPooling1D(pool_size=2))\n        model.add(Flatten())\n        model.add(Dense(1, activation='linear'))\n    \n    elif model_type == 'lstm':\n        model = Sequential()\n        model.add(Input(shape=(input_shape, 1)))  # Use Input layer for LSTM input shape\n        model.add(LSTM(units=trial.suggest_int('lstm_units', 16, 64, log=True)))\n        model.add(Dense(1, activation='linear'))\n    \n    elif model_type == 'transformer':\n        inputs = Input(shape=(input_shape, 1))\n        attention = MultiHeadAttention(num_heads=trial.suggest_int('num_heads', 2, 8), key_dim=trial.suggest_int('key_dim', 16, 64))(inputs, inputs)\n        attention = Add()([inputs, attention])\n        attention = LayerNormalization(epsilon=1e-6)(attention)\n        outputs = Flatten()(attention)\n        outputs = Dense(1, activation='linear')(outputs)\n        model = Model(inputs=inputs, outputs=outputs)\n    \n    elif model_type == 'tabnet':\n        tabnet_model = TabNetRegressor(\n            n_d=trial.suggest_int('n_d', 8, 64),\n            n_a=trial.suggest_int('n_a', 8, 64),\n            n_steps=trial.suggest_int('n_steps', 3, 10),\n            gamma=trial.suggest_float('gamma', 1.0, 2.0),\n            lambda_sparse=trial.suggest_float('lambda_sparse', 1e-6, 1e-3)\n        )\n        tabnet_model.fit(\n            X_train, y_train,\n            eval_set=[(X_val, y_val)],\n            eval_metric=['mae'],\n            max_epochs=100,\n            patience=10,\n            batch_size=256,\n            virtual_batch_size=128\n        )\n        return tabnet_model\n\n    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-2, log=True)\n    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mean_squared_error', metrics=['mean_absolute_error'])\n\n    return model\n\n# Define the objective function for Optuna\ndef objective(trial):\n    model = create_model(trial)\n\n    if isinstance(model, TabNetRegressor):\n        model.fit(\n            X_train, y_train,\n            eval_set=[(X_val, y_val)],\n            eval_metric=['mae'],\n            max_epochs=100,\n            patience=10,\n            batch_size=256,\n            virtual_batch_size=128\n        )\n        y_pred = model.predict(X_val)\n        mae = mean_absolute_error(y_val, y_pred)\n    else:\n        model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10, batch_size=32, verbose=0)\n        _, mae = model.evaluate(X_val, y_val, verbose=0)\n\n    model_type = trial.params['model_type']\n    model_performance[model_type] = mae\n    return mae\n\n# Run the Bayesian optimization\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=10)  # Set n_trials as needed\n\n# Get the best hyperparameters\nbest_params = study.best_params\nprint(f\"Best hyperparameters: {best_params}\")\n\n# Display performance of each model\nprint(\"Model performance (MAE):\")\nfor model_type, mae in model_performance.items():\n    print(f\"{model_type}: {mae}\")\n\n# Plot the model performance\nplt.figure(figsize=(10, 5))\nplt.bar(model_performance.keys(), model_performance.values(), color='skyblue')\nplt.xlabel('Model Type')\nplt.ylabel('Mean Absolute Error')\nplt.title('Model Performance Comparison')\nplt.show()\n",
  "history_output" : "Variables in the .npz file: ['X_train', 'X_test', 'y_train', 'y_test']\n[I 2024-09-27 16:19:06,229] A new study created in memory with name: no-name-3c39d225-174b-4dcc-8959-573f8da007a9\n[I 2024-09-27 16:19:07,494] Trial 0 finished with value: 0.09967934340238571 and parameters: {'model_type': 'transformer', 'num_heads': 5, 'key_dim': 56, 'learning_rate': 0.0007773963988492908}. Best is trial 0 with value: 0.09967934340238571.\n[I 2024-09-27 16:19:08,007] Trial 1 finished with value: 0.06007281318306923 and parameters: {'model_type': 'cnn', 'filters': 60, 'kernel_size': 3, 'learning_rate': 0.0004531814588984687}. Best is trial 1 with value: 0.06007281318306923.\n[I 2024-09-27 16:19:09,142] Trial 2 finished with value: 0.03789413347840309 and parameters: {'model_type': 'lstm', 'lstm_units': 40, 'learning_rate': 0.0038624270842853954}. Best is trial 2 with value: 0.03789413347840309.\n/opt/anaconda3/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n  warnings.warn(f\"Device used : {self.device}\")\n[W 2024-09-27 16:19:09,146] Trial 3 failed with parameters: {'model_type': 'tabnet', 'n_d': 33, 'n_a': 16, 'n_steps': 10, 'gamma': 1.3949642217263565, 'lambda_sparse': 0.00016546756763627434} because of the following error: ValueError('Found array with dim 3. None expected <= 2.').\nTraceback (most recent call last):\n  File \"/opt/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n    value_or_values = func(trial)\n                      ^^^^^^^^^^^\n  File \"/Users/vishesh/gw-workspace/k3vv45p5mqc/auto_ml.py\", line 100, in objective\n    model = create_model(trial)\n            ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/vishesh/gw-workspace/k3vv45p5mqc/auto_ml.py\", line 82, in create_model\n    tabnet_model.fit(\n  File \"/opt/anaconda3/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py\", line 217, in fit\n    check_input(X_train)\n  File \"/opt/anaconda3/lib/python3.11/site-packages/pytorch_tabnet/utils.py\", line 507, in check_input\n    check_array(X, accept_sparse=True)\n  File \"/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py\", line 915, in check_array\n    raise ValueError(\nValueError: Found array with dim 3. None expected <= 2.\n[W 2024-09-27 16:19:09,148] Trial 3 failed with value None.\nTraceback (most recent call last):\n  File \"/Users/vishesh/gw-workspace/k3vv45p5mqc/auto_ml.py\", line 124, in <module>\n    study.optimize(objective, n_trials=10)  # Set n_trials as needed\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/optuna/study/study.py\", line 475, in optimize\n    _optimize(\n  File \"/opt/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 63, in _optimize\n    _optimize_sequential(\n  File \"/opt/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 160, in _optimize_sequential\n    frozen_trial = _run_trial(study, func, catch)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 248, in _run_trial\n    raise func_err\n  File \"/opt/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n    value_or_values = func(trial)\n                      ^^^^^^^^^^^\n  File \"/Users/vishesh/gw-workspace/k3vv45p5mqc/auto_ml.py\", line 100, in objective\n    model = create_model(trial)\n            ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/vishesh/gw-workspace/k3vv45p5mqc/auto_ml.py\", line 82, in create_model\n    tabnet_model.fit(\n  File \"/opt/anaconda3/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py\", line 217, in fit\n    check_input(X_train)\n  File \"/opt/anaconda3/lib/python3.11/site-packages/pytorch_tabnet/utils.py\", line 507, in check_input\n    check_array(X, accept_sparse=True)\n  File \"/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py\", line 915, in check_array\n    raise ValueError(\nValueError: Found array with dim 3. None expected <= 2.\n",
  "history_begin_time" : 1727468342416,
  "history_end_time" : 1727468349711,
  "history_notes" : null,
  "history_process" : "vnrvoi",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "51ecty8pw5z",
  "history_input" : "import numpy as np\nimport optuna\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import Dense, Dropout, Conv1D, MaxPooling1D, Flatten, LSTM, Input, MultiHeadAttention, LayerNormalization, Add\nfrom tensorflow.keras.optimizers import Adam\nfrom pytorch_tabnet.tab_model import TabNetRegressor\n\n# Load preprocessed data\ndata_path = '/Users/vishesh/gw-workspace/L6gDfkNJofh8/preprocessed_data.npz'\ndata = np.load(data_path)\n\n# List all variable names in the .npz file to identify contents\nprint(\"Variables in the .npz file:\", data.files)\n\n# Load data based on contents of the .npz file\nX_train = data['X_train']\ny_train = data['y_train']\n\n# Check if 'X_val' and 'y_val' exist; if not, create validation split\nif 'X_val' in data.files and 'y_val' in data.files:\n    X_val = data['X_val']\n    y_val = data['y_val']\nelse:\n    # Split the data into training and validation sets\n    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n\n# Initialize a dictionary to store model performance\nmodel_performance = {}\n\n# Define the function to create various models\ndef create_model(trial):\n    input_shape = X_train.shape[1]\n    model_type = trial.suggest_categorical('model_type', ['dense', 'cnn', 'lstm', 'transformer', 'tabnet'])\n\n    if model_type == 'dense':\n        model = Sequential()\n        model.add(Input(shape=(input_shape,)))  # Use Input layer to specify input shape\n        num_layers = trial.suggest_int('num_layers', 1, 5)\n        for i in range(num_layers):\n            num_units = trial.suggest_int(f'num_units_l{i}', 16, 128, log=True)\n            model.add(Dense(num_units, activation='relu'))\n            dropout_rate = trial.suggest_float(f'dropout_rate_l{i}', 0.1, 0.5)\n            model.add(Dropout(dropout_rate))\n        model.add(Dense(1, activation='linear'))\n    \n    elif model_type == 'cnn':\n        model = Sequential()\n        model.add(Input(shape=(input_shape, 1)))  # Use Input layer for CNN input shape\n        model.add(Conv1D(filters=trial.suggest_int('filters', 16, 64, log=True),\n                         kernel_size=trial.suggest_int('kernel_size', 3, 5),\n                         activation='relu'))\n        model.add(MaxPooling1D(pool_size=2))\n        model.add(Flatten())\n        model.add(Dense(1, activation='linear'))\n    \n    elif model_type == 'lstm':\n        model = Sequential()\n        model.add(Input(shape=(input_shape, 1)))  # Use Input layer for LSTM input shape\n        model.add(LSTM(units=trial.suggest_int('lstm_units', 16, 64, log=True)))\n        model.add(Dense(1, activation='linear'))\n    \n    elif model_type == 'transformer':\n        inputs = Input(shape=(input_shape, 1))\n        attention = MultiHeadAttention(num_heads=trial.suggest_int('num_heads', 2, 8), key_dim=trial.suggest_int('key_dim', 16, 64))(inputs, inputs)\n        attention = Add()([inputs, attention])\n        attention = LayerNormalization(epsilon=1e-6)(attention)\n        outputs = Flatten()(attention)\n        outputs = Dense(1, activation='linear')(outputs)\n        model = Model(inputs=inputs, outputs=outputs)\n    \n    elif model_type == 'tabnet':\n        tabnet_model = TabNetRegressor(\n            n_d=trial.suggest_int('n_d', 8, 64),\n            n_a=trial.suggest_int('n_a', 8, 64),\n            n_steps=trial.suggest_int('n_steps', 3, 10),\n            gamma=trial.suggest_float('gamma', 1.0, 2.0),\n            lambda_sparse=trial.suggest_float('lambda_sparse', 1e-6, 1e-3)\n        )\n        tabnet_model.fit(\n            X_train, y_train,\n            eval_set=[(X_val, y_val)],\n            eval_metric=['mae'],\n            max_epochs=100,\n            patience=10,\n            batch_size=256,\n            virtual_batch_size=128\n        )\n        return tabnet_model\n\n    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-2, log=True)\n    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mean_squared_error', metrics=['mean_absolute_error'])\n\n    return model\n\n# Define the objective function for Optuna\ndef objective(trial):\n    model = create_model(trial)\n\n    if isinstance(model, TabNetRegressor):\n        model.fit(\n            X_train, y_train,\n            eval_set=[(X_val, y_val)],\n            eval_metric=['mae'],\n            max_epochs=100,\n            patience=10,\n            batch_size=256,\n            virtual_batch_size=128\n        )\n        y_pred = model.predict(X_val)\n        mae = mean_absolute_error(y_val, y_pred)\n    else:\n        model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10, batch_size=32, verbose=0)\n        _, mae = model.evaluate(X_val, y_val, verbose=0)\n\n    model_type = trial.params['model_type']\n    model_performance[model_type] = mae\n    return mae\n\n# Run the Bayesian optimization\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=10)  # Set n_trials as needed\n\n# Get the best hyperparameters\nbest_params = study.best_params\nprint(f\"Best hyperparameters: {best_params}\")\n\n# Display performance of each model\nprint(\"Model performance (MAE):\")\nfor model_type, mae in model_performance.items():\n    print(f\"{model_type}: {mae}\")\n\n# Plot the model performance\nplt.figure(figsize=(10, 5))\nplt.bar(model_performance.keys(), model_performance.values(), color='skyblue')\nplt.xlabel('Model Type')\nplt.ylabel('Mean Absolute Error')\nplt.title('Model Performance Comparison')\nplt.show()\n",
  "history_output" : "Variables in the .npz file: ['X_train', 'X_test', 'y_train', 'y_test']\n[I 2024-09-27 16:18:14,466] A new study created in memory with name: no-name-13b36e9b-8623-4adf-8dc8-52ceb04ee47f\n[I 2024-09-27 16:18:15,816] Trial 0 finished with value: 0.10084524005651474 and parameters: {'model_type': 'transformer', 'num_heads': 7, 'key_dim': 48, 'learning_rate': 0.002681520726582834}. Best is trial 0 with value: 0.10084524005651474.\n[I 2024-09-27 16:18:16,889] Trial 1 finished with value: 0.0784810483455658 and parameters: {'model_type': 'lstm', 'lstm_units': 20, 'learning_rate': 0.00015395164089192117}. Best is trial 1 with value: 0.0784810483455658.\n[I 2024-09-27 16:18:17,722] Trial 2 finished with value: 0.07196629047393799 and parameters: {'model_type': 'dense', 'num_layers': 3, 'num_units_l0': 22, 'dropout_rate_l0': 0.21543404066736993, 'num_units_l1': 53, 'dropout_rate_l1': 0.22056908562647057, 'num_units_l2': 72, 'dropout_rate_l2': 0.18181044195288562, 'learning_rate': 0.0035107093823946137}. Best is trial 2 with value: 0.07196629047393799.\n[I 2024-09-27 16:18:18,349] Trial 3 finished with value: 0.07505429536104202 and parameters: {'model_type': 'dense', 'num_layers': 2, 'num_units_l0': 44, 'dropout_rate_l0': 0.36251897420997586, 'num_units_l1': 21, 'dropout_rate_l1': 0.2513171977153634, 'learning_rate': 0.0004349775174661655}. Best is trial 2 with value: 0.07196629047393799.\n[I 2024-09-27 16:18:19,505] Trial 4 finished with value: 0.06955752521753311 and parameters: {'model_type': 'lstm', 'lstm_units': 48, 'learning_rate': 7.959066799447995e-05}. Best is trial 4 with value: 0.06955752521753311.\n[I 2024-09-27 16:18:20,511] Trial 5 finished with value: 0.1039140522480011 and parameters: {'model_type': 'lstm', 'lstm_units': 23, 'learning_rate': 5.6604814620760397e-05}. Best is trial 4 with value: 0.06955752521753311.\n[I 2024-09-27 16:18:20,996] Trial 6 finished with value: 0.03620130196213722 and parameters: {'model_type': 'dense', 'num_layers': 1, 'num_units_l0': 57, 'dropout_rate_l0': 0.2295830906235939, 'learning_rate': 0.0031816699772112818}. Best is trial 6 with value: 0.03620130196213722.\n[I 2024-09-27 16:18:21,944] Trial 7 finished with value: 0.19392570853233337 and parameters: {'model_type': 'dense', 'num_layers': 5, 'num_units_l0': 32, 'dropout_rate_l0': 0.4971360510228091, 'num_units_l1': 58, 'dropout_rate_l1': 0.48577309292626025, 'num_units_l2': 98, 'dropout_rate_l2': 0.2874729899173593, 'num_units_l3': 54, 'dropout_rate_l3': 0.30302410976802446, 'num_units_l4': 19, 'dropout_rate_l4': 0.14405669902548954, 'learning_rate': 8.072443561640008e-05}. Best is trial 6 with value: 0.03620130196213722.\n[I 2024-09-27 16:18:23,117] Trial 8 finished with value: 0.04415113478899002 and parameters: {'model_type': 'lstm', 'lstm_units': 63, 'learning_rate': 0.0009475788023192316}. Best is trial 6 with value: 0.03620130196213722.\n[I 2024-09-27 16:18:24,390] Trial 9 finished with value: 0.24924220144748688 and parameters: {'model_type': 'lstm', 'lstm_units': 60, 'learning_rate': 1.4865207146498436e-05}. Best is trial 6 with value: 0.03620130196213722.\nBest hyperparameters: {'model_type': 'dense', 'num_layers': 1, 'num_units_l0': 57, 'dropout_rate_l0': 0.2295830906235939, 'learning_rate': 0.0031816699772112818}\nModel performance (MAE):\ntransformer: 0.10084524005651474\nlstm: 0.24924220144748688\ndense: 0.19392570853233337\n2024-09-27 16:18:24.947 python[35562:5734945] +[IMKClient subclass]: chose IMKClient_Legacy\n2024-09-27 16:18:24.948 python[35562:5734945] +[IMKInputSession subclass]: chose IMKInputSession_Legacy\n",
  "history_begin_time" : 1727468290112,
  "history_end_time" : 1727468311727,
  "history_notes" : null,
  "history_process" : "vnrvoi",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "2vlyjf0rio2",
  "history_input" : "import numpy as np\nimport optuna\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import Dense, Dropout, Conv1D, MaxPooling1D, Flatten, LSTM, Input, MultiHeadAttention, LayerNormalization, Add\nfrom tensorflow.keras.optimizers import Adam\nfrom pytorch_tabnet.tab_model import TabNetRegressor\n\n# Load preprocessed data\ndata_path = '/Users/vishesh/gw-workspace/L6gDfkNJofh8/preprocessed_data.npz'\ndata = np.load(data_path)\n\n# List all variable names in the .npz file to identify contents\nprint(\"Variables in the .npz file:\", data.files)\n\n# Load data based on contents of the .npz file\nX_train = data['X_train']\ny_train = data['y_train']\n\n# Check if 'X_val' and 'y_val' exist; if not, create validation split\nif 'X_val' in data.files and 'y_val' in data.files:\n    X_val = data['X_val']\n    y_val = data['y_val']\nelse:\n    # Split the data into training and validation sets\n    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n\n# Initialize a dictionary to store model performance\nmodel_performance = {}\n\n# Define the function to create various models\ndef create_model(trial):\n    input_shape = X_train.shape[1]\n    model_type = trial.suggest_categorical('model_type', ['dense', 'cnn', 'lstm', 'transformer', 'tabnet'])\n\n    if model_type == 'dense':\n        model = Sequential()\n        model.add(Input(shape=(input_shape,)))  # Use Input layer to specify input shape\n        num_layers = trial.suggest_int('num_layers', 1, 5)\n        for i in range(num_layers):\n            num_units = trial.suggest_int(f'num_units_l{i}', 16, 128, log=True)\n            model.add(Dense(num_units, activation='relu'))\n            dropout_rate = trial.suggest_float(f'dropout_rate_l{i}', 0.1, 0.5)\n            model.add(Dropout(dropout_rate))\n        model.add(Dense(1, activation='linear'))\n    \n    elif model_type == 'cnn':\n        model = Sequential()\n        model.add(Input(shape=(input_shape, 1)))  # Use Input layer for CNN input shape\n        model.add(Conv1D(filters=trial.suggest_int('filters', 16, 64, log=True),\n                         kernel_size=trial.suggest_int('kernel_size', 3, 5),\n                         activation='relu'))\n        model.add(MaxPooling1D(pool_size=2))\n        model.add(Flatten())\n        model.add(Dense(1, activation='linear'))\n    \n    elif model_type == 'lstm':\n        model = Sequential()\n        model.add(Input(shape=(input_shape, 1)))  # Use Input layer for LSTM input shape\n        model.add(LSTM(units=trial.suggest_int('lstm_units', 16, 64, log=True)))\n        model.add(Dense(1, activation='linear'))\n    \n    elif model_type == 'transformer':\n        inputs = Input(shape=(input_shape, 1))\n        attention = MultiHeadAttention(num_heads=trial.suggest_int('num_heads', 2, 8), key_dim=trial.suggest_int('key_dim', 16, 64))(inputs, inputs)\n        attention = Add()([inputs, attention])\n        attention = LayerNormalization(epsilon=1e-6)(attention)\n        outputs = Flatten()(attention)\n        outputs = Dense(1, activation='linear')(outputs)\n        model = Model(inputs=inputs, outputs=outputs)\n    \n    elif model_type == 'tabnet':\n        tabnet_model = TabNetRegressor(\n            n_d=trial.suggest_int('n_d', 8, 64),\n            n_a=trial.suggest_int('n_a', 8, 64),\n            n_steps=trial.suggest_int('n_steps', 3, 10),\n            gamma=trial.suggest_float('gamma', 1.0, 2.0),\n            lambda_sparse=trial.suggest_float('lambda_sparse', 1e-6, 1e-3)\n        )\n        tabnet_model.fit(\n            X_train, y_train,\n            eval_set=[(X_val, y_val)],\n            eval_metric=['mae'],\n            max_epochs=100,\n            patience=10,\n            batch_size=256,\n            virtual_batch_size=128\n        )\n        return tabnet_model\n\n    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-2, log=True)\n    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mean_squared_error', metrics=['mean_absolute_error'])\n\n    return model\n\n# Define the objective function for Optuna\ndef objective(trial):\n    model = create_model(trial)\n\n    if isinstance(model, TabNetRegressor):\n        model.fit(\n            X_train, y_train,\n            eval_set=[(X_val, y_val)],\n            eval_metric=['mae'],\n            max_epochs=100,\n            patience=10,\n            batch_size=256,\n            virtual_batch_size=128\n        )\n        y_pred = model.predict(X_val)\n        mae = mean_absolute_error(y_val, y_pred)\n    else:\n        model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10, batch_size=32, verbose=0)\n        _, mae = model.evaluate(X_val, y_val, verbose=0)\n\n    model_type = trial.params['model_type']\n    model_performance[model_type] = mae\n    return mae\n\n# Run the Bayesian optimization\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=10)  # Set n_trials as needed\n\n# Get the best hyperparameters\nbest_params = study.best_params\nprint(f\"Best hyperparameters: {best_params}\")\n\n# Display performance of each model\nprint(\"Model performance (MAE):\")\nfor model_type, mae in model_performance.items():\n    print(f\"{model_type}: {mae}\")\n\n# Plot the model performance\nplt.figure(figsize=(10, 5))\nplt.bar(model_performance.keys(), model_performance.values(), color='skyblue')\nplt.xlabel('Model Type')\nplt.ylabel('Mean Absolute Error')\nplt.title('Model Performance Comparison')\nplt.show()\n",
  "history_output" : "Variables in the .npz file: ['X_train', 'X_test', 'y_train', 'y_test']\n[I 2024-09-27 16:17:19,547] A new study created in memory with name: no-name-a78612d9-4821-48e1-9658-bbc07ff15bdd\n[I 2024-09-27 16:17:21,432] Trial 0 finished with value: 0.09966925531625748 and parameters: {'model_type': 'transformer', 'num_heads': 7, 'key_dim': 63, 'learning_rate': 0.0010772565853657305}. Best is trial 0 with value: 0.09966925531625748.\n[I 2024-09-27 16:17:22,271] Trial 1 finished with value: 0.10364071279764175 and parameters: {'model_type': 'dense', 'num_layers': 4, 'num_units_l0': 45, 'dropout_rate_l0': 0.29706023895014866, 'num_units_l1': 19, 'dropout_rate_l1': 0.17102353149779398, 'num_units_l2': 29, 'dropout_rate_l2': 0.11857037775772361, 'num_units_l3': 100, 'dropout_rate_l3': 0.40609896068131157, 'learning_rate': 0.0002848917992839276}. Best is trial 0 with value: 0.09966925531625748.\n[I 2024-09-27 16:17:23,592] Trial 2 finished with value: 0.12180174142122269 and parameters: {'model_type': 'transformer', 'num_heads': 5, 'key_dim': 34, 'learning_rate': 0.0003502869857149193}. Best is trial 0 with value: 0.09966925531625748.\n[I 2024-09-27 16:17:24,630] Trial 3 finished with value: 0.06288791447877884 and parameters: {'model_type': 'lstm', 'lstm_units': 23, 'learning_rate': 0.0008689753971346424}. Best is trial 3 with value: 0.06288791447877884.\n/opt/anaconda3/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n  warnings.warn(f\"Device used : {self.device}\")\n[W 2024-09-27 16:17:24,633] Trial 4 failed with parameters: {'model_type': 'tabnet', 'n_d': 29, 'n_a': 34, 'n_steps': 5, 'gamma': 1.2488469032940572, 'lambda_sparse': 0.00035443179712279485} because of the following error: ValueError('Found array with dim 3. None expected <= 2.').\nTraceback (most recent call last):\n  File \"/opt/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n    value_or_values = func(trial)\n                      ^^^^^^^^^^^\n  File \"/Users/vishesh/gw-workspace/2vlyjf0rio2/auto_ml.py\", line 100, in objective\n    model = create_model(trial)\n            ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/vishesh/gw-workspace/2vlyjf0rio2/auto_ml.py\", line 82, in create_model\n    tabnet_model.fit(\n  File \"/opt/anaconda3/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py\", line 217, in fit\n    check_input(X_train)\n  File \"/opt/anaconda3/lib/python3.11/site-packages/pytorch_tabnet/utils.py\", line 507, in check_input\n    check_array(X, accept_sparse=True)\n  File \"/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py\", line 915, in check_array\n    raise ValueError(\nValueError: Found array with dim 3. None expected <= 2.\n[W 2024-09-27 16:17:24,635] Trial 4 failed with value None.\nTraceback (most recent call last):\n  File \"/Users/vishesh/gw-workspace/2vlyjf0rio2/auto_ml.py\", line 124, in <module>\n    study.optimize(objective, n_trials=10)  # Set n_trials as needed\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/optuna/study/study.py\", line 475, in optimize\n    _optimize(\n  File \"/opt/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 63, in _optimize\n    _optimize_sequential(\n  File \"/opt/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 160, in _optimize_sequential\n    frozen_trial = _run_trial(study, func, catch)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 248, in _run_trial\n    raise func_err\n  File \"/opt/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n    value_or_values = func(trial)\n                      ^^^^^^^^^^^\n  File \"/Users/vishesh/gw-workspace/2vlyjf0rio2/auto_ml.py\", line 100, in objective\n    model = create_model(trial)\n            ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/vishesh/gw-workspace/2vlyjf0rio2/auto_ml.py\", line 82, in create_model\n    tabnet_model.fit(\n  File \"/opt/anaconda3/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py\", line 217, in fit\n    check_input(X_train)\n  File \"/opt/anaconda3/lib/python3.11/site-packages/pytorch_tabnet/utils.py\", line 507, in check_input\n    check_array(X, accept_sparse=True)\n  File \"/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py\", line 915, in check_array\n    raise ValueError(\nValueError: Found array with dim 3. None expected <= 2.\n",
  "history_begin_time" : 1727468234600,
  "history_end_time" : 1727468245338,
  "history_notes" : null,
  "history_process" : "vnrvoi",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "K9LgcP4IZ56k",
  "history_input" : "import numpy as np\nimport optuna\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import Dense, Dropout, Conv1D, MaxPooling1D, Flatten, LSTM, Input, MultiHeadAttention, LayerNormalization, Add\nfrom tensorflow.keras.optimizers import Adam\nfrom pytorch_tabnet.tab_model import TabNetRegressor\n\n# Load preprocessed data\ndata_path = '/Users/vishesh/gw-workspace/L6gDfkNJofh8/preprocessed_data.npz'\ndata = np.load(data_path)\n\n# List all variable names in the .npz file to identify contents\nprint(\"Variables in the .npz file:\", data.files)\n\n# Load data based on contents of the .npz file\nX_train = data['X_train']\ny_train = data['y_train']\n\n# Check if 'X_val' and 'y_val' exist; if not, create validation split\nif 'X_val' in data.files and 'y_val' in data.files:\n    X_val = data['X_val']\n    y_val = data['y_val']\nelse:\n    # Split the data into training and validation sets\n    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n\n# Initialize a dictionary to store model performance\nmodel_performance = {}\n\n# Define the function to create various models\ndef create_model(trial):\n    input_shape = X_train.shape[1]\n    model_type = trial.suggest_categorical('model_type', ['dense', 'cnn', 'lstm', 'transformer', 'tabnet'])\n\n    if model_type == 'dense':\n        model = Sequential()\n        model.add(Input(shape=(input_shape,)))  # Use Input layer to specify input shape\n        num_layers = trial.suggest_int('num_layers', 1, 5)\n        for i in range(num_layers):\n            num_units = trial.suggest_int(f'num_units_l{i}', 16, 128, log=True)\n            model.add(Dense(num_units, activation='relu'))\n            dropout_rate = trial.suggest_float(f'dropout_rate_l{i}', 0.1, 0.5)\n            model.add(Dropout(dropout_rate))\n        model.add(Dense(1, activation='linear'))\n    \n    elif model_type == 'cnn':\n        model = Sequential()\n        model.add(Input(shape=(input_shape, 1)))  # Use Input layer for CNN input shape\n        model.add(Conv1D(filters=trial.suggest_int('filters', 16, 64, log=True),\n                         kernel_size=trial.suggest_int('kernel_size', 3, 5),\n                         activation='relu'))\n        model.add(MaxPooling1D(pool_size=2))\n        model.add(Flatten())\n        model.add(Dense(1, activation='linear'))\n    \n    elif model_type == 'lstm':\n        model = Sequential()\n        model.add(Input(shape=(input_shape, 1)))  # Use Input layer for LSTM input shape\n        model.add(LSTM(units=trial.suggest_int('lstm_units', 16, 64, log=True)))\n        model.add(Dense(1, activation='linear'))\n    \n    elif model_type == 'transformer':\n        inputs = Input(shape=(input_shape, 1))\n        attention = MultiHeadAttention(num_heads=trial.suggest_int('num_heads', 2, 8), key_dim=trial.suggest_int('key_dim', 16, 64))(inputs, inputs)\n        attention = Add()([inputs, attention])\n        attention = LayerNormalization(epsilon=1e-6)(attention)\n        outputs = Flatten()(attention)\n        outputs = Dense(1, activation='linear')(outputs)\n        model = Model(inputs=inputs, outputs=outputs)\n    \n    elif model_type == 'tabnet':\n        tabnet_model = TabNetRegressor(\n            n_d=trial.suggest_int('n_d', 8, 64),\n            n_a=trial.suggest_int('n_a', 8, 64),\n            n_steps=trial.suggest_int('n_steps', 3, 10),\n            gamma=trial.suggest_float('gamma', 1.0, 2.0),\n            lambda_sparse=trial.suggest_float('lambda_sparse', 1e-6, 1e-3)\n        )\n        tabnet_model.fit(\n            X_train, y_train,\n            eval_set=[(X_val, y_val)],\n            eval_metric=['mae'],\n            max_epochs=100,\n            patience=10,\n            batch_size=256,\n            virtual_batch_size=128\n        )\n        return tabnet_model\n\n    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-2, log=True)\n    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mean_squared_error', metrics=['mean_absolute_error'])\n\n    return model\n\n# Define the objective function for Optuna\ndef objective(trial):\n    model = create_model(trial)\n\n    if isinstance(model, TabNetRegressor):\n        model.fit(\n            X_train, y_train,\n            eval_set=[(X_val, y_val)],\n            eval_metric=['mae'],\n            max_epochs=100,\n            patience=10,\n            batch_size=256,\n            virtual_batch_size=128\n        )\n        y_pred = model.predict(X_val)\n        mae = mean_absolute_error(y_val, y_pred)\n    else:\n        model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10, batch_size=32, verbose=0)\n        _, mae = model.evaluate(X_val, y_val, verbose=0)\n\n    model_type = trial.params['model_type']\n    model_performance[model_type] = mae\n    return mae\n\n# Run the Bayesian optimization\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=10)  # Set n_trials as needed\n\n# Get the best hyperparameters\nbest_params = study.best_params\nprint(f\"Best hyperparameters: {best_params}\")\n\n# Display performance of each model\nprint(\"Model performance (MAE):\")\nfor model_type, mae in model_performance.items():\n    print(f\"{model_type}: {mae}\")\n\n# Plot the model performance\nplt.figure(figsize=(10, 5))\nplt.bar(model_performance.keys(), model_performance.values(), color='skyblue')\nplt.xlabel('Model Type')\nplt.ylabel('Mean Absolute Error')\nplt.title('Model Performance Comparison')\nplt.show()\n",
  "history_output" : "Variables in the .npz file: ['X_train', 'X_test', 'y_train', 'y_test']\n[I 2024-09-27 16:14:01,754] A new study created in memory with name: no-name-310ebaeb-e661-420f-9448-916f44446e4e\n[I 2024-09-27 16:14:02,342] Trial 0 finished with value: 0.18207550048828125 and parameters: {'model_type': 'cnn', 'filters': 47, 'kernel_size': 3, 'learning_rate': 2.4368304404840775e-05}. Best is trial 0 with value: 0.18207550048828125.\n[I 2024-09-27 16:14:03,192] Trial 1 finished with value: 0.06045731157064438 and parameters: {'model_type': 'dense', 'num_layers': 5, 'num_units_l0': 59, 'dropout_rate_l0': 0.195881014645129, 'num_units_l1': 81, 'dropout_rate_l1': 0.26156917541707986, 'num_units_l2': 16, 'dropout_rate_l2': 0.21621210365873092, 'num_units_l3': 27, 'dropout_rate_l3': 0.27297773872398484, 'num_units_l4': 23, 'dropout_rate_l4': 0.42550277634036854, 'learning_rate': 0.009941215728480068}. Best is trial 1 with value: 0.06045731157064438.\n[I 2024-09-27 16:14:03,954] Trial 2 finished with value: 0.10813374072313309 and parameters: {'model_type': 'dense', 'num_layers': 3, 'num_units_l0': 16, 'dropout_rate_l0': 0.4715621350512486, 'num_units_l1': 47, 'dropout_rate_l1': 0.21879214458345686, 'num_units_l2': 41, 'dropout_rate_l2': 0.2251875047258155, 'learning_rate': 0.005651502984635311}. Best is trial 1 with value: 0.06045731157064438.\n[I 2024-09-27 16:14:05,096] Trial 3 finished with value: 0.37968313694000244 and parameters: {'model_type': 'transformer', 'num_heads': 6, 'key_dim': 34, 'learning_rate': 1.984423976990144e-05}. Best is trial 1 with value: 0.06045731157064438.\n[I 2024-09-27 16:14:05,768] Trial 4 finished with value: 0.4266926646232605 and parameters: {'model_type': 'dense', 'num_layers': 3, 'num_units_l0': 16, 'dropout_rate_l0': 0.1696957296832156, 'num_units_l1': 19, 'dropout_rate_l1': 0.3100628186741047, 'num_units_l2': 54, 'dropout_rate_l2': 0.46571723155048356, 'learning_rate': 1.0570493961923364e-05}. Best is trial 1 with value: 0.06045731157064438.\n[I 2024-09-27 16:14:06,757] Trial 5 finished with value: 0.3567501902580261 and parameters: {'model_type': 'lstm', 'lstm_units': 27, 'learning_rate': 2.7974043501174875e-05}. Best is trial 1 with value: 0.06045731157064438.\n[I 2024-09-27 16:14:07,242] Trial 6 finished with value: 0.07542765885591507 and parameters: {'model_type': 'cnn', 'filters': 17, 'kernel_size': 3, 'learning_rate': 0.0001810139897548608}. Best is trial 1 with value: 0.06045731157064438.\n[I 2024-09-27 16:14:08,424] Trial 7 finished with value: 0.06919041275978088 and parameters: {'model_type': 'lstm', 'lstm_units': 58, 'learning_rate': 0.0001213515418861693}. Best is trial 1 with value: 0.06045731157064438.\n[I 2024-09-27 16:14:09,444] Trial 8 finished with value: 0.06996733695268631 and parameters: {'model_type': 'lstm', 'lstm_units': 22, 'learning_rate': 0.0004835481019648589}. Best is trial 1 with value: 0.06045731157064438.\n[I 2024-09-27 16:14:10,937] Trial 9 finished with value: 0.3234713673591614 and parameters: {'model_type': 'transformer', 'num_heads': 8, 'key_dim': 61, 'learning_rate': 0.00011010494156367027}. Best is trial 1 with value: 0.06045731157064438.\nBest hyperparameters: {'model_type': 'dense', 'num_layers': 5, 'num_units_l0': 59, 'dropout_rate_l0': 0.195881014645129, 'num_units_l1': 81, 'dropout_rate_l1': 0.26156917541707986, 'num_units_l2': 16, 'dropout_rate_l2': 0.21621210365873092, 'num_units_l3': 27, 'dropout_rate_l3': 0.27297773872398484, 'num_units_l4': 23, 'dropout_rate_l4': 0.42550277634036854, 'learning_rate': 0.009941215728480068}\nModel performance (MAE):\ncnn: 0.07542765885591507\ndense: 0.4266926646232605\ntransformer: 0.3234713673591614\nlstm: 0.06996733695268631\n2024-09-27 16:14:11.482 python[35335:5724078] +[IMKClient subclass]: chose IMKClient_Legacy\n2024-09-27 16:14:11.482 python[35335:5724078] +[IMKInputSession subclass]: chose IMKInputSession_Legacy\n",
  "history_begin_time" : 1727468037084,
  "history_end_time" : 1727468061689,
  "history_notes" : null,
  "history_process" : "vnrvoi",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "gdWC75PSrzFT",
  "history_input" : "import numpy as np\nimport optuna\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import Dense, Dropout, Conv1D, MaxPooling1D, Flatten, LSTM, Input, MultiHeadAttention, LayerNormalization, Add\nfrom tensorflow.keras.optimizers import Adam\nfrom pytorch_tabnet.tab_model import TabNetRegressor\n\n# Load preprocessed data\ndata_path = '/Users/vishesh/gw-workspace/L6gDfkNJofh8/preprocessed_data.npz'\ndata = np.load(data_path)\n\n# List all variable names in the .npz file to identify contents\nprint(\"Variables in the .npz file:\", data.files)\n\n# Load data based on contents of the .npz file\nX_train = data['X_train']\ny_train = data['y_train']\n\n# Check if 'X_val' and 'y_val' exist; if not, create validation split\nif 'X_val' in data.files and 'y_val' in data.files:\n    X_val = data['X_val']\n    y_val = data['y_val']\nelse:\n    # Split the data into training and validation sets\n    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n\n# Initialize a dictionary to store model performance\nmodel_performance = {}\n\n# Define the function to create various models\ndef create_model(trial):\n    input_shape = X_train.shape[1]\n    model_type = trial.suggest_categorical('model_type', ['dense', 'cnn', 'lstm', 'transformer', 'tabnet'])\n\n    if model_type == 'dense':\n        model = Sequential()\n        model.add(Input(shape=(input_shape,)))  # Use Input layer to specify input shape\n        num_layers = trial.suggest_int('num_layers', 1, 5)\n        for i in range(num_layers):\n            num_units = trial.suggest_int(f'num_units_l{i}', 16, 128, log=True)\n            model.add(Dense(num_units, activation='relu'))\n            dropout_rate = trial.suggest_float(f'dropout_rate_l{i}', 0.1, 0.5)\n            model.add(Dropout(dropout_rate))\n        model.add(Dense(1, activation='linear'))\n    \n    elif model_type == 'cnn':\n        model = Sequential()\n        model.add(Input(shape=(input_shape, 1)))  # Use Input layer for CNN input shape\n        model.add(Conv1D(filters=trial.suggest_int('filters', 16, 64, log=True),\n                         kernel_size=trial.suggest_int('kernel_size', 3, 5),\n                         activation='relu'))\n        model.add(MaxPooling1D(pool_size=2))\n        model.add(Flatten())\n        model.add(Dense(1, activation='linear'))\n    \n    elif model_type == 'lstm':\n        model = Sequential()\n        model.add(Input(shape=(input_shape, 1)))  # Use Input layer for LSTM input shape\n        model.add(LSTM(units=trial.suggest_int('lstm_units', 16, 64, log=True)))\n        model.add(Dense(1, activation='linear'))\n    \n    elif model_type == 'transformer':\n        inputs = Input(shape=(input_shape, 1))\n        attention = MultiHeadAttention(num_heads=trial.suggest_int('num_heads', 2, 8), key_dim=trial.suggest_int('key_dim', 16, 64))(inputs, inputs)\n        attention = Add()([inputs, attention])\n        attention = LayerNormalization(epsilon=1e-6)(attention)\n        outputs = Flatten()(attention)\n        outputs = Dense(1, activation='linear')(outputs)\n        model = Model(inputs=inputs, outputs=outputs)\n    \n    elif model_type == 'tabnet':\n        tabnet_model = TabNetRegressor(\n            n_d=trial.suggest_int('n_d', 8, 64),\n            n_a=trial.suggest_int('n_a', 8, 64),\n            n_steps=trial.suggest_int('n_steps', 3, 10),\n            gamma=trial.suggest_float('gamma', 1.0, 2.0),\n            lambda_sparse=trial.suggest_float('lambda_sparse', 1e-6, 1e-3)\n        )\n        tabnet_model.fit(\n            X_train, y_train,\n            eval_set=[(X_val, y_val)],\n            eval_metric=['mae'],\n            max_epochs=100,\n            patience=10,\n            batch_size=256,\n            virtual_batch_size=128\n        )\n        return tabnet_model\n\n    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-2, log=True)\n    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mean_squared_error', metrics=['mean_absolute_error'])\n\n    return model\n\n# Define the objective function for Optuna\ndef objective(trial):\n    model = create_model(trial)\n\n    if isinstance(model, TabNetRegressor):\n        model.fit(\n            X_train, y_train,\n            eval_set=[(X_val, y_val)],\n            eval_metric=['mae'],\n            max_epochs=100,\n            patience=10,\n            batch_size=256,\n            virtual_batch_size=128\n        )\n        y_pred = model.predict(X_val)\n        mae = mean_absolute_error(y_val, y_pred)\n    else:\n        model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10, batch_size=32, verbose=0)\n        _, mae = model.evaluate(X_val, y_val, verbose=0)\n\n    model_type = trial.params['model_type']\n    model_performance[model_type] = mae\n    return mae\n\n# Run the Bayesian optimization\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=10)  # Set n_trials as needed\n\n# Get the best hyperparameters\nbest_params = study.best_params\nprint(f\"Best hyperparameters: {best_params}\")\n\n# Display performance of each model\nprint(\"Model performance (MAE):\")\nfor model_type, mae in model_performance.items():\n    print(f\"{model_type}: {mae}\")\n\n# Plot the model performance\nplt.figure(figsize=(10, 5))\nplt.bar(model_performance.keys(), model_performance.values(), color='skyblue')\nplt.xlabel('Model Type')\nplt.ylabel('Mean Absolute Error')\nplt.title('Model Performance Comparison')\nplt.show()\n",
  "history_output" : "Variables in the .npz file: ['X_train', 'X_test', 'y_train', 'y_test']\n[I 2024-09-27 16:12:29,931] A new study created in memory with name: no-name-fc956dc0-116b-415a-ae0f-d0635b1e83b9\n[I 2024-09-27 16:12:30,523] Trial 0 finished with value: 0.034600578248500824 and parameters: {'model_type': 'cnn', 'filters': 57, 'kernel_size': 5, 'learning_rate': 0.00997042219435403}. Best is trial 0 with value: 0.034600578248500824.\n/opt/anaconda3/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n  warnings.warn(f\"Device used : {self.device}\")\n[W 2024-09-27 16:12:30,526] Trial 1 failed with parameters: {'model_type': 'tabnet', 'n_d': 24, 'n_a': 18, 'n_steps': 4, 'gamma': 1.7831216484190333, 'lambda_sparse': 0.0007973541005667682} because of the following error: ValueError('Found array with dim 3. None expected <= 2.').\nTraceback (most recent call last):\n  File \"/opt/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n    value_or_values = func(trial)\n                      ^^^^^^^^^^^\n  File \"/Users/vishesh/gw-workspace/gdWC75PSrzFT/auto_ml.py\", line 100, in objective\n    model = create_model(trial)\n            ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/vishesh/gw-workspace/gdWC75PSrzFT/auto_ml.py\", line 82, in create_model\n    tabnet_model.fit(\n  File \"/opt/anaconda3/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py\", line 217, in fit\n    check_input(X_train)\n  File \"/opt/anaconda3/lib/python3.11/site-packages/pytorch_tabnet/utils.py\", line 507, in check_input\n    check_array(X, accept_sparse=True)\n  File \"/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py\", line 915, in check_array\n    raise ValueError(\nValueError: Found array with dim 3. None expected <= 2.\n[W 2024-09-27 16:12:30,528] Trial 1 failed with value None.\nTraceback (most recent call last):\n  File \"/Users/vishesh/gw-workspace/gdWC75PSrzFT/auto_ml.py\", line 124, in <module>\n    study.optimize(objective, n_trials=10)  # Set n_trials as needed\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/optuna/study/study.py\", line 475, in optimize\n    _optimize(\n  File \"/opt/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 63, in _optimize\n    _optimize_sequential(\n  File \"/opt/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 160, in _optimize_sequential\n    frozen_trial = _run_trial(study, func, catch)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 248, in _run_trial\n    raise func_err\n  File \"/opt/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n    value_or_values = func(trial)\n                      ^^^^^^^^^^^\n  File \"/Users/vishesh/gw-workspace/gdWC75PSrzFT/auto_ml.py\", line 100, in objective\n    model = create_model(trial)\n            ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/vishesh/gw-workspace/gdWC75PSrzFT/auto_ml.py\", line 82, in create_model\n    tabnet_model.fit(\n  File \"/opt/anaconda3/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py\", line 217, in fit\n    check_input(X_train)\n  File \"/opt/anaconda3/lib/python3.11/site-packages/pytorch_tabnet/utils.py\", line 507, in check_input\n    check_array(X, accept_sparse=True)\n  File \"/opt/anaconda3/lib/python3.11/site-packages/sklearn/utils/validation.py\", line 915, in check_array\n    raise ValueError(\nValueError: Found array with dim 3. None expected <= 2.\n",
  "history_begin_time" : 1727467945100,
  "history_end_time" : 1727467951031,
  "history_notes" : null,
  "history_process" : "vnrvoi",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "anvbsAOTYweH",
  "history_input" : "import numpy as np\nimport optuna\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import Dense, Dropout, Conv1D, MaxPooling1D, Flatten, LSTM, Input, MultiHeadAttention, LayerNormalization, Add\nfrom tensorflow.keras.optimizers import Adam\nfrom pytorch_tabnet.tab_model import TabNetRegressor\n\n# Load preprocessed data\ndata_path = '/Users/vishesh/gw-workspace/L6gDfkNJofh8/preprocessed_data.npz'\ndata = np.load(data_path)\n\n# List all variable names in the .npz file to identify contents\nprint(\"Variables in the .npz file:\", data.files)\n\n# Load data based on contents of the .npz file\nX_train = data['X_train']\ny_train = data['y_train']\n\n# Check if 'X_val' and 'y_val' exist; if not, create validation split\nif 'X_val' in data.files and 'y_val' in data.files:\n    X_val = data['X_val']\n    y_val = data['y_val']\nelse:\n    # Split the data into training and validation sets\n    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n\n# Initialize a dictionary to store model performance\nmodel_performance = {}\n\n# Define the function to create various models\ndef create_model(trial):\n    input_shape = X_train.shape[1]\n    model_type = trial.suggest_categorical('model_type', ['dense', 'cnn', 'lstm', 'transformer', 'tabnet'])\n\n    if model_type == 'dense':\n        model = Sequential()\n        model.add(Input(shape=(input_shape,)))  # Use Input layer to specify input shape\n        num_layers = trial.suggest_int('num_layers', 1, 5)\n        for i in range(num_layers):\n            num_units = trial.suggest_int(f'num_units_l{i}', 16, 128, log=True)\n            model.add(Dense(num_units, activation='relu'))\n            dropout_rate = trial.suggest_float(f'dropout_rate_l{i}', 0.1, 0.5)\n            model.add(Dropout(dropout_rate))\n        model.add(Dense(1, activation='linear'))\n    \n    elif model_type == 'cnn':\n        model = Sequential()\n        model.add(Input(shape=(input_shape, 1)))  # Use Input layer for CNN input shape\n        model.add(Conv1D(filters=trial.suggest_int('filters', 16, 64, log=True),\n                         kernel_size=trial.suggest_int('kernel_size', 3, 5),\n                         activation='relu'))\n        model.add(MaxPooling1D(pool_size=2))\n        model.add(Flatten())\n        model.add(Dense(1, activation='linear'))\n    \n    elif model_type == 'lstm':\n        model = Sequential()\n        model.add(Input(shape=(input_shape, 1)))  # Use Input layer for LSTM input shape\n        model.add(LSTM(units=trial.suggest_int('lstm_units', 16, 64, log=True)))\n        model.add(Dense(1, activation='linear'))\n    \n    elif model_type == 'transformer':\n        inputs = Input(shape=(input_shape, 1))\n        attention = MultiHeadAttention(num_heads=trial.suggest_int('num_heads', 2, 8), key_dim=trial.suggest_int('key_dim', 16, 64))(inputs, inputs)\n        attention = Add()([inputs, attention])\n        attention = LayerNormalization(epsilon=1e-6)(attention)\n        outputs = Flatten()(attention)\n        outputs = Dense(1, activation='linear')(outputs)\n        model = Model(inputs=inputs, outputs=outputs)\n    \n    elif model_type == 'tabnet':\n        tabnet_model = TabNetRegressor(\n            n_d=trial.suggest_int('n_d', 8, 64),\n            n_a=trial.suggest_int('n_a', 8, 64),\n            n_steps=trial.suggest_int('n_steps', 3, 10),\n            gamma=trial.suggest_float('gamma', 1.0, 2.0),\n            lambda_sparse=trial.suggest_float('lambda_sparse', 1e-6, 1e-3)\n        )\n        tabnet_model.fit(\n            X_train, y_train,\n            eval_set=[(X_val, y_val)],\n            eval_metric=['mae'],\n            max_epochs=100,\n            patience=10,\n            batch_size=256,\n            virtual_batch_size=128\n        )\n        return tabnet_model\n\n    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-2, log=True)\n    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mean_squared_error', metrics=['mean_absolute_error'])\n\n    return model\n\n# Define the objective function for Optuna\ndef objective(trial):\n    model = create_model(trial)\n\n    if isinstance(model, TabNetRegressor):\n        model.fit(\n            X_train, y_train,\n            eval_set=[(X_val, y_val)],\n            eval_metric=['mae'],\n            max_epochs=100,\n            patience=10,\n            batch_size=256,\n            virtual_batch_size=128\n        )\n        y_pred = model.predict(X_val)\n        mae = mean_absolute_error(y_val, y_pred)\n    else:\n        model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10, batch_size=32, verbose=0)\n        _, mae = model.evaluate(X_val, y_val, verbose=0)\n\n    model_type = trial.params['model_type']\n    model_performance[model_type] = mae\n    return mae\n\n# Run the Bayesian optimization\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=10)  # Set n_trials as needed\n\n# Get the best hyperparameters\nbest_params = study.best_params\nprint(f\"Best hyperparameters: {best_params}\")\n\n# Display performance of each model\nprint(\"Model performance (MAE):\")\nfor model_type, mae in model_performance.items():\n    print(f\"{model_type}: {mae}\")\n\n# Plot the model performance\nplt.figure(figsize=(10, 5))\nplt.bar(model_performance.keys(), model_performance.values(), color='skyblue')\nplt.xlabel('Model Type')\nplt.ylabel('Mean Absolute Error')\nplt.title('Model Performance Comparison')\nplt.show()\n",
  "history_output" : "Variables in the .npz file: ['X_train', 'X_test', 'y_train', 'y_test']\n[I 2024-09-27 16:08:28,211] A new study created in memory with name: no-name-be8cb031-51de-46a2-8ae1-b320e23692f8\n[I 2024-09-27 16:08:29,374] Trial 0 finished with value: 0.38547131419181824 and parameters: {'model_type': 'transformer', 'num_heads': 3, 'key_dim': 23, 'learning_rate': 2.940863596640327e-05}. Best is trial 0 with value: 0.38547131419181824.\n[I 2024-09-27 16:08:30,367] Trial 1 finished with value: 0.3632725477218628 and parameters: {'model_type': 'transformer', 'num_heads': 2, 'key_dim': 31, 'learning_rate': 3.171752157165369e-05}. Best is trial 1 with value: 0.3632725477218628.\n[I 2024-09-27 16:08:31,129] Trial 2 finished with value: 0.07857980579137802 and parameters: {'model_type': 'dense', 'num_layers': 3, 'num_units_l0': 58, 'dropout_rate_l0': 0.3764964310356789, 'num_units_l1': 58, 'dropout_rate_l1': 0.3730094748872972, 'num_units_l2': 127, 'dropout_rate_l2': 0.14924642069871458, 'learning_rate': 0.0022444528408875797}. Best is trial 2 with value: 0.07857980579137802.\n[I 2024-09-27 16:08:32,284] Trial 3 finished with value: 0.09976352006196976 and parameters: {'model_type': 'transformer', 'num_heads': 3, 'key_dim': 63, 'learning_rate': 0.002490766714908649}. Best is trial 2 with value: 0.07857980579137802.\n2024-09-27 16:08:32.805304: W tensorflow/core/framework/op_kernel.cc:1840] OP_REQUIRES failed at resource_variable_ops.cc:620 : INVALID_ARGUMENT: Cannot update variable with shape [1] using a Tensor with shape [0], shapes must be equal.\n2024-09-27 16:08:32.805321: I tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: INVALID_ARGUMENT: Cannot update variable with shape [1] using a Tensor with shape [0], shapes must be equal.\n\t [[{{function_node __inference_one_step_on_data_17769}}{{node adam/AssignSubVariableOp_8}}]]\n[W 2024-09-27 16:08:32,829] Trial 4 failed with parameters: {'model_type': 'transformer', 'num_heads': 8, 'key_dim': 32, 'learning_rate': 0.0001523421975464165} because of the following error: InvalidArgumentError().\nTraceback (most recent call last):\n  File \"/opt/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n    value_or_values = func(trial)\n                      ^^^^^^^^^^^\n  File \"/Users/vishesh/gw-workspace/anvbsAOTYweH/auto_ml.py\", line 115, in objective\n    model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10, batch_size=32, verbose=0)\n  File \"/opt/anaconda3/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\", line 122, in error_handler\n    raise e.with_traceback(filtered_tb) from None\n  File \"/opt/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/execute.py\", line 59, in quick_execute\n    except TypeError as e:\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Graph execution error:\nDetected at node adam/AssignSubVariableOp_8 defined at (most recent call last):\n  File \"/Users/vishesh/gw-workspace/anvbsAOTYweH/auto_ml.py\", line 124, in <module>\n  File \"/opt/anaconda3/lib/python3.11/site-packages/optuna/study/study.py\", line 475, in optimize\n  File \"/opt/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 63, in _optimize\n  File \"/opt/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 160, in _optimize_sequential\n  File \"/opt/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n  File \"/Users/vishesh/gw-workspace/anvbsAOTYweH/auto_ml.py\", line 115, in objective\n  File \"/opt/anaconda3/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n  File \"/opt/anaconda3/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py\", line 320, in fit\n  File \"/opt/anaconda3/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py\", line 121, in one_step_on_iterator\n  File \"/opt/anaconda3/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py\", line 108, in one_step_on_data\n  File \"/opt/anaconda3/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py\", line 73, in train_step\n  File \"/opt/anaconda3/lib/python3.11/site-packages/keras/src/optimizers/base_optimizer.py\", line 291, in apply_gradients\n  File \"/opt/anaconda3/lib/python3.11/site-packages/keras/src/optimizers/base_optimizer.py\", line 356, in apply\n  File \"/opt/anaconda3/lib/python3.11/site-packages/keras/src/optimizers/base_optimizer.py\", line 419, in _backend_apply_gradients\n  File \"/opt/anaconda3/lib/python3.11/site-packages/keras/src/backend/tensorflow/optimizer.py\", line 121, in _backend_update_step\n  File \"/opt/anaconda3/lib/python3.11/site-packages/keras/src/backend/tensorflow/optimizer.py\", line 135, in _distributed_tf_update_step\n  File \"/opt/anaconda3/lib/python3.11/site-packages/keras/src/backend/tensorflow/optimizer.py\", line 132, in apply_grad_to_update_var\n  File \"/opt/anaconda3/lib/python3.11/site-packages/keras/src/optimizers/adam.py\", line 145, in update_step\n  File \"/opt/anaconda3/lib/python3.11/site-packages/keras/src/backend/tensorflow/optimizer.py\", line 73, in assign_sub\nCannot update variable with shape [1] using a Tensor with shape [0], shapes must be equal.\n\t [[{{node adam/AssignSubVariableOp_8}}]] [Op:__inference_one_step_on_iterator_17858]\n[W 2024-09-27 16:08:32,830] Trial 4 failed with value None.\nTraceback (most recent call last):\n  File \"/Users/vishesh/gw-workspace/anvbsAOTYweH/auto_ml.py\", line 124, in <module>\n    study.optimize(objective, n_trials=10)  # Set n_trials as needed\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/optuna/study/study.py\", line 475, in optimize\n    _optimize(\n  File \"/opt/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 63, in _optimize\n    _optimize_sequential(\n  File \"/opt/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 160, in _optimize_sequential\n    frozen_trial = _run_trial(study, func, catch)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 248, in _run_trial\n    raise func_err\n  File \"/opt/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n    value_or_values = func(trial)\n                      ^^^^^^^^^^^\n  File \"/Users/vishesh/gw-workspace/anvbsAOTYweH/auto_ml.py\", line 115, in objective\n    model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10, batch_size=32, verbose=0)\n  File \"/opt/anaconda3/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\", line 122, in error_handler\n    raise e.with_traceback(filtered_tb) from None\n  File \"/opt/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/execute.py\", line 53, in quick_execute\n    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\ntensorflow.python.framework.errors_impl.InvalidArgumentError: Graph execution error:\nDetected at node adam/AssignSubVariableOp_8 defined at (most recent call last):\n  File \"/Users/vishesh/gw-workspace/anvbsAOTYweH/auto_ml.py\", line 124, in <module>\n  File \"/opt/anaconda3/lib/python3.11/site-packages/optuna/study/study.py\", line 475, in optimize\n  File \"/opt/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 63, in _optimize\n  File \"/opt/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 160, in _optimize_sequential\n  File \"/opt/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n  File \"/Users/vishesh/gw-workspace/anvbsAOTYweH/auto_ml.py\", line 115, in objective\n  File \"/opt/anaconda3/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n  File \"/opt/anaconda3/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py\", line 320, in fit\n  File \"/opt/anaconda3/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py\", line 121, in one_step_on_iterator\n  File \"/opt/anaconda3/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py\", line 108, in one_step_on_data\n  File \"/opt/anaconda3/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py\", line 73, in train_step\n  File \"/opt/anaconda3/lib/python3.11/site-packages/keras/src/optimizers/base_optimizer.py\", line 291, in apply_gradients\n  File \"/opt/anaconda3/lib/python3.11/site-packages/keras/src/optimizers/base_optimizer.py\", line 356, in apply\n  File \"/opt/anaconda3/lib/python3.11/site-packages/keras/src/optimizers/base_optimizer.py\", line 419, in _backend_apply_gradients\n  File \"/opt/anaconda3/lib/python3.11/site-packages/keras/src/backend/tensorflow/optimizer.py\", line 121, in _backend_update_step\n  File \"/opt/anaconda3/lib/python3.11/site-packages/keras/src/backend/tensorflow/optimizer.py\", line 135, in _distributed_tf_update_step\n  File \"/opt/anaconda3/lib/python3.11/site-packages/keras/src/backend/tensorflow/optimizer.py\", line 132, in apply_grad_to_update_var\n  File \"/opt/anaconda3/lib/python3.11/site-packages/keras/src/optimizers/adam.py\", line 145, in update_step\n  File \"/opt/anaconda3/lib/python3.11/site-packages/keras/src/backend/tensorflow/optimizer.py\", line 73, in assign_sub\nCannot update variable with shape [1] using a Tensor with shape [0], shapes must be equal.\n\t [[{{node adam/AssignSubVariableOp_8}}]] [Op:__inference_one_step_on_iterator_17858]\n",
  "history_begin_time" : 1727467701968,
  "history_end_time" : 1727467713463,
  "history_notes" : null,
  "history_process" : "vnrvoi",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "knXO3RcSNJOY",
  "history_input" : "import numpy as np\nimport optuna\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import Dense, Dropout, Conv1D, MaxPooling1D, Flatten, LSTM, Input, MultiHeadAttention, LayerNormalization, Add\nfrom tensorflow.keras.optimizers import Adam\nfrom pytorch_tabnet.tab_model import TabNetRegressor\n\n# Load preprocessed data\ndata_path = '/Users/vishesh/gw-workspace/L6gDfkNJofh8/preprocessed_data.npz'\ndata = np.load(data_path)\n\n# List all variable names in the .npz file to identify contents\nprint(\"Variables in the .npz file:\", data.files)\n\n# Load data based on contents of the .npz file\nX_train = data['X_train']\ny_train = data['y_train']\n\n# Check if 'X_val' and 'y_val' exist; if not, create validation split\nif 'X_val' in data.files and 'y_val' in data.files:\n    X_val = data['X_val']\n    y_val = data['y_val']\nelse:\n    # Split the data into training and validation sets\n    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n\n# Initialize a dictionary to store model performance\nmodel_performance = {}\n\n# Define the function to create various models\ndef create_model(trial):\n    input_shape = X_train.shape[1]\n    model_type = trial.suggest_categorical('model_type', ['dense', 'cnn', 'lstm', 'transformer', 'tabnet'])\n\n    if model_type == 'dense':\n        model = Sequential()\n        model.add(Input(shape=(input_shape,)))  # Use Input layer to specify input shape\n        num_layers = trial.suggest_int('num_layers', 1, 5)\n        for i in range(num_layers):\n            num_units = trial.suggest_int(f'num_units_l{i}', 16, 128, log=True)\n            model.add(Dense(num_units, activation='relu'))\n            dropout_rate = trial.suggest_float(f'dropout_rate_l{i}', 0.1, 0.5)\n            model.add(Dropout(dropout_rate))\n        model.add(Dense(1, activation='linear'))\n    \n    elif model_type == 'cnn':\n        model = Sequential()\n        model.add(Input(shape=(input_shape, 1)))  # Use Input layer for CNN input shape\n        model.add(Conv1D(filters=trial.suggest_int('filters', 16, 64, log=True),\n                         kernel_size=trial.suggest_int('kernel_size', 3, 5),\n                         activation='relu'))\n        model.add(MaxPooling1D(pool_size=2))\n        model.add(Flatten())\n        model.add(Dense(1, activation='linear'))\n    \n    elif model_type == 'lstm':\n        model = Sequential()\n        model.add(Input(shape=(input_shape, 1)))  # Use Input layer for LSTM input shape\n        model.add(LSTM(units=trial.suggest_int('lstm_units', 16, 64, log=True)))\n        model.add(Dense(1, activation='linear'))\n    \n    elif model_type == 'transformer':\n        inputs = Input(shape=(input_shape, 1))\n        attention = MultiHeadAttention(num_heads=trial.suggest_int('num_heads', 2, 8), key_dim=trial.suggest_int('key_dim', 16, 64))(inputs, inputs)\n        attention = Add()([inputs, attention])\n        attention = LayerNormalization(epsilon=1e-6)(attention)\n        outputs = Flatten()(attention)\n        outputs = Dense(1, activation='linear')(outputs)\n        model = Model(inputs=inputs, outputs=outputs)\n    \n    elif model_type == 'tabnet':\n        tabnet_model = TabNetRegressor(\n            n_d=trial.suggest_int('n_d', 8, 64),\n            n_a=trial.suggest_int('n_a', 8, 64),\n            n_steps=trial.suggest_int('n_steps', 3, 10),\n            gamma=trial.suggest_float('gamma', 1.0, 2.0),\n            lambda_sparse=trial.suggest_float('lambda_sparse', 1e-6, 1e-3)\n        )\n        tabnet_model.fit(\n            X_train, y_train,\n            eval_set=[(X_val, y_val)],\n            eval_metric=['mae'],\n            max_epochs=100,\n            patience=10,\n            batch_size=256,\n            virtual_batch_size=128\n        )\n        return tabnet_model\n\n    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-2, log=True)\n    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mean_squared_error', metrics=['mean_absolute_error'])\n\n    return model\n\n# Define the objective function for Optuna\ndef objective(trial):\n    model = create_model(trial)\n\n    if isinstance(model, TabNetRegressor):\n        model.fit(\n            X_train, y_train,\n            eval_set=[(X_val, y_val)],\n            eval_metric=['mae'],\n            max_epochs=100,\n            patience=10,\n            batch_size=256,\n            virtual_batch_size=128\n        )\n        y_pred = model.predict(X_val)\n        mae = mean_absolute_error(y_val, y_pred)\n    else:\n        model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10, batch_size=32, verbose=0)\n        _, mae = model.evaluate(X_val, y_val, verbose=0)\n\n    model_type = trial.params['model_type']\n    model_performance[model_type] = mae\n    return mae\n\n# Run the Bayesian optimization\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=10)  # Set n_trials as needed\n\n# Get the best hyperparameters\nbest_params = study.best_params\nprint(f\"Best hyperparameters: {best_params}\")\n\n# Display performance of each model\nprint(\"Model performance (MAE):\")\nfor model_type, mae in model_performance.items():\n    print(f\"{model_type}: {mae}\")\n\n# Plot the model performance\nplt.figure(figsize=(10, 5))\nplt.bar(model_performance.keys(), model_performance.values(), color='skyblue')\nplt.xlabel('Model Type')\nplt.ylabel('Mean Absolute Error')\nplt.title('Model Performance Comparison')\nplt.show()\n",
  "history_output" : "Variables in the .npz file: ['X_train', 'X_test', 'y_train', 'y_test']\n[I 2024-09-26 20:59:57,606] A new study created in memory with name: no-name-d8d0a4e9-a933-41ab-9b11-cf7ed1927265\n[I 2024-09-26 20:59:58,278] Trial 0 finished with value: 0.03446311876177788 and parameters: {'model_type': 'cnn', 'filters': 17, 'kernel_size': 5, 'learning_rate': 0.002870764559505644}. Best is trial 0 with value: 0.03446311876177788.\n[I 2024-09-26 20:59:58,977] Trial 1 finished with value: 0.15529881417751312 and parameters: {'model_type': 'dense', 'num_layers': 3, 'num_units_l0': 26, 'dropout_rate_l0': 0.4267717383452116, 'num_units_l1': 17, 'dropout_rate_l1': 0.4869891866594333, 'num_units_l2': 122, 'dropout_rate_l2': 0.34091848351185494, 'learning_rate': 6.145943260032576e-05}. Best is trial 0 with value: 0.03446311876177788.\n[I 2024-09-26 20:59:59,482] Trial 2 finished with value: 0.03561609983444214 and parameters: {'model_type': 'cnn', 'filters': 48, 'kernel_size': 5, 'learning_rate': 0.008454798714136999}. Best is trial 0 with value: 0.03446311876177788.\n[I 2024-09-26 21:00:00,646] Trial 3 finished with value: 0.3913005590438843 and parameters: {'model_type': 'transformer', 'num_heads': 3, 'key_dim': 20, 'learning_rate': 1.5074229250416678e-05}. Best is trial 0 with value: 0.03446311876177788.\n[I 2024-09-26 21:00:01,961] Trial 4 finished with value: 0.35033637285232544 and parameters: {'model_type': 'transformer', 'num_heads': 4, 'key_dim': 16, 'learning_rate': 6.532684414699938e-05}. Best is trial 0 with value: 0.03446311876177788.\n[I 2024-09-26 21:00:02,480] Trial 5 finished with value: 0.07135145366191864 and parameters: {'model_type': 'cnn', 'filters': 42, 'kernel_size': 3, 'learning_rate': 0.0001387778012544223}. Best is trial 0 with value: 0.03446311876177788.\n[I 2024-09-26 21:00:03,522] Trial 6 finished with value: 0.10036443918943405 and parameters: {'model_type': 'transformer', 'num_heads': 3, 'key_dim': 26, 'learning_rate': 0.0030997896013530367}. Best is trial 0 with value: 0.03446311876177788.\n[I 2024-09-26 21:00:04,767] Trial 7 finished with value: 0.10015176981687546 and parameters: {'model_type': 'transformer', 'num_heads': 5, 'key_dim': 58, 'learning_rate': 0.004225795847330142}. Best is trial 0 with value: 0.03446311876177788.\n[I 2024-09-26 21:00:05,921] Trial 8 finished with value: 0.21268607676029205 and parameters: {'model_type': 'lstm', 'lstm_units': 41, 'learning_rate': 1.2541966350631335e-05}. Best is trial 0 with value: 0.03446311876177788.\n[I 2024-09-26 21:00:07,238] Trial 9 finished with value: 0.0996764749288559 and parameters: {'model_type': 'transformer', 'num_heads': 3, 'key_dim': 53, 'learning_rate': 0.0009541678650183223}. Best is trial 0 with value: 0.03446311876177788.\nBest hyperparameters: {'model_type': 'cnn', 'filters': 17, 'kernel_size': 5, 'learning_rate': 0.002870764559505644}\nModel performance (MAE):\ncnn: 0.07135145366191864\ndense: 0.15529881417751312\ntransformer: 0.0996764749288559\nlstm: 0.21268607676029205\n2024-09-26 21:00:07.855 python[21947:5247336] +[IMKClient subclass]: chose IMKClient_Legacy\n2024-09-26 21:00:07.855 python[21947:5247336] +[IMKInputSession subclass]: chose IMKInputSession_Legacy\n",
  "history_begin_time" : 1727398792733,
  "history_end_time" : 1727398824197,
  "history_notes" : null,
  "history_process" : "vnrvoi",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "WKOM5ZWC9q0V",
  "history_input" : "import numpy as np\nimport optuna\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import Dense, Dropout, Conv1D, MaxPooling1D, Flatten, LSTM, Input, MultiHeadAttention, LayerNormalization, Add\nfrom tensorflow.keras.optimizers import Adam\nfrom pytorch_tabnet.tab_model import TabNetRegressor\n\n# Load preprocessed data\ndata_path = '/Users/vishesh/gw-workspace/L6gDfkNJofh8/preprocessed_data.npz'\ndata = np.load(data_path)\n\n# List all variable names in the .npz file to identify contents\nprint(\"Variables in the .npz file:\", data.files)\n\n# Load data based on contents of the .npz file\nX_train = data['X_train']\ny_train = data['y_train']\n\n# Check if 'X_val' and 'y_val' exist; if not, create validation split\nif 'X_val' in data.files and 'y_val' in data.files:\n    X_val = data['X_val']\n    y_val = data['y_val']\nelse:\n    # Split the data into training and validation sets\n    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n\n# Initialize a dictionary to store model performance\nmodel_performance = {}\n\n# Define the function to create various models\ndef create_model(trial):\n    input_shape = X_train.shape[1]\n    model_type = trial.suggest_categorical('model_type', ['dense', 'cnn', 'lstm', 'transformer', 'tabnet'])\n\n    if model_type == 'dense':\n        model = Sequential()\n        model.add(Input(shape=(input_shape,)))  # Use Input layer to specify input shape\n        num_layers = trial.suggest_int('num_layers', 1, 5)\n        for i in range(num_layers):\n            num_units = trial.suggest_int(f'num_units_l{i}', 16, 128, log=True)\n            model.add(Dense(num_units, activation='relu'))\n            dropout_rate = trial.suggest_float(f'dropout_rate_l{i}', 0.1, 0.5)\n            model.add(Dropout(dropout_rate))\n        model.add(Dense(1, activation='linear'))\n    \n    elif model_type == 'cnn':\n        model = Sequential()\n        model.add(Input(shape=(input_shape, 1)))  # Use Input layer for CNN input shape\n        model.add(Conv1D(filters=trial.suggest_int('filters', 16, 64, log=True),\n                         kernel_size=trial.suggest_int('kernel_size', 3, 5),\n                         activation='relu'))\n        model.add(MaxPooling1D(pool_size=2))\n        model.add(Flatten())\n        model.add(Dense(1, activation='linear'))\n    \n    elif model_type == 'lstm':\n        model = Sequential()\n        model.add(Input(shape=(input_shape, 1)))  # Use Input layer for LSTM input shape\n        model.add(LSTM(units=trial.suggest_int('lstm_units', 16, 64, log=True)))\n        model.add(Dense(1, activation='linear'))\n    \n    elif model_type == 'transformer':\n        inputs = Input(shape=(input_shape, 1))\n        attention = MultiHeadAttention(num_heads=trial.suggest_int('num_heads', 2, 8), key_dim=trial.suggest_int('key_dim', 16, 64))(inputs, inputs)\n        attention = Add()([inputs, attention])\n        attention = LayerNormalization(epsilon=1e-6)(attention)\n        outputs = Flatten()(attention)\n        outputs = Dense(1, activation='linear')(outputs)\n        model = Model(inputs=inputs, outputs=outputs)\n    \n    elif model_type == 'tabnet':\n        tabnet_model = TabNetRegressor(\n            n_d=trial.suggest_int('n_d', 8, 64),\n            n_a=trial.suggest_int('n_a', 8, 64),\n            n_steps=trial.suggest_int('n_steps', 3, 10),\n            gamma=trial.suggest_float('gamma', 1.0, 2.0),\n            lambda_sparse=trial.suggest_float('lambda_sparse', 1e-6, 1e-3)\n        )\n        tabnet_model.fit(\n            X_train, y_train,\n            eval_set=[(X_val, y_val)],\n            eval_metric=['mae'],\n            max_epochs=100,\n            patience=10,\n            batch_size=256,\n            virtual_batch_size=128,\n            verbose=0\n        )\n        return tabnet_model\n\n    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-2, log=True)\n    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mean_squared_error', metrics=['mean_absolute_error'])\n\n    return model\n\n# Define the objective function for Optuna\ndef objective(trial):\n    model = create_model(trial)\n\n    if isinstance(model, TabNetRegressor):\n        model.fit(\n            X_train, y_train,\n            eval_set=[(X_val, y_val)],\n            eval_metric=['mae'],\n            max_epochs=100,\n            patience=10,\n            batch_size=256,\n            virtual_batch_size=128,\n            verbose=0\n        )\n        y_pred = model.predict(X_val)\n        mae = mean_absolute_error(y_val, y_pred)\n    else:\n        model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10, batch_size=32, verbose=0)\n        _, mae = model.evaluate(X_val, y_val, verbose=0)\n\n    model_type = trial.params['model_type']\n    model_performance[model_type] = mae\n    return mae\n\n# Run the Bayesian optimization\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=10)  # Set n_trials as needed\n\n# Get the best hyperparameters\nbest_params = study.best_params\nprint(f\"Best hyperparameters: {best_params}\")\n\n# Display performance of each model\nprint(\"Model performance (MAE):\")\nfor model_type, mae in model_performance.items():\n    print(f\"{model_type}: {mae}\")\n\n# Plot the model performance\nplt.figure(figsize=(10, 5))\nplt.bar(model_performance.keys(), model_performance.values(), color='skyblue')\nplt.xlabel('Model Type')\nplt.ylabel('Mean Absolute Error')\nplt.title('Model Performance Comparison')\nplt.show()\n",
  "history_output" : "Variables in the .npz file: ['X_train', 'X_test', 'y_train', 'y_test']\n[I 2024-09-26 20:57:30,778] A new study created in memory with name: no-name-b21ad3c2-8d97-4268-8683-0698818ebcf6\n[I 2024-09-26 20:57:31,997] Trial 0 finished with value: 0.3613779842853546 and parameters: {'model_type': 'transformer', 'num_heads': 4, 'key_dim': 30, 'learning_rate': 3.780818946070929e-05}. Best is trial 0 with value: 0.3613779842853546.\n/opt/anaconda3/lib/python3.11/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n  warnings.warn(f\"Device used : {self.device}\")\n[W 2024-09-26 20:57:32,001] Trial 1 failed with parameters: {'model_type': 'tabnet', 'n_d': 30, 'n_a': 20, 'n_steps': 6, 'gamma': 1.5524732697544916, 'lambda_sparse': 0.0006458275075846734} because of the following error: TypeError(\"TabModel.fit() got an unexpected keyword argument 'verbose'\").\nTraceback (most recent call last):\n  File \"/opt/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n    value_or_values = func(trial)\n                      ^^^^^^^^^^^\n  File \"/Users/vishesh/gw-workspace/WKOM5ZWC9q0V/auto_ml.py\", line 101, in objective\n    model = create_model(trial)\n            ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/vishesh/gw-workspace/WKOM5ZWC9q0V/auto_ml.py\", line 82, in create_model\n    tabnet_model.fit(\nTypeError: TabModel.fit() got an unexpected keyword argument 'verbose'\n[W 2024-09-26 20:57:32,001] Trial 1 failed with value None.\nTraceback (most recent call last):\n  File \"/Users/vishesh/gw-workspace/WKOM5ZWC9q0V/auto_ml.py\", line 126, in <module>\n    study.optimize(objective, n_trials=10)  # Set n_trials as needed\n    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/optuna/study/study.py\", line 475, in optimize\n    _optimize(\n  File \"/opt/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 63, in _optimize\n    _optimize_sequential(\n  File \"/opt/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 160, in _optimize_sequential\n    frozen_trial = _run_trial(study, func, catch)\n                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 248, in _run_trial\n    raise func_err\n  File \"/opt/anaconda3/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n    value_or_values = func(trial)\n                      ^^^^^^^^^^^\n  File \"/Users/vishesh/gw-workspace/WKOM5ZWC9q0V/auto_ml.py\", line 101, in objective\n    model = create_model(trial)\n            ^^^^^^^^^^^^^^^^^^^\n  File \"/Users/vishesh/gw-workspace/WKOM5ZWC9q0V/auto_ml.py\", line 82, in create_model\n    tabnet_model.fit(\nTypeError: TabModel.fit() got an unexpected keyword argument 'verbose'\n",
  "history_begin_time" : 1727398645372,
  "history_end_time" : 1727398652690,
  "history_notes" : null,
  "history_process" : "vnrvoi",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "ib3ZigRV4jeF",
  "history_input" : "import numpy as np\nimport optuna\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import Dense, Dropout, Conv1D, MaxPooling1D, Flatten, LSTM, Input, MultiHeadAttention, LayerNormalization, Add\nfrom tensorflow.keras.optimizers import Adam\nfrom pytorch_tabnet.tab_model import TabNetRegressor\n\n# Load preprocessed data\ndata_path = '/Users/vishesh/gw-workspace/L6gDfkNJofh8/preprocessed_data.npz'\ndata = np.load(data_path)\n\n# List all variable names in the .npz file to identify contents\nprint(\"Variables in the .npz file:\", data.files)\n\n# Load data based on contents of the .npz file\nX_train = data['X_train']\ny_train = data['y_train']\n\n# Check if 'X_val' and 'y_val' exist; if not, create validation split\nif 'X_val' in data.files and 'y_val' in data.files:\n    X_val = data['X_val']\n    y_val = data['y_val']\nelse:\n    # Split the data into training and validation sets\n    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n\n# Initialize a dictionary to store model performance\nmodel_performance = {}\n\n# Define the function to create various models\ndef create_model(trial):\n    input_shape = X_train.shape[1]\n    model_type = trial.suggest_categorical('model_type', ['dense', 'cnn', 'lstm', 'transformer', 'tabnet'])\n\n    if model_type == 'dense':\n        model = Sequential()\n        num_layers = trial.suggest_int('num_layers', 1, 5)\n        for i in range(num_layers):\n            num_units = trial.suggest_int(f'num_units_l{i}', 16, 128, log=True)\n            if i == 0:\n                model.add(Dense(num_units, activation='relu', input_shape=(input_shape,)))\n            else:\n                model.add(Dense(num_units, activation='relu'))\n            dropout_rate = trial.suggest_float(f'dropout_rate_l{i}', 0.1, 0.5)\n            model.add(Dropout(dropout_rate))\n        model.add(Dense(1, activation='linear'))\n    \n    elif model_type == 'cnn':\n        model = Sequential()\n        model.add(Conv1D(filters=trial.suggest_int('filters', 16, 64, log=True),\n                         kernel_size=trial.suggest_int('kernel_size', 3, 5),\n                         activation='relu',\n                         input_shape=(input_shape, 1)))\n        model.add(MaxPooling1D(pool_size=2))\n        model.add(Flatten())\n        model.add(Dense(1, activation='linear'))\n    \n    elif model_type == 'lstm':\n        model = Sequential()\n        model.add(LSTM(units=trial.suggest_int('lstm_units', 16, 64, log=True),\n                       input_shape=(input_shape, 1)))\n        model.add(Dense(1, activation='linear'))\n    \n    elif model_type == 'transformer':\n        inputs = Input(shape=(input_shape, 1))\n        attention = MultiHeadAttention(num_heads=trial.suggest_int('num_heads', 2, 8), key_dim=trial.suggest_int('key_dim', 16, 64))(inputs, inputs)\n        attention = Add()([inputs, attention])\n        attention = LayerNormalization(epsilon=1e-6)(attention)\n        outputs = Flatten()(attention)\n        outputs = Dense(1, activation='linear')(outputs)\n        model = Model(inputs=inputs, outputs=outputs)\n    \n    elif model_type == 'tabnet':\n        tabnet_model = TabNetRegressor(\n            n_d=trial.suggest_int('n_d', 8, 64),\n            n_a=trial.suggest_int('n_a', 8, 64),\n            n_steps=trial.suggest_int('n_steps', 3, 10),\n            gamma=trial.suggest_float('gamma', 1.0, 2.0),\n            lambda_sparse=trial.suggest_float('lambda_sparse', 1e-6, 1e-3)\n        )\n        tabnet_model.fit(\n            X_train, y_train,\n            eval_set=[(X_val, y_val)],\n            eval_metric=['mae'],\n            max_epochs=100,\n            patience=10,\n            batch_size=256,\n            virtual_batch_size=128,\n            verbose=0\n        )\n        return tabnet_model\n\n    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-2, log=True)\n    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mean_squared_error', metrics=['mean_absolute_error'])\n\n    return model\n\n# Define the objective function for Optuna\ndef objective(trial):\n    model = create_model(trial)\n\n    if isinstance(model, TabNetRegressor):\n        model.fit(\n            X_train, y_train,\n            eval_set=[(X_val, y_val)],\n            eval_metric=['mae'],\n            max_epochs=100,\n            patience=10,\n            batch_size=256,\n            virtual_batch_size=128,\n            verbose=0\n        )\n        y_pred = model.predict(X_val)\n        mae = mean_absolute_error(y_val, y_pred)\n    else:\n        model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10, batch_size=32, verbose=0)\n        _, mae = model.evaluate(X_val, y_val, verbose=0)\n\n    model_type = trial.params['model_type']\n    model_performance[model_type] = mae\n    return mae\n\n# Run the Bayesian optimization\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=10)  # Set n_trials as needed\n\n# Get the best hyperparameters\nbest_params = study.best_params\nprint(f\"Best hyperparameters: {best_params}\")\n\n# Display performance of each model\nprint(\"Model performance (MAE):\")\nfor model_type, mae in model_performance.items():\n    print(f\"{model_type}: {mae}\")\n\n# Plot the model performance\nplt.figure(figsize=(10, 5))\nplt.bar(model_performance.keys(), model_performance.values(), color='skyblue')\nplt.xlabel('Model Type')\nplt.ylabel('Mean Absolute Error')\nplt.title('Model Performance Comparison')\nplt.show()\n",
  "history_output" : "Variables in the .npz file: ['X_train', 'X_test', 'y_train', 'y_test']\n[I 2024-09-26 20:54:31,814] A new study created in memory with name: no-name-c33d1400-656b-4784-b949-fcfa219fa282\n/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n[I 2024-09-26 20:54:32,440] Trial 0 finished with value: 0.0582464225590229 and parameters: {'model_type': 'cnn', 'filters': 28, 'kernel_size': 4, 'learning_rate': 0.0011205771598866447}. Best is trial 0 with value: 0.0582464225590229.\n[I 2024-09-26 20:54:33,562] Trial 1 finished with value: 0.10115814208984375 and parameters: {'model_type': 'transformer', 'num_heads': 8, 'key_dim': 16, 'learning_rate': 0.00723293610510261}. Best is trial 0 with value: 0.0582464225590229.\n/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n[I 2024-09-26 20:54:34,252] Trial 2 finished with value: 0.18992897868156433 and parameters: {'model_type': 'dense', 'num_layers': 3, 'num_units_l0': 32, 'dropout_rate_l0': 0.38937973578904733, 'num_units_l1': 59, 'dropout_rate_l1': 0.3313620163331966, 'num_units_l2': 121, 'dropout_rate_l2': 0.39415851855318873, 'learning_rate': 1.1445128415116676e-05}. Best is trial 0 with value: 0.0582464225590229.\n[I 2024-09-26 20:54:34,855] Trial 3 finished with value: 0.048518091440200806 and parameters: {'model_type': 'cnn', 'filters': 64, 'kernel_size': 4, 'learning_rate': 0.004315381028482892}. Best is trial 3 with value: 0.048518091440200806.\n[I 2024-09-26 20:54:35,349] Trial 4 finished with value: 0.15052244067192078 and parameters: {'model_type': 'cnn', 'filters': 25, 'kernel_size': 3, 'learning_rate': 6.762909525889676e-05}. Best is trial 3 with value: 0.048518091440200806.\n[I 2024-09-26 20:54:36,452] Trial 5 finished with value: 0.09974905848503113 and parameters: {'model_type': 'transformer', 'num_heads': 7, 'key_dim': 20, 'learning_rate': 0.0022314839600297994}. Best is trial 3 with value: 0.048518091440200806.\n[I 2024-09-26 20:54:37,503] Trial 6 finished with value: 0.09966925531625748 and parameters: {'model_type': 'transformer', 'num_heads': 3, 'key_dim': 45, 'learning_rate': 0.0017807779311195195}. Best is trial 3 with value: 0.048518091440200806.\n[I 2024-09-26 20:54:38,592] Trial 7 finished with value: 0.09966925531625748 and parameters: {'model_type': 'transformer', 'num_heads': 3, 'key_dim': 30, 'learning_rate': 0.0006072333434777797}. Best is trial 3 with value: 0.048518091440200806.\n[I 2024-09-26 20:54:39,162] Trial 8 finished with value: 0.08314809948205948 and parameters: {'model_type': 'dense', 'num_layers': 2, 'num_units_l0': 59, 'dropout_rate_l0': 0.20398356508816398, 'num_units_l1': 38, 'dropout_rate_l1': 0.2962664016318603, 'learning_rate': 4.6681330281646985e-05}. Best is trial 3 with value: 0.048518091440200806.\n[I 2024-09-26 20:54:40,171] Trial 9 finished with value: 0.13539573550224304 and parameters: {'model_type': 'dense', 'num_layers': 5, 'num_units_l0': 30, 'dropout_rate_l0': 0.3712635233048482, 'num_units_l1': 42, 'dropout_rate_l1': 0.29073573143691117, 'num_units_l2': 93, 'dropout_rate_l2': 0.12819471392904536, 'num_units_l3': 76, 'dropout_rate_l3': 0.3217464723680501, 'num_units_l4': 17, 'dropout_rate_l4': 0.4327404120013775, 'learning_rate': 0.0002715777837133682}. Best is trial 3 with value: 0.048518091440200806.\nBest hyperparameters: {'model_type': 'cnn', 'filters': 64, 'kernel_size': 4, 'learning_rate': 0.004315381028482892}\nModel performance (MAE):\ncnn: 0.15052244067192078\ntransformer: 0.09966925531625748\ndense: 0.13539573550224304\n2024-09-26 20:54:40.944 python[21716:5235207] +[IMKClient subclass]: chose IMKClient_Legacy\n2024-09-26 20:54:40.945 python[21716:5235207] +[IMKInputSession subclass]: chose IMKInputSession_Legacy\n",
  "history_begin_time" : 1727398467037,
  "history_end_time" : 1727398512900,
  "history_notes" : null,
  "history_process" : "vnrvoi",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "UcB1IrrWbpKA",
  "history_input" : "import numpy as np\nimport optuna\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import mean_absolute_error\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import Dense, Dropout, Conv1D, MaxPooling1D, Flatten, LSTM, Input, MultiHeadAttention, LayerNormalization, Add\nfrom tensorflow.keras.optimizers import Adam\nfrom pytorch_tabnet.tab_model import TabNetRegressor\n\n# Load preprocessed data\ndata_path = '/Users/vishesh/gw-workspace/L6gDfkNJofh8/preprocessed_data.npz'\ndata = np.load(data_path)\nX_train, X_val, y_train, y_val = data['X_train'], data['X_val'], data['y_train'], data['y_val']\n\n# Initialize a dictionary to store model performance\nmodel_performance = {}\n\n# Define the function to create various models\ndef create_model(trial):\n    input_shape = X_train.shape[1]\n    model_type = trial.suggest_categorical('model_type', ['dense', 'cnn', 'lstm', 'transformer', 'tabnet'])\n\n    if model_type == 'dense':\n        model = Sequential()\n        num_layers = trial.suggest_int('num_layers', 1, 5)\n        for i in range(num_layers):\n            num_units = trial.suggest_int(f'num_units_l{i}', 16, 128, log=True)\n            if i == 0:\n                model.add(Dense(num_units, activation='relu', input_shape=(input_shape,)))\n            else:\n                model.add(Dense(num_units, activation='relu'))\n            dropout_rate = trial.suggest_float(f'dropout_rate_l{i}', 0.1, 0.5)\n            model.add(Dropout(dropout_rate))\n        model.add(Dense(1, activation='linear'))\n    \n    elif model_type == 'cnn':\n        model = Sequential()\n        model.add(Conv1D(filters=trial.suggest_int('filters', 16, 64, log=True),\n                         kernel_size=trial.suggest_int('kernel_size', 3, 5),\n                         activation='relu',\n                         input_shape=(input_shape, 1)))\n        model.add(MaxPooling1D(pool_size=2))\n        model.add(Flatten())\n        model.add(Dense(1, activation='linear'))\n    \n    elif model_type == 'lstm':\n        model = Sequential()\n        model.add(LSTM(units=trial.suggest_int('lstm_units', 16, 64, log=True),\n                       input_shape=(input_shape, 1)))\n        model.add(Dense(1, activation='linear'))\n    \n    elif model_type == 'transformer':\n        inputs = Input(shape=(input_shape, 1))\n        attention = MultiHeadAttention(num_heads=trial.suggest_int('num_heads', 2, 8), key_dim=trial.suggest_int('key_dim', 16, 64))(inputs, inputs)\n        attention = Add()([inputs, attention])\n        attention = LayerNormalization(epsilon=1e-6)(attention)\n        outputs = Flatten()(attention)\n        outputs = Dense(1, activation='linear')(outputs)\n        model = Model(inputs=inputs, outputs=outputs)\n    \n    elif model_type == 'tabnet':\n        tabnet_model = TabNetRegressor(\n            n_d=trial.suggest_int('n_d', 8, 64),\n            n_a=trial.suggest_int('n_a', 8, 64),\n            n_steps=trial.suggest_int('n_steps', 3, 10),\n            gamma=trial.suggest_float('gamma', 1.0, 2.0),\n            lambda_sparse=trial.suggest_float('lambda_sparse', 1e-6, 1e-3)\n        )\n        tabnet_model.fit(\n            X_train, y_train,\n            eval_set=[(X_val, y_val)],\n            eval_metric=['mae'],\n            max_epochs=100,\n            patience=10,\n            batch_size=256,\n            virtual_batch_size=128,\n            verbose=0\n        )\n        return tabnet_model\n\n    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-2, log=True)\n    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mean_squared_error', metrics=['mean_absolute_error'])\n\n    return model\n\n# Define the objective function for Optuna\ndef objective(trial):\n    model = create_model(trial)\n\n    if isinstance(model, TabNetRegressor):\n        model.fit(\n            X_train, y_train,\n            eval_set=[(X_val, y_val)],\n            eval_metric=['mae'],\n            max_epochs=100,\n            patience=10,\n            batch_size=256,\n            virtual_batch_size=128,\n            verbose=0\n        )\n        y_pred = model.predict(X_val)\n        mae = mean_absolute_error(y_val, y_pred)\n    else:\n        model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10, batch_size=32, verbose=0)\n        _, mae = model.evaluate(X_val, y_val, verbose=0)\n\n    model_type = trial.params['model_type']\n    model_performance[model_type] = mae\n    return mae\n\n# Run the Bayesian optimization\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=10)  # Set n_trials as needed\n\n# Get the best hyperparameters\nbest_params = study.best_params\nprint(f\"Best hyperparameters: {best_params}\")\n\n# Display performance of each model\nprint(\"Model performance (MAE):\")\nfor model_type, mae in model_performance.items():\n    print(f\"{model_type}: {mae}\")\n\n# Plot the model performance\nplt.figure(figsize=(10, 5))\nplt.bar(model_performance.keys(), model_performance.values(), color='skyblue')\nplt.xlabel('Model Type')\nplt.ylabel('Mean Absolute Error')\nplt.title('Model Performance Comparison')\nplt.show()\n",
  "history_output" : "Traceback (most recent call last):\n  File \"/Users/vishesh/gw-workspace/UcB1IrrWbpKA/auto_ml.py\", line 13, in <module>\n    X_train, X_val, y_train, y_val = data['X_train'], data['X_val'], data['y_train'], data['y_val']\n                                                      ~~~~^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.11/site-packages/numpy/lib/npyio.py\", line 263, in __getitem__\n    raise KeyError(f\"{key} is not a file in the archive\")\nKeyError: 'X_val is not a file in the archive'\n",
  "history_begin_time" : 1727398353995,
  "history_end_time" : 1727398362030,
  "history_notes" : null,
  "history_process" : "vnrvoi",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "vpqCon0z73Uo",
  "history_input" : "import numpy as np\nimport optuna\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import mean_absolute_error\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import Dense, Dropout, Conv1D, MaxPooling1D, Flatten, LSTM, Input, MultiHeadAttention, LayerNormalization, Add\nfrom tensorflow.keras.optimizers import Adam\nfrom pytorch_tabnet.tab_model import TabNetRegressor\n\n# Load preprocessed data\ndata_path = '/Users/vishesh/gw-workspace/L6gDfkNJofh8/preprocessed_data.npz'\ndata = np.load(data_path)\nX_train, X_val, y_train, y_val = data['X_train'], data['X_val'], data['y_train'], data['y_val']\n\n# Initialize a dictionary to store model performance\nmodel_performance = {}\n\n# Define the function to create various models\ndef create_model(trial):\n    input_shape = X_train.shape[1]\n    model_type = trial.suggest_categorical('model_type', ['dense', 'cnn', 'lstm', 'transformer', 'tabnet'])\n\n    if model_type == 'dense':\n        model = Sequential()\n        num_layers = trial.suggest_int('num_layers', 1, 5)\n        for i in range(num_layers):\n            num_units = trial.suggest_int(f'num_units_l{i}', 16, 128, log=True)\n            if i == 0:\n                model.add(Dense(num_units, activation='relu', input_shape=(input_shape,)))\n            else:\n                model.add(Dense(num_units, activation='relu'))\n            dropout_rate = trial.suggest_float(f'dropout_rate_l{i}', 0.1, 0.5)\n            model.add(Dropout(dropout_rate))\n        model.add(Dense(1, activation='linear'))\n    \n    elif model_type == 'cnn':\n        model = Sequential()\n        model.add(Conv1D(filters=trial.suggest_int('filters', 16, 64, log=True),\n                         kernel_size=trial.suggest_int('kernel_size', 3, 5),\n                         activation='relu',\n                         input_shape=(input_shape, 1)))\n        model.add(MaxPooling1D(pool_size=2))\n        model.add(Flatten())\n        model.add(Dense(1, activation='linear'))\n    \n    elif model_type == 'lstm':\n        model = Sequential()\n        model.add(LSTM(units=trial.suggest_int('lstm_units', 16, 64, log=True),\n                       input_shape=(input_shape, 1)))\n        model.add(Dense(1, activation='linear'))\n    \n    elif model_type == 'transformer':\n        inputs = Input(shape=(input_shape, 1))\n        attention = MultiHeadAttention(num_heads=trial.suggest_int('num_heads', 2, 8), key_dim=trial.suggest_int('key_dim', 16, 64))(inputs, inputs)\n        attention = Add()([inputs, attention])\n        attention = LayerNormalization(epsilon=1e-6)(attention)\n        outputs = Flatten()(attention)\n        outputs = Dense(1, activation='linear')(outputs)\n        model = Model(inputs=inputs, outputs=outputs)\n    \n    elif model_type == 'tabnet':\n        tabnet_model = TabNetRegressor(\n            n_d=trial.suggest_int('n_d', 8, 64),\n            n_a=trial.suggest_int('n_a', 8, 64),\n            n_steps=trial.suggest_int('n_steps', 3, 10),\n            gamma=trial.suggest_float('gamma', 1.0, 2.0),\n            lambda_sparse=trial.suggest_float('lambda_sparse', 1e-6, 1e-3)\n        )\n        tabnet_model.fit(\n            X_train, y_train,\n            eval_set=[(X_val, y_val)],\n            eval_metric=['mae'],\n            max_epochs=100,\n            patience=10,\n            batch_size=256,\n            virtual_batch_size=128,\n            verbose=0\n        )\n        return tabnet_model\n\n    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-2, log=True)\n    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mean_squared_error', metrics=['mean_absolute_error'])\n\n    return model\n\n# Define the objective function for Optuna\ndef objective(trial):\n    model = create_model(trial)\n\n    if isinstance(model, TabNetRegressor):\n        model.fit(\n            X_train, y_train,\n            eval_set=[(X_val, y_val)],\n            eval_metric=['mae'],\n            max_epochs=100,\n            patience=10,\n            batch_size=256,\n            virtual_batch_size=128,\n            verbose=0\n        )\n        y_pred = model.predict(X_val)\n        mae = mean_absolute_error(y_val, y_pred)\n    else:\n        model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10, batch_size=32, verbose=0)\n        _, mae = model.evaluate(X_val, y_val, verbose=0)\n\n    model_type = trial.params['model_type']\n    model_performance[model_type] = mae\n    return mae\n\n# Run the Bayesian optimization\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=10)  # Set n_trials as needed\n\n# Get the best hyperparameters\nbest_params = study.best_params\nprint(f\"Best hyperparameters: {best_params}\")\n\n# Display performance of each model\nprint(\"Model performance (MAE):\")\nfor model_type, mae in model_performance.items():\n    print(f\"{model_type}: {mae}\")\n\n# Plot the model performance\nplt.figure(figsize=(10, 5))\nplt.bar(model_performance.keys(), model_performance.values(), color='skyblue')\nplt.xlabel('Model Type')\nplt.ylabel('Mean Absolute Error')\nplt.title('Model Performance Comparison')\nplt.show()\n",
  "history_output" : "Traceback (most recent call last):\n  File \"/Users/vishesh/gw-workspace/vpqCon0z73Uo/auto_ml.py\", line 8, in <module>\n    from pytorch_tabnet.tab_model import TabNetRegressor\nModuleNotFoundError: No module named 'pytorch_tabnet'\n",
  "history_begin_time" : 1727398289468,
  "history_end_time" : 1727398308192,
  "history_notes" : null,
  "history_process" : "vnrvoi",
  "host_id" : null,
  "indicator" : "Failed"
},{
  "history_id" : "PW8GYESpFE8V",
  "history_input" : "import numpy as np\nimport optuna\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import mean_absolute_error\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import Dense, Dropout, Conv1D, MaxPooling1D, Flatten, LSTM, Input, MultiHeadAttention, LayerNormalization, Add\nfrom tensorflow.keras.optimizers import Adam\nfrom pytorch_tabnet.tab_model import TabNetRegressor\n\n# Load preprocessed data\ndata_path = '/Users/vishesh/gw-workspace/L6gDfkNJofh8/preprocessed_data.npz'\ndata = np.load(data_path)\nX_train, X_val, y_train, y_val = data['X_train'], data['X_val'], data['y_train'], data['y_val']\n\n# Initialize a dictionary to store model performance\nmodel_performance = {}\n\n# Define the function to create various models\ndef create_model(trial):\n    input_shape = X_train.shape[1]\n    model_type = trial.suggest_categorical('model_type', ['dense', 'cnn', 'lstm', 'transformer', 'tabnet'])\n\n    if model_type == 'dense':\n        model = Sequential()\n        num_layers = trial.suggest_int('num_layers', 1, 5)\n        for i in range(num_layers):\n            num_units = trial.suggest_int(f'num_units_l{i}', 16, 128, log=True)\n            if i == 0:\n                model.add(Dense(num_units, activation='relu', input_shape=(input_shape,)))\n            else:\n                model.add(Dense(num_units, activation='relu'))\n            dropout_rate = trial.suggest_float(f'dropout_rate_l{i}', 0.1, 0.5)\n            model.add(Dropout(dropout_rate))\n        model.add(Dense(1, activation='linear'))\n    \n    elif model_type == 'cnn':\n        model = Sequential()\n        model.add(Conv1D(filters=trial.suggest_int('filters', 16, 64, log=True),\n                         kernel_size=trial.suggest_int('kernel_size', 3, 5),\n                         activation='relu',\n                         input_shape=(input_shape, 1)))\n        model.add(MaxPooling1D(pool_size=2))\n        model.add(Flatten())\n        model.add(Dense(1, activation='linear'))\n    \n    elif model_type == 'lstm':\n        model = Sequential()\n        model.add(LSTM(units=trial.suggest_int('lstm_units', 16, 64, log=True),\n                       input_shape=(input_shape, 1)))\n        model.add(Dense(1, activation='linear'))\n    \n    elif model_type == 'transformer':\n        inputs = Input(shape=(input_shape, 1))\n        attention = MultiHeadAttention(num_heads=trial.suggest_int('num_heads', 2, 8), key_dim=trial.suggest_int('key_dim', 16, 64))(inputs, inputs)\n        attention = Add()([inputs, attention])\n        attention = LayerNormalization(epsilon=1e-6)(attention)\n        outputs = Flatten()(attention)\n        outputs = Dense(1, activation='linear')(outputs)\n        model = Model(inputs=inputs, outputs=outputs)\n    \n    elif model_type == 'tabnet':\n        tabnet_model = TabNetRegressor(\n            n_d=trial.suggest_int('n_d', 8, 64),\n            n_a=trial.suggest_int('n_a', 8, 64),\n            n_steps=trial.suggest_int('n_steps', 3, 10),\n            gamma=trial.suggest_float('gamma', 1.0, 2.0),\n            lambda_sparse=trial.suggest_float('lambda_sparse', 1e-6, 1e-3)\n        )\n        tabnet_model.fit(\n            X_train, y_train,\n            eval_set=[(X_val, y_val)],\n            eval_metric=['mae'],\n            max_epochs=100,\n            patience=10,\n            batch_size=256,\n            virtual_batch_size=128,\n            verbose=0\n        )\n        return tabnet_model\n\n    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-2, log=True)\n    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mean_squared_error', metrics=['mean_absolute_error'])\n\n    return model\n\n# Define the objective function for Optuna\ndef objective(trial):\n    model = create_model(trial)\n\n    if isinstance(model, TabNetRegressor):\n        model.fit(\n            X_train, y_train,\n            eval_set=[(X_val, y_val)],\n            eval_metric=['mae'],\n            max_epochs=100,\n            patience=10,\n            batch_size=256,\n            virtual_batch_size=128,\n            verbose=0\n        )\n        y_pred = model.predict(X_val)\n        mae = mean_absolute_error(y_val, y_pred)\n    else:\n        model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10, batch_size=32, verbose=0)\n        _, mae = model.evaluate(X_val, y_val, verbose=0)\n\n    model_type = trial.params['model_type']\n    model_performance[model_type] = mae\n    return mae\n\n# Run the Bayesian optimization\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=10)  # Set n_trials as needed\n\n# Get the best hyperparameters\nbest_params = study.best_params\nprint(f\"Best hyperparameters: {best_params}\")\n\n# Display performance of each model\nprint(\"Model performance (MAE):\")\nfor model_type, mae in model_performance.items():\n    print(f\"{model_type}: {mae}\")\n\n# Plot the model performance\nplt.figure(figsize=(10, 5))\nplt.bar(model_performance.keys(), model_performance.values(), color='skyblue')\nplt.xlabel('Model Type')\nplt.ylabel('Mean Absolute Error')\nplt.title('Model Performance Comparison')\nplt.show()\n",
  "history_output" : "Traceback (most recent call last):\n  File \"/Users/vishesh/gw-workspace/PW8GYESpFE8V/auto_ml.py\", line 2, in <module>\n    import optuna\nModuleNotFoundError: No module named 'optuna'\n",
  "history_begin_time" : 1727398256312,
  "history_end_time" : 1727398257568,
  "history_notes" : null,
  "history_process" : "vnrvoi",
  "host_id" : null,
  "indicator" : "Failed"
},]
