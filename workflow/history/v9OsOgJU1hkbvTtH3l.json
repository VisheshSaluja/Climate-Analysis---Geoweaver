[{
  "history_id" : "wzw5jhg7xmw",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1727468502534,
  "history_end_time" : 1727468502534,
  "history_notes" : null,
  "history_process" : "9cqdos",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "7rcducrr9sh",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1727468502536,
  "history_end_time" : 1727468502536,
  "history_notes" : null,
  "history_process" : "iq85px",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "qczz74kdruc",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1727468502536,
  "history_end_time" : 1727468502536,
  "history_notes" : null,
  "history_process" : "c8851t",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "50knc5tpcfl",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1727468502537,
  "history_end_time" : 1727468502537,
  "history_notes" : null,
  "history_process" : "s20a2d",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "akwdexaht0m",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1727468502538,
  "history_end_time" : 1727468502538,
  "history_notes" : null,
  "history_process" : "xnk7n1",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "ljqgrgnfz1u",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1727468509406,
  "history_notes" : null,
  "history_process" : "x4ch0z",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "nbdomp8x4nk",
  "history_input" : "import numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nimport joblib\nimport matplotlib.pyplot as plt\n\n# Step 1: Load the dataset\nfile_path = '/Users/vishesh/Desktop/geo_project/data.csv'\ndata = pd.read_csv(file_path, skiprows=3)\n\n# Step 2: Preprocess the data\ndata['Date'] = pd.to_datetime(data['Date'], format='%Y%m')\nvalues = data['Value'].values\n\n# Step 3: Normalize the data for LSTM\nscaler = MinMaxScaler(feature_range=(0, 1))\nscaled_data = scaler.fit_transform(values.reshape(-1, 1))\n\n# Save the scaler for inverse transformation later\njoblib.dump(scaler, 'scaler.pkl')\n\n# Step 4: Create sequences for LSTM\ndef create_sequences(data, sequence_length):\n    sequences = []\n    targets = []\n    for i in range(len(data) - sequence_length):\n        seq = data[i:i + sequence_length]\n        target = data[i + sequence_length]\n        sequences.append(seq)\n        targets.append(target)\n    return np.array(sequences), np.array(targets)\n\nsequence_length = 12  # 12 months\nX, y = create_sequences(scaled_data, sequence_length)\n\n# Step 5: Split into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n\n# Reshape the data for LSTM\nX_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\nX_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n\n# Step 6: Save preprocessed data for later use\nnp.savez('preprocessed_data.npz', X_train=X_train, X_test=X_test, y_train=y_train, y_test=y_test)\n\nprint(\"Data preprocessing completed and saved!\")\n\n\n\nplt.figure(figsize=(10,6))\nplt.plot(data['Date'], data['Value'], label='Temperature Values')\nplt.title('Temperature Values Over Time')\nplt.xlabel('Date')\nplt.ylabel('Temperature (Degrees Fahrenheit)')\nplt.grid(True)\nplt.legend()\nplt.show()\n",
  "history_output" : "Data preprocessing completed and saved!\n2024-09-27 16:21:45.800 python[35720:5748170] +[IMKClient subclass]: chose IMKClient_Legacy\n",
  "history_begin_time" : 1727468503808,
  "history_end_time" : 1727468508051,
  "history_notes" : null,
  "history_process" : "keghqi",
  "host_id" : "100001",
  "indicator" : "Done"
},{
  "history_id" : "h5ngsbdc9u0",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1727468502541,
  "history_end_time" : 1727468502541,
  "history_notes" : null,
  "history_process" : "fhhx69",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "b4bjoq541mt",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1727468502541,
  "history_end_time" : 1727468502541,
  "history_notes" : null,
  "history_process" : "rfwos9",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "w4bbn3wt3s6",
  "history_input" : "import numpy as np\nimport optuna\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.models import Sequential, Model\nfrom tensorflow.keras.layers import Dense, Dropout, Conv1D, MaxPooling1D, Flatten, LSTM, Input, MultiHeadAttention, LayerNormalization, Add\nfrom tensorflow.keras.optimizers import Adam\nfrom pytorch_tabnet.tab_model import TabNetRegressor\n\n# Load preprocessed data\ndata_path = '/Users/vishesh/gw-workspace/L6gDfkNJofh8/preprocessed_data.npz'\ndata = np.load(data_path)\n\n# List all variable names in the .npz file to identify contents\nprint(\"Variables in the .npz file:\", data.files)\n\n# Load data based on contents of the .npz file\nX_train = data['X_train']\ny_train = data['y_train']\n\n# Check if 'X_val' and 'y_val' exist; if not, create validation split\nif 'X_val' in data.files and 'y_val' in data.files:\n    X_val = data['X_val']\n    y_val = data['y_val']\nelse:\n    # Split the data into training and validation sets\n    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n\n# Initialize a dictionary to store model performance\nmodel_performance = {}\n\n# Define the function to create various models\ndef create_model(trial):\n    input_shape = X_train.shape[1]\n    model_type = trial.suggest_categorical('model_type', ['dense', 'cnn', 'lstm', 'transformer', 'tabnet'])\n\n    if model_type == 'dense':\n        model = Sequential()\n        model.add(Input(shape=(input_shape,)))  # Use Input layer to specify input shape\n        num_layers = trial.suggest_int('num_layers', 1, 5)\n        for i in range(num_layers):\n            num_units = trial.suggest_int(f'num_units_l{i}', 16, 128, log=True)\n            model.add(Dense(num_units, activation='relu'))\n            dropout_rate = trial.suggest_float(f'dropout_rate_l{i}', 0.1, 0.5)\n            model.add(Dropout(dropout_rate))\n        model.add(Dense(1, activation='linear'))\n    \n    elif model_type == 'cnn':\n        model = Sequential()\n        model.add(Input(shape=(input_shape, 1)))  # Use Input layer for CNN input shape\n        model.add(Conv1D(filters=trial.suggest_int('filters', 16, 64, log=True),\n                         kernel_size=trial.suggest_int('kernel_size', 3, 5),\n                         activation='relu'))\n        model.add(MaxPooling1D(pool_size=2))\n        model.add(Flatten())\n        model.add(Dense(1, activation='linear'))\n    \n    elif model_type == 'lstm':\n        model = Sequential()\n        model.add(Input(shape=(input_shape, 1)))  # Use Input layer for LSTM input shape\n        model.add(LSTM(units=trial.suggest_int('lstm_units', 16, 64, log=True)))\n        model.add(Dense(1, activation='linear'))\n    \n    elif model_type == 'transformer':\n        inputs = Input(shape=(input_shape, 1))\n        attention = MultiHeadAttention(num_heads=trial.suggest_int('num_heads', 2, 8), key_dim=trial.suggest_int('key_dim', 16, 64))(inputs, inputs)\n        attention = Add()([inputs, attention])\n        attention = LayerNormalization(epsilon=1e-6)(attention)\n        outputs = Flatten()(attention)\n        outputs = Dense(1, activation='linear')(outputs)\n        model = Model(inputs=inputs, outputs=outputs)\n    \n    elif model_type == 'tabnet':\n        # tabnet_model = TabNetRegressor(\n        #     n_d=trial.suggest_int('n_d', 8, 64),\n        #     n_a=trial.suggest_int('n_a', 8, 64),\n        #     n_steps=trial.suggest_int('n_steps', 3, 10),\n        #     gamma=trial.suggest_float('gamma', 1.0, 2.0),\n        #     lambda_sparse=trial.suggest_float('lambda_sparse', 1e-6, 1e-3)\n        # )\n        # tabnet_model.fit(\n        #     X_train, y_train,\n        #     eval_set=[(X_val, y_val)],\n        #     eval_metric=['mae'],\n        #     max_epochs=100,\n        #     patience=10,\n        #     batch_size=256,\n        #     virtual_batch_size=128\n        # )\n        # return tabnet_model\n        # Reshape the data to 2D for TabNet\n    X_train_tabnet = X_train.reshape(X_train.shape[0], -1)\n    X_val_tabnet = X_val.reshape(X_val.shape[0], -1)\n    \n    tabnet_model = TabNetRegressor(\n        n_d=trial.suggest_int('n_d', 8, 64),\n        n_a=trial.suggest_int('n_a', 8, 64),\n        n_steps=trial.suggest_int('n_steps', 3, 10),\n        gamma=trial.suggest_float('gamma', 1.0, 2.0),\n        lambda_sparse=trial.suggest_float('lambda_sparse', 1e-6, 1e-3)\n    )\n    tabnet_model.fit(\n        X_train_tabnet, y_train,\n        eval_set=[(X_val_tabnet, y_val)],\n        eval_metric=['mae'],\n        max_epochs=100,\n        patience=10,\n        batch_size=256,\n        virtual_batch_size=128\n    )\n    return tabnet_model\n\n    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-2, log=True)\n    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mean_squared_error', metrics=['mean_absolute_error'])\n\n    return model\n\n# Define the objective function for Optuna\ndef objective(trial):\n    model = create_model(trial)\n\n    if isinstance(model, TabNetRegressor):\n        model.fit(\n            X_train, y_train,\n            eval_set=[(X_val, y_val)],\n            eval_metric=['mae'],\n            max_epochs=100,\n            patience=10,\n            batch_size=256,\n            virtual_batch_size=128\n        )\n        y_pred = model.predict(X_val)\n        mae = mean_absolute_error(y_val, y_pred)\n    else:\n        model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=10, batch_size=32, verbose=0)\n        _, mae = model.evaluate(X_val, y_val, verbose=0)\n\n    model_type = trial.params['model_type']\n    model_performance[model_type] = mae\n    return mae\n\n# Run the Bayesian optimization\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=10)  # Set n_trials as needed\n\n# Get the best hyperparameters\nbest_params = study.best_params\nprint(f\"Best hyperparameters: {best_params}\")\n\n# Display performance of each model\nprint(\"Model performance (MAE):\")\nfor model_type, mae in model_performance.items():\n    print(f\"{model_type}: {mae}\")\n\n# Plot the model performance\nplt.figure(figsize=(10, 5))\nplt.bar(model_performance.keys(), model_performance.values(), color='skyblue')\nplt.xlabel('Model Type')\nplt.ylabel('Mean Absolute Error')\nplt.title('Model Performance Comparison')\nplt.show()\n",
  "history_output" : "  File \"/Users/vishesh/gw-workspace/w4bbn3wt3s6/auto_ml.py\", line 93\n    X_train_tabnet = X_train.reshape(X_train.shape[0], -1)\n    ^\nIndentationError: expected an indented block after 'elif' statement on line 74\n",
  "history_begin_time" : 1727468509298,
  "history_end_time" : 1727468509405,
  "history_notes" : null,
  "history_process" : "vnrvoi",
  "host_id" : "100001",
  "indicator" : "Failed"
}]
